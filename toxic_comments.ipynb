{"cells":[{"cell_type":"code","execution_count":2,"id":"acquired-complaint","metadata":{"id":"acquired-complaint","executionInfo":{"status":"ok","timestamp":1643993958358,"user_tz":300,"elapsed":6826,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"outputs":[],"source":["import pandas as pd\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","source":[""],"metadata":{"id":"zBllfTAE0CkJ"},"id":"zBllfTAE0CkJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G38No3-P0E55","executionInfo":{"status":"ok","timestamp":1643993996192,"user_tz":300,"elapsed":37839,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"d10ab24f-c997-4132-a66a-214942fb873b"},"id":"G38No3-P0E55","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"id":"settled-monaco","metadata":{"id":"settled-monaco","executionInfo":{"status":"ok","timestamp":1643993998032,"user_tz":300,"elapsed":1843,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"outputs":[],"source":["df = pd.read_csv(\"drive/MyDrive/toxic_comments/train.csv\")"]},{"cell_type":"code","execution_count":null,"id":"simple-minnesota","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"simple-minnesota","executionInfo":{"status":"ok","timestamp":1643979469201,"user_tz":300,"elapsed":6,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"2c9cccdc-02da-4579-f1fa-9410b3fb2966"},"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-de76e98e-0d62-4136-a247-6b4bca18d2df\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de76e98e-0d62-4136-a247-6b4bca18d2df')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-de76e98e-0d62-4136-a247-6b4bca18d2df button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-de76e98e-0d62-4136-a247-6b4bca18d2df');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id  ... identity_hate\n","0  0000997932d777bf  ...             0\n","1  000103f0d9cfb60f  ...             0\n","2  000113f07ec002fd  ...             0\n","3  0001b41b1c6bb37e  ...             0\n","4  0001d958c54c6e35  ...             0\n","\n","[5 rows x 8 columns]"]},"metadata":{},"execution_count":5}],"source":["df.head(5)"]},{"cell_type":"code","execution_count":null,"id":"lesser-permission","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lesser-permission","executionInfo":{"status":"ok","timestamp":1643733166382,"user_tz":300,"elapsed":17,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"7b762680-343f-404d-f4dd-b503ab70cf84"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(159571, 8)"]},"metadata":{},"execution_count":5}],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"id":"changing-cinema","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"changing-cinema","executionInfo":{"status":"ok","timestamp":1643733188709,"user_tz":300,"elapsed":22338,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"86738ac4-04ae-45a4-d984-e2b929d514d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}],"source":["groups = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n","data = dict(zip(groups, [0]*len(groups)))\n","for group in groups:\n","    data[group] = df[df[group] == 1].shape[0]\n","data[\"healthy\"] = df.shape[0] - sum(list(data.values()))\n","healthy = []\n","for index, row in df.iterrows():\n","    healthy.append(max(0, 1 - row[\"toxic\"]-row[\"severe_toxic\"]-row[\"obscene\"]-row[\"threat\"]-row[\"insult\"]-row[\"identity_hate\"]))\n","\n","print(min(healthy))"]},{"cell_type":"code","source":["df[\"label\"] = df.iloc[:, 2:8].values.tolist()\n","df.label = df.label.apply(lambda x: np.array(x).astype('float32'))"],"metadata":{"id":"nXKd3NRUzap9","executionInfo":{"status":"ok","timestamp":1643995133809,"user_tz":300,"elapsed":1401,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"nXKd3NRUzap9","execution_count":23,"outputs":[]},{"cell_type":"code","source":["df.label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hrynzQMVulii","executionInfo":{"status":"ok","timestamp":1643995133810,"user_tz":300,"elapsed":5,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"f4face7f-8ae1-4015-b865-76a157dd805d"},"id":"hrynzQMVulii","execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","                       ...              \n","159566    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","159567    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","159568    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","159569    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","159570    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","Name: label, Length: 159571, dtype: object"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras import layers, activations, models, optimizers, losses, metrics, regularizers\n","import tensorflow_hub as hub\n","import numpy as np\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","features = df[\"comment_text\"]\n","labels = df[\"label\"]\n","train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.3)\n","\n","x = df[\"comment_text\"]\n","token = Tokenizer()\n","token.fit_on_texts(train_features)\n","train_seq = token.texts_to_sequences(train_features)\n","pad_seq_train = pad_sequences(train_seq,maxlen=300)\n","\n","test_seq = token.texts_to_sequences(test_features)\n","pad_seq_test = pad_sequences(test_seq, maxlen=300)\n","vocab_size = len(token.word_index)+1\n","embedding_vector = {}\n","\n","f = open('drive/MyDrive/toxic_comments/glove.6B.50d.txt', encoding='utf-8')\n","for line in tqdm(f):\n","    value = line.split(' ')\n","    word = value[0]\n","    coef = np.array(value[1:],dtype = 'float32')\n","    embedding_vector[word] = coef\n","\n","embedding_matrix = np.zeros((vocab_size,50))\n","for word,i in tqdm(token.word_index.items()):\n","    embedding_value = embedding_vector.get(word)\n","    if embedding_value is not None:\n","        embedding_matrix[i] = embedding_value"],"metadata":{"id":"UqRaHEZWzHx5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643995159777,"user_tz":300,"elapsed":24676,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"ee53b638-e9de-4a77-b132-8d18838eb0e1"},"id":"UqRaHEZWzHx5","execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["400000it [00:06, 57452.34it/s]\n","100%|██████████| 169228/169228 [00:00<00:00, 644016.42it/s]\n"]}]},{"cell_type":"code","source":["\n","\n","class MODEL(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        embed_layer = layers.Embedding(vocab_size,50,weights = [embedding_matrix],input_length=300,trainable = False)\n","\n","        self.glove_head = models.Sequential([\n","            embed_layer,\n","            layers.Dropout(0.2),\n","            layers.BatchNormalization(),\n","            layers.Conv1D(64, 4, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","            bias_regularizer=regularizers.l2(1e-4),\n","            activity_regularizer=regularizers.l2(1e-5)),\n","            layers.MaxPooling1D(),\n","            layers.Conv1D(32, 4, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","            bias_regularizer=regularizers.l2(1e-4),\n","            activity_regularizer=regularizers.l2(1e-5)),\n","            layers.MaxPooling1D(),\n","            layers.Conv1D(16, 4, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","            bias_regularizer=regularizers.l2(1e-4),\n","            activity_regularizer=regularizers.l2(1e-5)),\n","            layers.MaxPooling1D(),\n","            layers.GRU(100),\n","            layers.Dense(50, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","            bias_regularizer=regularizers.l2(1e-4),\n","            activity_regularizer=regularizers.l2(1e-5)),\n","            layers.Dense(100, activation=\"sigmoid\")\n","        ])\n","\n","        url_model = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n","        embed_layer_2 = hub.KerasLayer(url_model, trainable=True, input_shape=[], dtype=tf.string)\n","\n","        self.nnlm_head = models.Sequential([\n","            embed_layer_2,\n","            layers.Dropout(0.2),\n","            layers.BatchNormalization(),\n","            layers.Dense(64, activation=\"relu\"),\n","            layers.Dropout(0.2),\n","            layers.BatchNormalization(),\n","            layers.Dense(32, activation=\"relu\"),\n","            layers.Dropout(0.2),\n","            layers.BatchNormalization(),\n","            layers.Dense(100, activation=\"sigmoid\")\n","        ])\n","\n","        self.final_head = models.Sequential([\n","            layers.Dense(64, activation=\"relu\"),\n","            layers.Dropout(0.2),\n","            layers.BatchNormalization(),\n","            layers.Dense(32, activation=\"relu\"),\n","            layers.Dropout(0.2),\n","            layers.BatchNormalization(),\n","            layers.Dense(6, activation=\"sigmoid\")\n","        ])\n","\n","    def call(self, x):\n","        seq, text = x\n","        seq_out = self.glove_head(seq)\n","        text_out = self.nnlm_head(text)\n","        cc = tf.concat([seq_out, text_out], axis=1)\n","\n","        return self.final_head(cc)\n","\n"],"metadata":{"id":"4EZIJ6elxCT4","executionInfo":{"status":"ok","timestamp":1643995475368,"user_tz":300,"elapsed":255,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"4EZIJ6elxCT4","execution_count":32,"outputs":[]},{"cell_type":"code","source":["embed_layer = layers.Embedding(vocab_size,50,weights = [embedding_matrix],input_length=300,trainable = False)\n","model_2 = models.Sequential([\n","    embed_layer,    \n","    layers.Bidirectional(layers.GRU(100)),\n","    layers.Dense(6, activation=\"sigmoid\")\n","])\n"],"metadata":{"id":"FJofR2n9oXEM","executionInfo":{"status":"ok","timestamp":1644011420621,"user_tz":300,"elapsed":823,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"FJofR2n9oXEM","execution_count":155,"outputs":[]},{"cell_type":"code","source":["model_2.compile(optimizers.Adam(learning_rate=5e-4), loss=losses.BinaryCrossentropy(),\n","              metrics=[metrics.BinaryAccuracy(name='accuracy'), metrics.AUC()])\n","\n","\n","epochs_history_simple = model_2.fit(pad_seq_train, train_labels, epochs=3,\n","                           validation_data=(pad_seq_test, test_labels),\n","                           verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"XsCtXzQ_rSsc","executionInfo":{"status":"error","timestamp":1644015058813,"user_tz":300,"elapsed":142,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"0cf3a21c-1aa8-46ed-e06e-947eab4f6585"},"id":"XsCtXzQ_rSsc","execution_count":157,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-157-a6e563bf81af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_2.compile(optimizers.Adam(learning_rate=5e-4), loss=losses.AUC(),\n\u001b[0m\u001b[1;32m      2\u001b[0m               metrics=[metrics.BinaryAccuracy(name='accuracy'), metrics.AUC()])\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m epochs_history_simple = model_2.fit(pad_seq_train, train_labels, epochs=10,\n","\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.losses' has no attribute 'AUC'"]}]},{"cell_type":"code","source":["train_labels = np.asarray(train_labels.to_list()).astype(np.float32)"],"metadata":{"id":"FsD9MEMEv3ty","executionInfo":{"status":"ok","timestamp":1643995425284,"user_tz":300,"elapsed":235,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"FsD9MEMEv3ty","execution_count":29,"outputs":[]},{"cell_type":"code","source":["test_labels = np.asarray(test_labels.to_list()).astype(np.float32)"],"metadata":{"id":"35QiuPEew4me","executionInfo":{"status":"ok","timestamp":1643995445146,"user_tz":300,"elapsed":92,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"35QiuPEew4me","execution_count":30,"outputs":[]},{"cell_type":"code","source":["model = MODEL()\n","model.compile(optimizers.Adam(learning_rate=5e-4), loss=losses.BinaryCrossentropy(),\n","              metrics=[metrics.BinaryAccuracy(name='accuracy')])\n","\n","\n","epochs_history_simple = model.fit((pad_seq_train, train_features), train_labels, epochs=100,\n","                           validation_data=((pad_seq_test, test_features), test_labels),\n","                           verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":971},"id":"jfkU6dc5tMKn","executionInfo":{"status":"error","timestamp":1644005176063,"user_tz":300,"elapsed":54934,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"b919c22c-f09a-4b6f-886c-c92d0e75133b"},"id":"jfkU6dc5tMKn","execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","3491/3491 [==============================] - 546s 155ms/step - loss: 0.1762 - accuracy: 0.9360 - val_loss: 0.0776 - val_accuracy: 0.9730\n","Epoch 2/100\n","3491/3491 [==============================] - 538s 154ms/step - loss: 0.0804 - accuracy: 0.9704 - val_loss: 0.0733 - val_accuracy: 0.9734\n","Epoch 3/100\n","3491/3491 [==============================] - 538s 154ms/step - loss: 0.0734 - accuracy: 0.9726 - val_loss: 0.0781 - val_accuracy: 0.9723\n","Epoch 4/100\n","3491/3491 [==============================] - 537s 154ms/step - loss: 0.0691 - accuracy: 0.9742 - val_loss: 0.0842 - val_accuracy: 0.9685\n","Epoch 5/100\n","3491/3491 [==============================] - 538s 154ms/step - loss: 0.0640 - accuracy: 0.9767 - val_loss: 0.0908 - val_accuracy: 0.9683\n","Epoch 6/100\n","3491/3491 [==============================] - 538s 154ms/step - loss: 0.0610 - accuracy: 0.9776 - val_loss: 0.0845 - val_accuracy: 0.9702\n","Epoch 7/100\n","3491/3491 [==============================] - 556s 159ms/step - loss: 0.0592 - accuracy: 0.9782 - val_loss: 0.0816 - val_accuracy: 0.9731\n","Epoch 8/100\n","3491/3491 [==============================] - 537s 154ms/step - loss: 0.0562 - accuracy: 0.9792 - val_loss: 0.0820 - val_accuracy: 0.9731\n","Epoch 9/100\n","3491/3491 [==============================] - 554s 159ms/step - loss: 0.0548 - accuracy: 0.9796 - val_loss: 0.0830 - val_accuracy: 0.9743\n","Epoch 10/100\n","3491/3491 [==============================] - 537s 154ms/step - loss: 0.0526 - accuracy: 0.9802 - val_loss: 0.0801 - val_accuracy: 0.9766\n","Epoch 11/100\n","3491/3491 [==============================] - 537s 154ms/step - loss: 0.0510 - accuracy: 0.9810 - val_loss: 0.0825 - val_accuracy: 0.9762\n","Epoch 12/100\n","3491/3491 [==============================] - 538s 154ms/step - loss: 0.0499 - accuracy: 0.9813 - val_loss: 0.0877 - val_accuracy: 0.9760\n","Epoch 13/100\n","3491/3491 [==============================] - 537s 154ms/step - loss: 0.0490 - accuracy: 0.9818 - val_loss: 0.0982 - val_accuracy: 0.9731\n","Epoch 14/100\n","3491/3491 [==============================] - 556s 159ms/step - loss: 0.0481 - accuracy: 0.9821 - val_loss: 0.0800 - val_accuracy: 0.9773\n","Epoch 15/100\n","3491/3491 [==============================] - 538s 154ms/step - loss: 0.0469 - accuracy: 0.9826 - val_loss: 0.0822 - val_accuracy: 0.9769\n","Epoch 16/100\n","3491/3491 [==============================] - 558s 160ms/step - loss: 0.0460 - accuracy: 0.9829 - val_loss: 0.0859 - val_accuracy: 0.9766\n","Epoch 17/100\n","3491/3491 [==============================] - 540s 155ms/step - loss: 0.0449 - accuracy: 0.9832 - val_loss: 0.0866 - val_accuracy: 0.9757\n","Epoch 18/100\n","1087/3491 [========>.....................] - ETA: 5:56 - loss: 0.0435 - accuracy: 0.9836"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-30dd46fd6dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m epochs_history_simple = model.fit((pad_seq_train, train_features), train_labels, epochs=100,\n\u001b[1;32m      7\u001b[0m                            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_seq_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                            verbose=1)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \"\"\"\n\u001b[1;32m   1148\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["model.save_weights(\"big_gains.h5\")"],"metadata":{"id":"GTOWLMeItNdz","executionInfo":{"status":"ok","timestamp":1644005303166,"user_tz":300,"elapsed":3326,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"GTOWLMeItNdz","execution_count":37,"outputs":[]},{"cell_type":"code","source":["model_load = MODEL()"],"metadata":{"id":"yfAdHTPAWiSQ","executionInfo":{"status":"ok","timestamp":1644005315641,"user_tz":300,"elapsed":2413,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"yfAdHTPAWiSQ","execution_count":38,"outputs":[]},{"cell_type":"code","source":["model_load((pad_seq_test[:1], test_features[:1]))\n","model_load.load_weights(\"big_gains.h5\")"],"metadata":{"id":"HwZ_hvJDWHYD","executionInfo":{"status":"ok","timestamp":1644005417113,"user_tz":300,"elapsed":975,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"HwZ_hvJDWHYD","execution_count":43,"outputs":[]},{"cell_type":"code","source":["model_load.compile(metrics=[metrics.BinaryAccuracy(\"binary_acc\"), metrics.AUC()])"],"metadata":{"id":"twECxvUkWnOV","executionInfo":{"status":"ok","timestamp":1644005581138,"user_tz":300,"elapsed":107,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"twECxvUkWnOV","execution_count":48,"outputs":[]},{"cell_type":"code","source":["model_2.evaluate(pad_seq_test, test_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_rvUxVzXHnV","executionInfo":{"status":"ok","timestamp":1644015199211,"user_tz":300,"elapsed":82131,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"86cff912-f1c6-4116-a067-4e3b5e02154c"},"id":"C_rvUxVzXHnV","execution_count":159,"outputs":[{"output_type":"stream","name":"stdout","text":["1496/1496 [==============================] - 69s 46ms/step - loss: 0.0475 - accuracy: 0.9821 - auc_3: 0.9832\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.0475340262055397, 0.9821003675460815, 0.9831740260124207]"]},"metadata":{},"execution_count":159}]},{"cell_type":"code","source":["df_test_text = pd.read_csv(\"drive/MyDrive/toxic_comments/test.csv\")\n","df_test_labels = pd.read_csv(\"drive/MyDrive/toxic_comments/test_labels.csv\")"],"metadata":{"id":"GoPZf88BZWl6","executionInfo":{"status":"ok","timestamp":1644007925503,"user_tz":300,"elapsed":1361,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"GoPZf88BZWl6","execution_count":128,"outputs":[]},{"cell_type":"code","source":["df_test_labels.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"hjfMweiSfxYm","executionInfo":{"status":"ok","timestamp":1644007925509,"user_tz":300,"elapsed":15,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"364f2f5e-d1f7-4781-a030-985b0f83ea9e"},"id":"hjfMweiSfxYm","execution_count":129,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-d0c93c8c-cf2a-406f-ae90-2944c7d31a53\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00001cee341fdb12</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0000247867823ef7</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00013b17ad220c46</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00017563c3f7919a</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00017695ad8997eb</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0c93c8c-cf2a-406f-ae90-2944c7d31a53')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d0c93c8c-cf2a-406f-ae90-2944c7d31a53 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d0c93c8c-cf2a-406f-ae90-2944c7d31a53');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id  toxic  severe_toxic  ...  threat  insult  identity_hate\n","0  00001cee341fdb12     -1            -1  ...      -1      -1             -1\n","1  0000247867823ef7     -1            -1  ...      -1      -1             -1\n","2  00013b17ad220c46     -1            -1  ...      -1      -1             -1\n","3  00017563c3f7919a     -1            -1  ...      -1      -1             -1\n","4  00017695ad8997eb     -1            -1  ...      -1      -1             -1\n","\n","[5 rows x 7 columns]"]},"metadata":{},"execution_count":129}]},{"cell_type":"code","source":["df_test_text[df_test_text.id==\"000247e83dcc1211\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"id":"1d9u4Va1f1_M","executionInfo":{"status":"ok","timestamp":1644007927512,"user_tz":300,"elapsed":195,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"7207b159-8098-464e-c1c7-efb0da2d0e03"},"id":"1d9u4Va1f1_M","execution_count":130,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-a98d1feb-8e50-4880-9a09-57c83b412ad7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>000247e83dcc1211</td>\n","      <td>:Dear god this site is horrible.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a98d1feb-8e50-4880-9a09-57c83b412ad7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a98d1feb-8e50-4880-9a09-57c83b412ad7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a98d1feb-8e50-4880-9a09-57c83b412ad7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id                      comment_text\n","7  000247e83dcc1211  :Dear god this site is horrible."]},"metadata":{},"execution_count":130}]},{"cell_type":"code","source":["df_test_text.head(6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"9HN-s0V0ZqVe","executionInfo":{"status":"ok","timestamp":1644007929507,"user_tz":300,"elapsed":174,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"13ad609d-1fce-4f67-e800-114b7d91a3b2"},"id":"9HN-s0V0ZqVe","execution_count":131,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-c6d7aa44-55ca-4300-9780-b7439d68271f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00001cee341fdb12</td>\n","      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0000247867823ef7</td>\n","      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00013b17ad220c46</td>\n","      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00017563c3f7919a</td>\n","      <td>:If you have a look back at the source, the in...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00017695ad8997eb</td>\n","      <td>I don't anonymously edit articles at all.</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0001ea8717f6de06</td>\n","      <td>Thank you for understanding. I think very high...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6d7aa44-55ca-4300-9780-b7439d68271f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c6d7aa44-55ca-4300-9780-b7439d68271f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c6d7aa44-55ca-4300-9780-b7439d68271f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id                                       comment_text\n","0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n","1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n","2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n","3  00017563c3f7919a  :If you have a look back at the source, the in...\n","4  00017695ad8997eb          I don't anonymously edit articles at all.\n","5  0001ea8717f6de06  Thank you for understanding. I think very high..."]},"metadata":{},"execution_count":131}]},{"cell_type":"code","source":["df_test_labels = df_test_labels[df_test_labels.toxic != -1]"],"metadata":{"id":"95YJaFcSZsjU","executionInfo":{"status":"ok","timestamp":1644007932219,"user_tz":300,"elapsed":151,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"95YJaFcSZsjU","execution_count":132,"outputs":[]},{"cell_type":"code","source":["df_test_labels[\"label\"] = df_test_labels.iloc[:, 1:7].values.tolist()\n","df_test_labels.label = df_test_labels.label.apply(lambda x: np.array(x).astype('float32'))"],"metadata":{"id":"f3e2umdHeJ1-","executionInfo":{"status":"ok","timestamp":1644007934224,"user_tz":300,"elapsed":1050,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"f3e2umdHeJ1-","execution_count":133,"outputs":[]},{"cell_type":"code","source":["df_test_text = df_test_text[df_test_text.id.isin(df_test_labels.id.tolist())]"],"metadata":{"id":"49T94yXQZ4Dw","executionInfo":{"status":"ok","timestamp":1644007934616,"user_tz":300,"elapsed":4,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"49T94yXQZ4Dw","execution_count":134,"outputs":[]},{"cell_type":"code","source":["print(df_test_text.comment_text.size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vzc_yFge7Vu","executionInfo":{"status":"ok","timestamp":1644007827755,"user_tz":300,"elapsed":210,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"178f932c-ef1e-4dd4-d68e-53fb8b75835c"},"id":"7vzc_yFge7Vu","execution_count":117,"outputs":[{"output_type":"stream","name":"stdout","text":["63978\n"]}]},{"cell_type":"code","source":["df_test = pd.DataFrame({\"text\":df_test_text.comment_text, \"label\":df_test_labels.label})"],"metadata":{"id":"NCk3b7vmaTqM","executionInfo":{"status":"ok","timestamp":1644007937393,"user_tz":300,"elapsed":345,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"NCk3b7vmaTqM","execution_count":135,"outputs":[]},{"cell_type":"code","source":["df_test.text.size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ijx1e0IqfYzP","executionInfo":{"status":"ok","timestamp":1644007831461,"user_tz":300,"elapsed":208,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"41baf788-63c7-4bdf-b75b-ae6267d035e3"},"id":"Ijx1e0IqfYzP","execution_count":119,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63978"]},"metadata":{},"execution_count":119}]},{"cell_type":"code","source":["df_test_labels.label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OkBwGtFWds2q","executionInfo":{"status":"ok","timestamp":1644007833951,"user_tz":300,"elapsed":376,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"74459741-bdbc-4ccf-bb59-a21e59755a75"},"id":"OkBwGtFWds2q","execution_count":120,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5         [0.0, 0.0, 0.0, 0.0, 0.0]\n","7         [0.0, 0.0, 0.0, 0.0, 0.0]\n","11        [0.0, 0.0, 0.0, 0.0, 0.0]\n","13        [0.0, 0.0, 0.0, 0.0, 0.0]\n","14        [0.0, 0.0, 0.0, 0.0, 0.0]\n","                    ...            \n","153150    [0.0, 0.0, 0.0, 0.0, 0.0]\n","153151    [0.0, 0.0, 0.0, 0.0, 0.0]\n","153154    [0.0, 0.0, 0.0, 0.0, 0.0]\n","153155    [0.0, 1.0, 0.0, 1.0, 0.0]\n","153156    [0.0, 0.0, 0.0, 0.0, 0.0]\n","Name: label, Length: 63978, dtype: object"]},"metadata":{},"execution_count":120}]},{"cell_type":"code","source":["super_test_features = df_test[\"text\"].astype(str)"],"metadata":{"id":"qWvGR2JNbbg0","executionInfo":{"status":"ok","timestamp":1644007939622,"user_tz":300,"elapsed":254,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"qWvGR2JNbbg0","execution_count":136,"outputs":[]},{"cell_type":"code","source":["print(super_test_features.size, )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQcLbeoKcPQq","executionInfo":{"status":"ok","timestamp":1644007838067,"user_tz":300,"elapsed":192,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"52ce3766-56a7-434b-e836-2ce03e97052c"},"id":"FQcLbeoKcPQq","execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["63978\n"]}]},{"cell_type":"code","source":["df_test_seq = token.texts_to_sequences(super_test_features)\n","df_test_seq_seq = pad_sequences(df_test_seq,maxlen=300)"],"metadata":{"id":"lwNNYLuIbH-V","executionInfo":{"status":"ok","timestamp":1644007945080,"user_tz":300,"elapsed":4022,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"lwNNYLuIbH-V","execution_count":137,"outputs":[]},{"cell_type":"code","source":["super_test_labels = np.asarray(df_test_labels.label.to_list()).astype(np.float32)"],"metadata":{"id":"vScDOriudh2_","executionInfo":{"status":"ok","timestamp":1644007945082,"user_tz":300,"elapsed":16,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"id":"vScDOriudh2_","execution_count":138,"outputs":[]},{"cell_type":"code","source":["model_load.evaluate((df_test_seq_seq, super_test_features), super_test_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D55T8ohtdD4p","executionInfo":{"status":"ok","timestamp":1644007971471,"user_tz":300,"elapsed":26401,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"634c4ad7-4115-49ac-f1a4-38731cf84201"},"id":"D55T8ohtdD4p","execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["2000/2000 [==============================] - 26s 13ms/step - loss: 2.3112e-05 - binary_acc: 0.9703 - auc: 0.9124\n"]},{"output_type":"execute_result","data":{"text/plain":["[2.311197567905765e-05, 0.9703261852264404, 0.9123623967170715]"]},"metadata":{},"execution_count":139}]},{"cell_type":"code","execution_count":null,"id":"advance-ebony","metadata":{"id":"advance-ebony"},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"alone-frequency","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"alone-frequency","executionInfo":{"status":"ok","timestamp":1643733188910,"user_tz":300,"elapsed":206,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"50f93267-b709-4580-bff1-009f615ad132"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BarContainer object of 7 artists>"]},"metadata":{},"execution_count":8},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["plt.bar(list(data.keys()), list(data.values()))"]},{"cell_type":"code","execution_count":null,"id":"tropical-payroll","metadata":{"id":"tropical-payroll"},"outputs":[],"source":["def tts(df, ratio):\n","    df = df.sample(frac=1).reset_index(drop=True)\n","   \n","    trains = df.iloc[:int(ratio*len(df))]\n","    tests = df.iloc[int(ratio*len(df)):]\n","\n","\n","        \n","    return trains, tests\n","        "]},{"cell_type":"code","execution_count":null,"id":"facial-builder","metadata":{"id":"facial-builder"},"outputs":[],"source":["#df, _ = tts(df, 0.1)"]},{"cell_type":"code","execution_count":null,"id":"sophisticated-portland","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sophisticated-portland","executionInfo":{"status":"ok","timestamp":1643733188912,"user_tz":300,"elapsed":12,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"c5edc3e5-71dc-42ad-f418-907d76d3943d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(159571, 8)"]},"metadata":{},"execution_count":11}],"source":["df.shape"]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YwtpMXi0g9S","executionInfo":{"status":"ok","timestamp":1643733192138,"user_tz":300,"elapsed":3232,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"94243c70-4510-4257-e49b-3c8480a116e5"},"id":"9YwtpMXi0g9S","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"]}]},{"cell_type":"code","execution_count":null,"id":"guided-taiwan","metadata":{"id":"guided-taiwan"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"]},{"cell_type":"code","execution_count":null,"id":"swedish-charge","metadata":{"id":"swedish-charge"},"outputs":[],"source":["train, test = tts(df, 0.8)"]},{"cell_type":"code","execution_count":null,"id":"scenic-ontario","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"scenic-ontario","executionInfo":{"status":"ok","timestamp":1643733192783,"user_tz":300,"elapsed":17,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"5f2291f1-832b-4a2f-a138-08361b748cda"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((127656, 8), (31915, 8))"]},"metadata":{},"execution_count":15}],"source":["train.shape, test.shape"]},{"cell_type":"code","execution_count":null,"id":"modular-marathon","metadata":{"id":"modular-marathon"},"outputs":[],"source":["class ToxicDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","    \n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        text = row[\"comment_text\"]\n","        \n","        text = tokenizer(text,\n","                               padding='max_length', max_length = 90, truncation=True,\n","                                return_tensors=\"pt\")\n","        \n","        groups = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n","        label = torch.tensor([row[g] for g in groups], dtype=torch.float32)\n","        \n","        return text, label"]},{"cell_type":"code","source":[""],"metadata":{"id":"OqjPamz7y_o7"},"id":"OqjPamz7y_o7","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"curious-recorder","metadata":{"id":"curious-recorder"},"outputs":[],"source":["from torch import nn\n","from transformers import BertModel\n","\n","class BertFakeNews(nn.Module):\n","\n","    def __init__(self):\n","\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","        self.fc = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.Linear(768, 400),\n","            nn.Dropout(0.2),\n","            nn.Linear(400, 6),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input_id, mask):\n","\n","        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n","\n","        return self.fc(pooled_output)"]},{"cell_type":"code","execution_count":null,"id":"correct-orlando","metadata":{"id":"correct-orlando"},"outputs":[],"source":["train_data = ToxicDataset(train)\n","test_data = ToxicDataset(test)\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=1)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=1)"]},{"cell_type":"code","execution_count":null,"id":"round-subscriber","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"round-subscriber","executionInfo":{"status":"ok","timestamp":1643733204481,"user_tz":300,"elapsed":11550,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"aca5b909-9a21-432e-c97b-620c38605f04"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = BertFakeNews().to(device)"]},{"cell_type":"code","execution_count":null,"id":"excellent-mouth","metadata":{"id":"excellent-mouth"},"outputs":[],"source":["criterion = nn.MSELoss()\n","optimizer = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"id":"reasonable-charge","metadata":{"id":"reasonable-charge"},"outputs":[],"source":["import tqdm\n","def test_model():\n","    valid = 0\n","    running_loss = 0\n","    for text, labels in tqdm.tqdm(test_loader):\n","        labels = labels.to(device)\n","        mask = text['attention_mask'].to(device)\n","        input_id = text['input_ids'].squeeze(1).to(device)\n","        \n","        output = model(input_id, mask)\n","        loss = criterion(output, labels)\n","        running_loss += loss.item()\n","        \n","        pred = torch.round(output)\n","        valid += (pred.eq(labels)).sum()\n","    \n","    print(f\"loss: {running_loss}, accuracy: {valid/len(test_data)/6}\")\n","        "]},{"cell_type":"code","execution_count":null,"id":"sweet-serum","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sweet-serum","outputId":"11143729-7b41-4a6c-ed5c-a6f0c6ca9d9c","executionInfo":{"status":"error","timestamp":1643740377062,"user_tz":300,"elapsed":7172603,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","epoch 0, step: 77660/127656, loss: 0.008935777470469475\n","epoch 0, step: 77670/127656, loss: 0.011893446557223797\n","epoch 0, step: 77680/127656, loss: 0.010815953835844994\n","epoch 0, step: 77690/127656, loss: 0.01226853858679533\n","epoch 0, step: 77700/127656, loss: 0.12021078169345856\n","epoch 0, step: 77710/127656, loss: 0.012108748778700829\n","epoch 0, step: 77720/127656, loss: 0.013231161050498486\n","epoch 0, step: 77730/127656, loss: 0.011196605861186981\n","epoch 0, step: 77740/127656, loss: 0.006551904603838921\n","epoch 0, step: 77750/127656, loss: 0.010317899286746979\n","epoch 0, step: 77760/127656, loss: 0.015065275132656097\n","epoch 0, step: 77770/127656, loss: 0.01031577866524458\n","epoch 0, step: 77780/127656, loss: 0.011830447241663933\n","epoch 0, step: 77790/127656, loss: 0.013245361857116222\n","epoch 0, step: 77800/127656, loss: 0.013129651546478271\n","epoch 0, step: 77810/127656, loss: 0.008599158376455307\n","epoch 0, step: 77820/127656, loss: 0.009865225292742252\n","epoch 0, step: 77830/127656, loss: 0.0098256915807724\n","epoch 0, step: 77840/127656, loss: 0.010202266275882721\n","epoch 0, step: 77850/127656, loss: 0.010135363787412643\n","epoch 0, step: 77860/127656, loss: 0.020061613991856575\n","epoch 0, step: 77870/127656, loss: 0.009976845234632492\n","epoch 0, step: 77880/127656, loss: 0.010210893116891384\n","epoch 0, step: 77890/127656, loss: 0.0116446977481246\n","epoch 0, step: 77900/127656, loss: 0.012371443212032318\n","epoch 0, step: 77910/127656, loss: 0.4141688346862793\n","epoch 0, step: 77920/127656, loss: 0.011430470272898674\n","epoch 0, step: 77930/127656, loss: 0.009802250191569328\n","epoch 0, step: 77940/127656, loss: 0.1347605288028717\n","epoch 0, step: 77950/127656, loss: 0.014114576391875744\n","epoch 0, step: 77960/127656, loss: 0.01402204018086195\n","epoch 0, step: 77970/127656, loss: 0.013685420155525208\n","epoch 0, step: 77980/127656, loss: 0.009214425459504128\n","epoch 0, step: 77990/127656, loss: 0.5408028960227966\n","epoch 0, step: 78000/127656, loss: 0.014314383268356323\n","epoch 0, step: 78010/127656, loss: 0.010624036192893982\n","epoch 0, step: 78020/127656, loss: 0.013195127248764038\n","epoch 0, step: 78030/127656, loss: 0.013613550923764706\n","epoch 0, step: 78040/127656, loss: 0.010798218660056591\n","epoch 0, step: 78050/127656, loss: 0.010492924600839615\n","epoch 0, step: 78060/127656, loss: 0.009619370102882385\n","epoch 0, step: 78070/127656, loss: 0.01351424865424633\n","epoch 0, step: 78080/127656, loss: 0.01045800931751728\n","epoch 0, step: 78090/127656, loss: 0.008103936910629272\n","epoch 0, step: 78100/127656, loss: 0.011763131245970726\n","epoch 0, step: 78110/127656, loss: 0.014270363375544548\n","epoch 0, step: 78120/127656, loss: 0.008936574682593346\n","epoch 0, step: 78130/127656, loss: 0.017739344388246536\n","epoch 0, step: 78140/127656, loss: 0.010008654557168484\n","epoch 0, step: 78150/127656, loss: 0.013265411369502544\n","epoch 0, step: 78160/127656, loss: 0.019799625501036644\n","epoch 0, step: 78170/127656, loss: 0.008308256044983864\n","epoch 0, step: 78180/127656, loss: 0.009374517016112804\n","epoch 0, step: 78190/127656, loss: 0.011708184145390987\n","epoch 0, step: 78200/127656, loss: 0.013940650038421154\n","epoch 0, step: 78210/127656, loss: 0.0134958541020751\n","epoch 0, step: 78220/127656, loss: 0.008774101734161377\n","epoch 0, step: 78230/127656, loss: 0.017058061435818672\n","epoch 0, step: 78240/127656, loss: 0.011137181892991066\n","epoch 0, step: 78250/127656, loss: 0.01092548668384552\n","epoch 0, step: 78260/127656, loss: 0.010400473140180111\n","epoch 0, step: 78270/127656, loss: 0.013600528240203857\n","epoch 0, step: 78280/127656, loss: 0.01205956470221281\n","epoch 0, step: 78290/127656, loss: 0.014087717048823833\n","epoch 0, step: 78300/127656, loss: 0.016130197793245316\n","epoch 0, step: 78310/127656, loss: 0.3969011902809143\n","epoch 0, step: 78320/127656, loss: 0.011851636692881584\n","epoch 0, step: 78330/127656, loss: 0.016442671418190002\n","epoch 0, step: 78340/127656, loss: 0.1365557461977005\n","epoch 0, step: 78350/127656, loss: 0.01826801709830761\n","epoch 0, step: 78360/127656, loss: 0.14554238319396973\n","epoch 0, step: 78370/127656, loss: 0.009100745432078838\n","epoch 0, step: 78380/127656, loss: 0.011807240545749664\n","epoch 0, step: 78390/127656, loss: 0.007923501543700695\n","epoch 0, step: 78400/127656, loss: 0.014805320650339127\n","epoch 0, step: 78410/127656, loss: 0.011115193367004395\n","epoch 0, step: 78420/127656, loss: 0.015160041861236095\n","epoch 0, step: 78430/127656, loss: 0.010523269884288311\n","epoch 0, step: 78440/127656, loss: 0.01574699394404888\n","epoch 0, step: 78450/127656, loss: 0.013565322384238243\n","epoch 0, step: 78460/127656, loss: 0.016325941309332848\n","epoch 0, step: 78470/127656, loss: 0.008111167699098587\n","epoch 0, step: 78480/127656, loss: 0.14885956048965454\n","epoch 0, step: 78490/127656, loss: 0.0131126893684268\n","epoch 0, step: 78500/127656, loss: 0.00788929220288992\n","epoch 0, step: 78510/127656, loss: 0.012094411998987198\n","epoch 0, step: 78520/127656, loss: 0.006655462086200714\n","epoch 0, step: 78530/127656, loss: 0.018129128962755203\n","epoch 0, step: 78540/127656, loss: 0.009395547211170197\n","epoch 0, step: 78550/127656, loss: 0.2805368900299072\n","epoch 0, step: 78560/127656, loss: 0.38487645983695984\n","epoch 0, step: 78570/127656, loss: 0.014699689112603664\n","epoch 0, step: 78580/127656, loss: 0.01497203391045332\n","epoch 0, step: 78590/127656, loss: 0.008958331309258938\n","epoch 0, step: 78600/127656, loss: 0.007795021869242191\n","epoch 0, step: 78610/127656, loss: 0.020169297233223915\n","epoch 0, step: 78620/127656, loss: 0.012173190712928772\n","epoch 0, step: 78630/127656, loss: 0.011259174905717373\n","epoch 0, step: 78640/127656, loss: 0.011001061648130417\n","epoch 0, step: 78650/127656, loss: 0.13299119472503662\n","epoch 0, step: 78660/127656, loss: 0.011719303205609322\n","epoch 0, step: 78670/127656, loss: 0.017049912363290787\n","epoch 0, step: 78680/127656, loss: 0.011301751248538494\n","epoch 0, step: 78690/127656, loss: 0.012483859434723854\n","epoch 0, step: 78700/127656, loss: 0.01022302731871605\n","epoch 0, step: 78710/127656, loss: 0.012247543781995773\n","epoch 0, step: 78720/127656, loss: 0.013146847486495972\n","epoch 0, step: 78730/127656, loss: 0.010921543464064598\n","epoch 0, step: 78740/127656, loss: 0.008134681731462479\n","epoch 0, step: 78750/127656, loss: 0.01223753672093153\n","epoch 0, step: 78760/127656, loss: 0.012624070048332214\n","epoch 0, step: 78770/127656, loss: 0.01126891653984785\n","epoch 0, step: 78780/127656, loss: 0.010016567073762417\n","epoch 0, step: 78790/127656, loss: 0.014428882859647274\n","epoch 0, step: 78800/127656, loss: 0.40335023403167725\n","epoch 0, step: 78810/127656, loss: 0.009060395881533623\n","epoch 0, step: 78820/127656, loss: 0.01610555127263069\n","epoch 0, step: 78830/127656, loss: 0.39513444900512695\n","epoch 0, step: 78840/127656, loss: 0.010503476485610008\n","epoch 0, step: 78850/127656, loss: 0.007848568260669708\n","epoch 0, step: 78860/127656, loss: 0.01227772980928421\n","epoch 0, step: 78870/127656, loss: 0.015234804712235928\n","epoch 0, step: 78880/127656, loss: 0.015837397426366806\n","epoch 0, step: 78890/127656, loss: 0.011163221672177315\n","epoch 0, step: 78900/127656, loss: 0.01013186015188694\n","epoch 0, step: 78910/127656, loss: 0.009217014536261559\n","epoch 0, step: 78920/127656, loss: 0.01323164626955986\n","epoch 0, step: 78930/127656, loss: 0.011148247867822647\n","epoch 0, step: 78940/127656, loss: 0.013359514996409416\n","epoch 0, step: 78950/127656, loss: 0.015558738261461258\n","epoch 0, step: 78960/127656, loss: 0.017269110307097435\n","epoch 0, step: 78970/127656, loss: 0.018204834312200546\n","epoch 0, step: 78980/127656, loss: 0.00987707544118166\n","epoch 0, step: 78990/127656, loss: 0.014058454893529415\n","epoch 0, step: 79000/127656, loss: 0.5178942680358887\n","epoch 0, step: 79010/127656, loss: 0.01128004863858223\n","epoch 0, step: 79020/127656, loss: 0.011954608373343945\n","epoch 0, step: 79030/127656, loss: 0.015012132935225964\n","epoch 0, step: 79040/127656, loss: 0.013564379885792732\n","epoch 0, step: 79050/127656, loss: 0.008711453527212143\n","epoch 0, step: 79060/127656, loss: 0.011861877515912056\n","epoch 0, step: 79070/127656, loss: 0.010296130552887917\n","epoch 0, step: 79080/127656, loss: 0.134484201669693\n","epoch 0, step: 79090/127656, loss: 0.011253046803176403\n","epoch 0, step: 79100/127656, loss: 0.5477999448776245\n","epoch 0, step: 79110/127656, loss: 0.009758634492754936\n","epoch 0, step: 79120/127656, loss: 0.009954459965229034\n","epoch 0, step: 79130/127656, loss: 0.008340917527675629\n","epoch 0, step: 79140/127656, loss: 0.015261726453900337\n","epoch 0, step: 79150/127656, loss: 0.010008472017943859\n","epoch 0, step: 79160/127656, loss: 0.011128145270049572\n","epoch 0, step: 79170/127656, loss: 0.01605604961514473\n","epoch 0, step: 79180/127656, loss: 0.009225282818078995\n","epoch 0, step: 79190/127656, loss: 0.018720611929893494\n","epoch 0, step: 79200/127656, loss: 0.012930288910865784\n","epoch 0, step: 79210/127656, loss: 0.009791385382413864\n","epoch 0, step: 79220/127656, loss: 0.015291961841285229\n","epoch 0, step: 79230/127656, loss: 0.012177202850580215\n","epoch 0, step: 79240/127656, loss: 0.010002506896853447\n","epoch 0, step: 79250/127656, loss: 0.012578371912240982\n","epoch 0, step: 79260/127656, loss: 0.01631089113652706\n","epoch 0, step: 79270/127656, loss: 0.016378935426473618\n","epoch 0, step: 79280/127656, loss: 0.012295919470489025\n","epoch 0, step: 79290/127656, loss: 0.009908298030495644\n","epoch 0, step: 79300/127656, loss: 0.019883893430233\n","epoch 0, step: 79310/127656, loss: 0.018072759732604027\n","epoch 0, step: 79320/127656, loss: 0.008244717493653297\n","epoch 0, step: 79330/127656, loss: 0.13570278882980347\n","epoch 0, step: 79340/127656, loss: 0.012103683315217495\n","epoch 0, step: 79350/127656, loss: 0.0154073191806674\n","epoch 0, step: 79360/127656, loss: 0.010451081208884716\n","epoch 0, step: 79370/127656, loss: 0.37552493810653687\n","epoch 0, step: 79380/127656, loss: 0.019213568419218063\n","epoch 0, step: 79390/127656, loss: 0.011672443710267544\n","epoch 0, step: 79400/127656, loss: 0.017400994896888733\n","epoch 0, step: 79410/127656, loss: 0.011507237330079079\n","epoch 0, step: 79420/127656, loss: 0.008524319157004356\n","epoch 0, step: 79430/127656, loss: 0.01302514411509037\n","epoch 0, step: 79440/127656, loss: 0.007560603320598602\n","epoch 0, step: 79450/127656, loss: 0.007854338735342026\n","epoch 0, step: 79460/127656, loss: 0.009858943521976471\n","epoch 0, step: 79470/127656, loss: 0.013876637443900108\n","epoch 0, step: 79480/127656, loss: 0.013908675871789455\n","epoch 0, step: 79490/127656, loss: 0.011658094823360443\n","epoch 0, step: 79500/127656, loss: 0.015329902991652489\n","epoch 0, step: 79510/127656, loss: 0.011674627661705017\n","epoch 0, step: 79520/127656, loss: 0.010644017718732357\n","epoch 0, step: 79530/127656, loss: 0.013903656974434853\n","epoch 0, step: 79540/127656, loss: 0.010093078948557377\n","epoch 0, step: 79550/127656, loss: 0.010491888970136642\n","epoch 0, step: 79560/127656, loss: 0.009137368761003017\n","epoch 0, step: 79570/127656, loss: 0.012989707291126251\n","epoch 0, step: 79580/127656, loss: 0.13227762281894684\n","epoch 0, step: 79590/127656, loss: 0.010269691236317158\n","epoch 0, step: 79600/127656, loss: 0.010899027809500694\n","epoch 0, step: 79610/127656, loss: 0.40360960364341736\n","epoch 0, step: 79620/127656, loss: 0.017920780926942825\n","epoch 0, step: 79630/127656, loss: 0.020236406475305557\n","epoch 0, step: 79640/127656, loss: 0.12548798322677612\n","epoch 0, step: 79650/127656, loss: 0.012018229812383652\n","epoch 0, step: 79660/127656, loss: 0.010576687753200531\n","epoch 0, step: 79670/127656, loss: 0.019512172788381577\n","epoch 0, step: 79680/127656, loss: 0.008126656524837017\n","epoch 0, step: 79690/127656, loss: 0.01270919106900692\n","epoch 0, step: 79700/127656, loss: 0.009358677081763744\n","epoch 0, step: 79710/127656, loss: 0.007603380363434553\n","epoch 0, step: 79720/127656, loss: 0.018867649137973785\n","epoch 0, step: 79730/127656, loss: 0.008071605116128922\n","epoch 0, step: 79740/127656, loss: 0.009347876533865929\n","epoch 0, step: 79750/127656, loss: 0.0109595637768507\n","epoch 0, step: 79760/127656, loss: 0.01059102825820446\n","epoch 0, step: 79770/127656, loss: 0.0063475631177425385\n","epoch 0, step: 79780/127656, loss: 0.011392710730433464\n","epoch 0, step: 79790/127656, loss: 0.013776198029518127\n","epoch 0, step: 79800/127656, loss: 0.006708625704050064\n","epoch 0, step: 79810/127656, loss: 0.00865273829549551\n","epoch 0, step: 79820/127656, loss: 0.00919189490377903\n","epoch 0, step: 79830/127656, loss: 0.01101118978112936\n","epoch 0, step: 79840/127656, loss: 0.014152079820632935\n","epoch 0, step: 79850/127656, loss: 0.021092362701892853\n","epoch 0, step: 79860/127656, loss: 0.010061986744403839\n","epoch 0, step: 79870/127656, loss: 0.010623931884765625\n","epoch 0, step: 79880/127656, loss: 0.012284151278436184\n","epoch 0, step: 79890/127656, loss: 0.011411665938794613\n","epoch 0, step: 79900/127656, loss: 0.017145946621894836\n","epoch 0, step: 79910/127656, loss: 0.0073502762243151665\n","epoch 0, step: 79920/127656, loss: 0.011150761507451534\n","epoch 0, step: 79930/127656, loss: 0.542811930179596\n","epoch 0, step: 79940/127656, loss: 0.01697959564626217\n","epoch 0, step: 79950/127656, loss: 0.012434481643140316\n","epoch 0, step: 79960/127656, loss: 0.011885459534823895\n","epoch 0, step: 79970/127656, loss: 0.012957796454429626\n","epoch 0, step: 79980/127656, loss: 0.010631168261170387\n","epoch 0, step: 79990/127656, loss: 0.017214922234416008\n","epoch 0, step: 80000/127656, loss: 0.011435678228735924\n","epoch 0, step: 80010/127656, loss: 0.5318726897239685\n","epoch 0, step: 80020/127656, loss: 0.013292156159877777\n","epoch 0, step: 80030/127656, loss: 0.013026367872953415\n","epoch 0, step: 80040/127656, loss: 0.012865958735346794\n","epoch 0, step: 80050/127656, loss: 0.008491424843668938\n","epoch 0, step: 80060/127656, loss: 0.013587580993771553\n","epoch 0, step: 80070/127656, loss: 0.01186516135931015\n","epoch 0, step: 80080/127656, loss: 0.005807945039123297\n","epoch 0, step: 80090/127656, loss: 0.008705801330506802\n","epoch 0, step: 80100/127656, loss: 0.00813377182930708\n","epoch 0, step: 80110/127656, loss: 0.1380631923675537\n","epoch 0, step: 80120/127656, loss: 0.13481496274471283\n","epoch 0, step: 80130/127656, loss: 0.008339200168848038\n","epoch 0, step: 80140/127656, loss: 0.009024962782859802\n","epoch 0, step: 80150/127656, loss: 0.008028166368603706\n","epoch 0, step: 80160/127656, loss: 0.018476827070116997\n","epoch 0, step: 80170/127656, loss: 0.011733944527804852\n","epoch 0, step: 80180/127656, loss: 0.016647594049572945\n","epoch 0, step: 80190/127656, loss: 0.01443934254348278\n","epoch 0, step: 80200/127656, loss: 0.01264480035752058\n","epoch 0, step: 80210/127656, loss: 0.020635873079299927\n","epoch 0, step: 80220/127656, loss: 0.009104834869503975\n","epoch 0, step: 80230/127656, loss: 0.012074729427695274\n","epoch 0, step: 80240/127656, loss: 0.1406887173652649\n","epoch 0, step: 80250/127656, loss: 0.00908816047012806\n","epoch 0, step: 80260/127656, loss: 0.008842703886330128\n","epoch 0, step: 80270/127656, loss: 0.010672311298549175\n","epoch 0, step: 80280/127656, loss: 0.01147795282304287\n","epoch 0, step: 80290/127656, loss: 0.011364957317709923\n","epoch 0, step: 80300/127656, loss: 0.011855694465339184\n","epoch 0, step: 80310/127656, loss: 0.0121584078297019\n","epoch 0, step: 80320/127656, loss: 0.014319950714707375\n","epoch 0, step: 80330/127656, loss: 0.009041312150657177\n","epoch 0, step: 80340/127656, loss: 0.009143675677478313\n","epoch 0, step: 80350/127656, loss: 0.009279916994273663\n","epoch 0, step: 80360/127656, loss: 0.010805381461977959\n","epoch 0, step: 80370/127656, loss: 0.012008852325379848\n","epoch 0, step: 80380/127656, loss: 0.011468509212136269\n","epoch 0, step: 80390/127656, loss: 0.012939798645675182\n","epoch 0, step: 80400/127656, loss: 0.015933280810713768\n","epoch 0, step: 80410/127656, loss: 0.4018363356590271\n","epoch 0, step: 80420/127656, loss: 0.010966815054416656\n","epoch 0, step: 80430/127656, loss: 0.013302563689649105\n","epoch 0, step: 80440/127656, loss: 0.009059462696313858\n","epoch 0, step: 80450/127656, loss: 0.007883984595537186\n","epoch 0, step: 80460/127656, loss: 0.014412017539143562\n","epoch 0, step: 80470/127656, loss: 0.018578341230750084\n","epoch 0, step: 80480/127656, loss: 0.26309865713119507\n","epoch 0, step: 80490/127656, loss: 0.011651488021016121\n","epoch 0, step: 80500/127656, loss: 0.007244182750582695\n","epoch 0, step: 80510/127656, loss: 0.012868141755461693\n","epoch 0, step: 80520/127656, loss: 0.006972048431634903\n","epoch 0, step: 80530/127656, loss: 0.011685702949762344\n","epoch 0, step: 80540/127656, loss: 0.007872030138969421\n","epoch 0, step: 80550/127656, loss: 0.014326109550893307\n","epoch 0, step: 80560/127656, loss: 0.009782548993825912\n","epoch 0, step: 80570/127656, loss: 0.0068231001496315\n","epoch 0, step: 80580/127656, loss: 0.01092101912945509\n","epoch 0, step: 80590/127656, loss: 0.009408032521605492\n","epoch 0, step: 80600/127656, loss: 0.009322863072156906\n","epoch 0, step: 80610/127656, loss: 0.011387120932340622\n","epoch 0, step: 80620/127656, loss: 0.010336593724787235\n","epoch 0, step: 80630/127656, loss: 0.013273311778903008\n","epoch 0, step: 80640/127656, loss: 0.2780880928039551\n","epoch 0, step: 80650/127656, loss: 0.009196560829877853\n","epoch 0, step: 80660/127656, loss: 0.013489346019923687\n","epoch 0, step: 80670/127656, loss: 0.3858693540096283\n","epoch 0, step: 80680/127656, loss: 0.015808213502168655\n","epoch 0, step: 80690/127656, loss: 0.26138782501220703\n","epoch 0, step: 80700/127656, loss: 0.014807607978582382\n","epoch 0, step: 80710/127656, loss: 0.012366904877126217\n","epoch 0, step: 80720/127656, loss: 0.011465337127447128\n","epoch 0, step: 80730/127656, loss: 0.010673359036445618\n","epoch 0, step: 80740/127656, loss: 0.013940571807324886\n","epoch 0, step: 80750/127656, loss: 0.00909140333533287\n","epoch 0, step: 80760/127656, loss: 0.0070260739885270596\n","epoch 0, step: 80770/127656, loss: 0.00858253799378872\n","epoch 0, step: 80780/127656, loss: 0.006053892429918051\n","epoch 0, step: 80790/127656, loss: 0.01297110877931118\n","epoch 0, step: 80800/127656, loss: 0.010203373618423939\n","epoch 0, step: 80810/127656, loss: 0.01611568033695221\n","epoch 0, step: 80820/127656, loss: 0.2669382691383362\n","epoch 0, step: 80830/127656, loss: 0.27013546228408813\n","epoch 0, step: 80840/127656, loss: 0.012111607939004898\n","epoch 0, step: 80850/127656, loss: 0.13523021340370178\n","epoch 0, step: 80860/127656, loss: 0.018267348408699036\n","epoch 0, step: 80870/127656, loss: 0.015949739143252373\n","epoch 0, step: 80880/127656, loss: 0.020797589793801308\n","epoch 0, step: 80890/127656, loss: 0.015075497329235077\n","epoch 0, step: 80900/127656, loss: 0.40507566928863525\n","epoch 0, step: 80910/127656, loss: 0.011301388964056969\n","epoch 0, step: 80920/127656, loss: 0.011959883384406567\n","epoch 0, step: 80930/127656, loss: 0.012568479403853416\n","epoch 0, step: 80940/127656, loss: 0.014497149735689163\n","epoch 0, step: 80950/127656, loss: 0.01244195457547903\n","epoch 0, step: 80960/127656, loss: 0.011709804646670818\n","epoch 0, step: 80970/127656, loss: 0.00857466459274292\n","epoch 0, step: 80980/127656, loss: 0.009545248933136463\n","epoch 0, step: 80990/127656, loss: 0.011682461947202682\n","epoch 0, step: 81000/127656, loss: 0.011548275128006935\n","epoch 0, step: 81010/127656, loss: 0.020328212529420853\n","epoch 0, step: 81020/127656, loss: 0.013819856569170952\n","epoch 0, step: 81030/127656, loss: 0.014602629467844963\n","epoch 0, step: 81040/127656, loss: 0.008579607121646404\n","epoch 0, step: 81050/127656, loss: 0.011288988403975964\n","epoch 0, step: 81060/127656, loss: 0.012649605050683022\n","epoch 0, step: 81070/127656, loss: 0.015944547951221466\n","epoch 0, step: 81080/127656, loss: 0.010267924517393112\n","epoch 0, step: 81090/127656, loss: 0.015497462823987007\n","epoch 0, step: 81100/127656, loss: 0.010749630630016327\n","epoch 0, step: 81110/127656, loss: 0.008777955546975136\n","epoch 0, step: 81120/127656, loss: 0.010534718632698059\n","epoch 0, step: 81130/127656, loss: 0.011645968072116375\n","epoch 0, step: 81140/127656, loss: 0.01063293032348156\n","epoch 0, step: 81150/127656, loss: 0.01034768670797348\n","epoch 0, step: 81160/127656, loss: 0.00840197317302227\n","epoch 0, step: 81170/127656, loss: 0.0075164674781262875\n","epoch 0, step: 81180/127656, loss: 0.011624839156866074\n","epoch 0, step: 81190/127656, loss: 0.014635525643825531\n","epoch 0, step: 81200/127656, loss: 0.40920329093933105\n","epoch 0, step: 81210/127656, loss: 0.009020084515213966\n","epoch 0, step: 81220/127656, loss: 0.009035659953951836\n","epoch 0, step: 81230/127656, loss: 0.011456514708697796\n","epoch 0, step: 81240/127656, loss: 0.00755775673314929\n","epoch 0, step: 81250/127656, loss: 0.38533902168273926\n","epoch 0, step: 81260/127656, loss: 0.011172937229275703\n","epoch 0, step: 81270/127656, loss: 0.006120922975242138\n","epoch 0, step: 81280/127656, loss: 0.009599555283784866\n","epoch 0, step: 81290/127656, loss: 0.01609603688120842\n","epoch 0, step: 81300/127656, loss: 0.011731898412108421\n","epoch 0, step: 81310/127656, loss: 0.1341768205165863\n","epoch 0, step: 81320/127656, loss: 0.013221485540270805\n","epoch 0, step: 81330/127656, loss: 0.016556721180677414\n","epoch 0, step: 81340/127656, loss: 0.012306967750191689\n","epoch 0, step: 81350/127656, loss: 0.01134573109447956\n","epoch 0, step: 81360/127656, loss: 0.013770188204944134\n","epoch 0, step: 81370/127656, loss: 0.014332829974591732\n","epoch 0, step: 81380/127656, loss: 0.00948740541934967\n","epoch 0, step: 81390/127656, loss: 0.012441559694707394\n","epoch 0, step: 81400/127656, loss: 0.016021281480789185\n","epoch 0, step: 81410/127656, loss: 0.011836899444460869\n","epoch 0, step: 81420/127656, loss: 0.014563163742423058\n","epoch 0, step: 81430/127656, loss: 0.006607602350413799\n","epoch 0, step: 81440/127656, loss: 0.013141192495822906\n","epoch 0, step: 81450/127656, loss: 0.018413636833429337\n","epoch 0, step: 81460/127656, loss: 0.009301750920712948\n","epoch 0, step: 81470/127656, loss: 0.14449277520179749\n","epoch 0, step: 81480/127656, loss: 0.016889913007616997\n","epoch 0, step: 81490/127656, loss: 0.01465656142681837\n","epoch 0, step: 81500/127656, loss: 0.014824889600276947\n","epoch 0, step: 81510/127656, loss: 0.012034034356474876\n","epoch 0, step: 81520/127656, loss: 0.009032322093844414\n","epoch 0, step: 81530/127656, loss: 0.01361408457159996\n","epoch 0, step: 81540/127656, loss: 0.01074876170605421\n","epoch 0, step: 81550/127656, loss: 0.013629808090627193\n","epoch 0, step: 81560/127656, loss: 0.02065116912126541\n","epoch 0, step: 81570/127656, loss: 0.010717942379415035\n","epoch 0, step: 81580/127656, loss: 0.009885506704449654\n","epoch 0, step: 81590/127656, loss: 0.009091528132557869\n","epoch 0, step: 81600/127656, loss: 0.007837953045964241\n","epoch 0, step: 81610/127656, loss: 0.01062007062137127\n","epoch 0, step: 81620/127656, loss: 0.4136204123497009\n","epoch 0, step: 81630/127656, loss: 0.011179303750395775\n","epoch 0, step: 81640/127656, loss: 0.01416341494768858\n","epoch 0, step: 81650/127656, loss: 0.009591242298483849\n","epoch 0, step: 81660/127656, loss: 0.012761220335960388\n","epoch 0, step: 81670/127656, loss: 0.01433783769607544\n","epoch 0, step: 81680/127656, loss: 0.012254996225237846\n","epoch 0, step: 81690/127656, loss: 0.009294368326663971\n","epoch 0, step: 81700/127656, loss: 0.013188011944293976\n","epoch 0, step: 81710/127656, loss: 0.016340039670467377\n","epoch 0, step: 81720/127656, loss: 0.009578969329595566\n","epoch 0, step: 81730/127656, loss: 0.010886195115745068\n","epoch 0, step: 81740/127656, loss: 0.010286767035722733\n","epoch 0, step: 81750/127656, loss: 0.014446211978793144\n","epoch 0, step: 81760/127656, loss: 0.009111057966947556\n","epoch 0, step: 81770/127656, loss: 0.006651784293353558\n","epoch 0, step: 81780/127656, loss: 0.01247271429747343\n","epoch 0, step: 81790/127656, loss: 0.010491883382201195\n","epoch 0, step: 81800/127656, loss: 0.2723979353904724\n","epoch 0, step: 81810/127656, loss: 0.009680035524070263\n","epoch 0, step: 81820/127656, loss: 0.015160799026489258\n","epoch 0, step: 81830/127656, loss: 0.010187205858528614\n","epoch 0, step: 81840/127656, loss: 0.017369423061609268\n","epoch 0, step: 81850/127656, loss: 0.01144471950829029\n","epoch 0, step: 81860/127656, loss: 0.00639227544888854\n","epoch 0, step: 81870/127656, loss: 0.010308295488357544\n","epoch 0, step: 81880/127656, loss: 0.009868617169559002\n","epoch 0, step: 81890/127656, loss: 0.007913022302091122\n","epoch 0, step: 81900/127656, loss: 0.010765465907752514\n","epoch 0, step: 81910/127656, loss: 0.008732425980269909\n","epoch 0, step: 81920/127656, loss: 0.00954497791826725\n","epoch 0, step: 81930/127656, loss: 0.008313953876495361\n","epoch 0, step: 81940/127656, loss: 0.012877295725047588\n","epoch 0, step: 81950/127656, loss: 0.008946371264755726\n","epoch 0, step: 81960/127656, loss: 0.40648505091667175\n","epoch 0, step: 81970/127656, loss: 0.01336595043540001\n","epoch 0, step: 81980/127656, loss: 0.015115349553525448\n","epoch 0, step: 81990/127656, loss: 0.27650851011276245\n","epoch 0, step: 82000/127656, loss: 0.007674090564250946\n","epoch 0, step: 82010/127656, loss: 0.01130413543432951\n","epoch 0, step: 82020/127656, loss: 0.008551574312150478\n","epoch 0, step: 82030/127656, loss: 0.009062548168003559\n","epoch 0, step: 82040/127656, loss: 0.01256596576422453\n","epoch 0, step: 82050/127656, loss: 0.011986943893134594\n","epoch 0, step: 82060/127656, loss: 0.01108359731733799\n","epoch 0, step: 82070/127656, loss: 0.010205722413957119\n","epoch 0, step: 82080/127656, loss: 0.005763399414718151\n","epoch 0, step: 82090/127656, loss: 0.012592770159244537\n","epoch 0, step: 82100/127656, loss: 0.01325294654816389\n","epoch 0, step: 82110/127656, loss: 0.010940666310489178\n","epoch 0, step: 82120/127656, loss: 0.02132534049451351\n","epoch 0, step: 82130/127656, loss: 0.011706037446856499\n","epoch 0, step: 82140/127656, loss: 0.1485692411661148\n","epoch 0, step: 82150/127656, loss: 0.011340394616127014\n","epoch 0, step: 82160/127656, loss: 0.012387275695800781\n","epoch 0, step: 82170/127656, loss: 0.013923789374530315\n","epoch 0, step: 82180/127656, loss: 0.015796560794115067\n","epoch 0, step: 82190/127656, loss: 0.010936928912997246\n","epoch 0, step: 82200/127656, loss: 0.012676621787250042\n","epoch 0, step: 82210/127656, loss: 0.013242058455944061\n","epoch 0, step: 82220/127656, loss: 0.12796860933303833\n","epoch 0, step: 82230/127656, loss: 0.011358620598912239\n","epoch 0, step: 82240/127656, loss: 0.011196933686733246\n","epoch 0, step: 82250/127656, loss: 0.008680172264575958\n","epoch 0, step: 82260/127656, loss: 0.01293941494077444\n","epoch 0, step: 82270/127656, loss: 0.012806452810764313\n","epoch 0, step: 82280/127656, loss: 0.006242131348699331\n","epoch 0, step: 82290/127656, loss: 0.011469549499452114\n","epoch 0, step: 82300/127656, loss: 0.00922304019331932\n","epoch 0, step: 82310/127656, loss: 0.014542879536747932\n","epoch 0, step: 82320/127656, loss: 0.010478479787707329\n","epoch 0, step: 82330/127656, loss: 0.014207112602889538\n","epoch 0, step: 82340/127656, loss: 0.008482093922793865\n","epoch 0, step: 82350/127656, loss: 0.013170069083571434\n","epoch 0, step: 82360/127656, loss: 0.011233886703848839\n","epoch 0, step: 82370/127656, loss: 0.008280506357550621\n","epoch 0, step: 82380/127656, loss: 0.011508296243846416\n","epoch 0, step: 82390/127656, loss: 0.009763844311237335\n","epoch 0, step: 82400/127656, loss: 0.01779480278491974\n","epoch 0, step: 82410/127656, loss: 0.009324553422629833\n","epoch 0, step: 82420/127656, loss: 0.26227694749832153\n","epoch 0, step: 82430/127656, loss: 0.010293155908584595\n","epoch 0, step: 82440/127656, loss: 0.012462194077670574\n","epoch 0, step: 82450/127656, loss: 0.007622886449098587\n","epoch 0, step: 82460/127656, loss: 0.013996250927448273\n","epoch 0, step: 82470/127656, loss: 0.013783848844468594\n","epoch 0, step: 82480/127656, loss: 0.014743469655513763\n","epoch 0, step: 82490/127656, loss: 0.006912056356668472\n","epoch 0, step: 82500/127656, loss: 0.01602490246295929\n","epoch 0, step: 82510/127656, loss: 0.01287512294948101\n","epoch 0, step: 82520/127656, loss: 0.012969913892447948\n","epoch 0, step: 82530/127656, loss: 0.015468485653400421\n","epoch 0, step: 82540/127656, loss: 0.01444033533334732\n","epoch 0, step: 82550/127656, loss: 0.00996118038892746\n","epoch 0, step: 82560/127656, loss: 0.009267078712582588\n","epoch 0, step: 82570/127656, loss: 0.013030032627284527\n","epoch 0, step: 82580/127656, loss: 0.011469395831227303\n","epoch 0, step: 82590/127656, loss: 0.009967202320694923\n","epoch 0, step: 82600/127656, loss: 0.014197010546922684\n","epoch 0, step: 82610/127656, loss: 0.011409851722419262\n","epoch 0, step: 82620/127656, loss: 0.010096948593854904\n","epoch 0, step: 82630/127656, loss: 0.010541213676333427\n","epoch 0, step: 82640/127656, loss: 0.014693045988678932\n","epoch 0, step: 82650/127656, loss: 0.014977018348872662\n","epoch 0, step: 82660/127656, loss: 0.013727603480219841\n","epoch 0, step: 82670/127656, loss: 0.011360976845026016\n","epoch 0, step: 82680/127656, loss: 0.010662419721484184\n","epoch 0, step: 82690/127656, loss: 0.009369335137307644\n","epoch 0, step: 82700/127656, loss: 0.013300600461661816\n","epoch 0, step: 82710/127656, loss: 0.011298909783363342\n","epoch 0, step: 82720/127656, loss: 0.010624369606375694\n","epoch 0, step: 82730/127656, loss: 0.007998095825314522\n","epoch 0, step: 82740/127656, loss: 0.01432097889482975\n","epoch 0, step: 82750/127656, loss: 0.01824697107076645\n","epoch 0, step: 82760/127656, loss: 0.5259912610054016\n","epoch 0, step: 82770/127656, loss: 0.014471501111984253\n","epoch 0, step: 82780/127656, loss: 0.00898320134729147\n","epoch 0, step: 82790/127656, loss: 0.00863857101649046\n","epoch 0, step: 82800/127656, loss: 0.012295505031943321\n","epoch 0, step: 82810/127656, loss: 0.01130226906388998\n","epoch 0, step: 82820/127656, loss: 0.013238437473773956\n","epoch 0, step: 82830/127656, loss: 0.010814627632498741\n","epoch 0, step: 82840/127656, loss: 0.01018090546131134\n","epoch 0, step: 82850/127656, loss: 0.011471464298665524\n","epoch 0, step: 82860/127656, loss: 0.013407744467258453\n","epoch 0, step: 82870/127656, loss: 0.009118612855672836\n","epoch 0, step: 82880/127656, loss: 0.009196707047522068\n","epoch 0, step: 82890/127656, loss: 0.008727936074137688\n","epoch 0, step: 82900/127656, loss: 0.39584022760391235\n","epoch 0, step: 82910/127656, loss: 0.01563413254916668\n","epoch 0, step: 82920/127656, loss: 0.007941984571516514\n","epoch 0, step: 82930/127656, loss: 0.017522990703582764\n","epoch 0, step: 82940/127656, loss: 0.010153215378522873\n","epoch 0, step: 82950/127656, loss: 0.008387770503759384\n","epoch 0, step: 82960/127656, loss: 0.39576953649520874\n","epoch 0, step: 82970/127656, loss: 0.00970243476331234\n","epoch 0, step: 82980/127656, loss: 0.012280112132430077\n","epoch 0, step: 82990/127656, loss: 0.020150020718574524\n","epoch 0, step: 83000/127656, loss: 0.013137716799974442\n","epoch 0, step: 83010/127656, loss: 0.015806598588824272\n","epoch 0, step: 83020/127656, loss: 0.008153333328664303\n","epoch 0, step: 83030/127656, loss: 0.01224946603178978\n","epoch 0, step: 83040/127656, loss: 0.13938650488853455\n","epoch 0, step: 83050/127656, loss: 0.13425448536872864\n","epoch 0, step: 83060/127656, loss: 0.020212728530168533\n","epoch 0, step: 83070/127656, loss: 0.0057959118857979774\n","epoch 0, step: 83080/127656, loss: 0.009340270422399044\n","epoch 0, step: 83090/127656, loss: 0.011246033012866974\n","epoch 0, step: 83100/127656, loss: 0.010017498396337032\n","epoch 0, step: 83110/127656, loss: 0.011047023348510265\n","epoch 0, step: 83120/127656, loss: 0.01076442375779152\n","epoch 0, step: 83130/127656, loss: 0.010798316448926926\n","epoch 0, step: 83140/127656, loss: 0.00858234241604805\n","epoch 0, step: 83150/127656, loss: 0.012025284580886364\n","epoch 0, step: 83160/127656, loss: 0.012878223322331905\n","epoch 0, step: 83170/127656, loss: 0.014305993914604187\n","epoch 0, step: 83180/127656, loss: 0.014330321922898293\n","epoch 0, step: 83190/127656, loss: 0.010641047731041908\n","epoch 0, step: 83200/127656, loss: 0.007289757486432791\n","epoch 0, step: 83210/127656, loss: 0.021457817405462265\n","epoch 0, step: 83220/127656, loss: 0.010839085094630718\n","epoch 0, step: 83230/127656, loss: 0.013369503431022167\n","epoch 0, step: 83240/127656, loss: 0.012451167218387127\n","epoch 0, step: 83250/127656, loss: 0.00949130393564701\n","epoch 0, step: 83260/127656, loss: 0.012920835986733437\n","epoch 0, step: 83270/127656, loss: 0.014460733160376549\n","epoch 0, step: 83280/127656, loss: 0.009117938578128815\n","epoch 0, step: 83290/127656, loss: 0.01246910821646452\n","epoch 0, step: 83300/127656, loss: 0.011472980491816998\n","epoch 0, step: 83310/127656, loss: 0.011219710111618042\n","epoch 0, step: 83320/127656, loss: 0.01051042228937149\n","epoch 0, step: 83330/127656, loss: 0.009873962961137295\n","epoch 0, step: 83340/127656, loss: 0.011787228286266327\n","epoch 0, step: 83350/127656, loss: 0.009615253657102585\n","epoch 0, step: 83360/127656, loss: 0.00799507088959217\n","epoch 0, step: 83370/127656, loss: 0.009683111682534218\n","epoch 0, step: 83380/127656, loss: 0.01197111513465643\n","epoch 0, step: 83390/127656, loss: 0.012637622654438019\n","epoch 0, step: 83400/127656, loss: 0.01606850139796734\n","epoch 0, step: 83410/127656, loss: 0.015307769179344177\n","epoch 0, step: 83420/127656, loss: 0.009656460955739021\n","epoch 0, step: 83430/127656, loss: 0.0081561841070652\n","epoch 0, step: 83440/127656, loss: 0.4114863872528076\n","epoch 0, step: 83450/127656, loss: 0.0059423549100756645\n","epoch 0, step: 83460/127656, loss: 0.009357953444123268\n","epoch 0, step: 83470/127656, loss: 0.005083658266812563\n","epoch 0, step: 83480/127656, loss: 0.00926396157592535\n","epoch 0, step: 83490/127656, loss: 0.013187977485358715\n","epoch 0, step: 83500/127656, loss: 0.007518378086388111\n","epoch 0, step: 83510/127656, loss: 0.015528278425335884\n","epoch 0, step: 83520/127656, loss: 0.011360581032931805\n","epoch 0, step: 83530/127656, loss: 0.009041902609169483\n","epoch 0, step: 83540/127656, loss: 0.012803779914975166\n","epoch 0, step: 83550/127656, loss: 0.011122986674308777\n","epoch 0, step: 83560/127656, loss: 0.008721481077373028\n","epoch 0, step: 83570/127656, loss: 0.013859212398529053\n","epoch 0, step: 83580/127656, loss: 0.13516561686992645\n","epoch 0, step: 83590/127656, loss: 0.007607938721776009\n","epoch 0, step: 83600/127656, loss: 0.011899583041667938\n","epoch 0, step: 83610/127656, loss: 0.011682160198688507\n","epoch 0, step: 83620/127656, loss: 0.009860165417194366\n","epoch 0, step: 83630/127656, loss: 0.012135017663240433\n","epoch 0, step: 83640/127656, loss: 0.010271101258695126\n","epoch 0, step: 83650/127656, loss: 0.4086439609527588\n","epoch 0, step: 83660/127656, loss: 0.00836189091205597\n","epoch 0, step: 83670/127656, loss: 0.007819540798664093\n","epoch 0, step: 83680/127656, loss: 0.006035039201378822\n","epoch 0, step: 83690/127656, loss: 0.010119800455868244\n","epoch 0, step: 83700/127656, loss: 0.011364090256392956\n","epoch 0, step: 83710/127656, loss: 0.014423955231904984\n","epoch 0, step: 83720/127656, loss: 0.14320290088653564\n","epoch 0, step: 83730/127656, loss: 0.015886392444372177\n","epoch 0, step: 83740/127656, loss: 0.1441657543182373\n","epoch 0, step: 83750/127656, loss: 0.13530656695365906\n","epoch 0, step: 83760/127656, loss: 0.010454362258315086\n","epoch 0, step: 83770/127656, loss: 0.006876954343169928\n","epoch 0, step: 83780/127656, loss: 0.008548391982913017\n","epoch 0, step: 83790/127656, loss: 0.00743873929604888\n","epoch 0, step: 83800/127656, loss: 0.2646660804748535\n","epoch 0, step: 83810/127656, loss: 0.01052176021039486\n","epoch 0, step: 83820/127656, loss: 0.012479819357395172\n","epoch 0, step: 83830/127656, loss: 0.007902106270194054\n","epoch 0, step: 83840/127656, loss: 0.014024412259459496\n","epoch 0, step: 83850/127656, loss: 0.013252725824713707\n","epoch 0, step: 83860/127656, loss: 0.008757811039686203\n","epoch 0, step: 83870/127656, loss: 0.012863529846072197\n","epoch 0, step: 83880/127656, loss: 0.015755008906126022\n","epoch 0, step: 83890/127656, loss: 0.009260481223464012\n","epoch 0, step: 83900/127656, loss: 0.008953104726970196\n","epoch 0, step: 83910/127656, loss: 0.13911668956279755\n","epoch 0, step: 83920/127656, loss: 0.01324932835996151\n","epoch 0, step: 83930/127656, loss: 0.012910909950733185\n","epoch 0, step: 83940/127656, loss: 0.011897251009941101\n","epoch 0, step: 83950/127656, loss: 0.012313301675021648\n","epoch 0, step: 83960/127656, loss: 0.008693674579262733\n","epoch 0, step: 83970/127656, loss: 0.007959868758916855\n","epoch 0, step: 83980/127656, loss: 0.007675012573599815\n","epoch 0, step: 83990/127656, loss: 0.007702467497438192\n","epoch 0, step: 84000/127656, loss: 0.013450190424919128\n","epoch 0, step: 84010/127656, loss: 0.2855663597583771\n","epoch 0, step: 84020/127656, loss: 0.3873178958892822\n","epoch 0, step: 84030/127656, loss: 0.01395890861749649\n","epoch 0, step: 84040/127656, loss: 0.009989025071263313\n","epoch 0, step: 84050/127656, loss: 0.010984647087752819\n","epoch 0, step: 84060/127656, loss: 0.01618768647313118\n","epoch 0, step: 84070/127656, loss: 0.012598609551787376\n","epoch 0, step: 84080/127656, loss: 0.016712060198187828\n","epoch 0, step: 84090/127656, loss: 0.012467860244214535\n","epoch 0, step: 84100/127656, loss: 0.008214116096496582\n","epoch 0, step: 84110/127656, loss: 0.013232378289103508\n","epoch 0, step: 84120/127656, loss: 0.008513102307915688\n","epoch 0, step: 84130/127656, loss: 0.01417393609881401\n","epoch 0, step: 84140/127656, loss: 0.012344365939497948\n","epoch 0, step: 84150/127656, loss: 0.010098117403686047\n","epoch 0, step: 84160/127656, loss: 0.008945810608565807\n","epoch 0, step: 84170/127656, loss: 0.011521326377987862\n","epoch 0, step: 84180/127656, loss: 0.012253038585186005\n","epoch 0, step: 84190/127656, loss: 0.4092833399772644\n","epoch 0, step: 84200/127656, loss: 0.008706651628017426\n","epoch 0, step: 84210/127656, loss: 0.011142352595925331\n","epoch 0, step: 84220/127656, loss: 0.2595127820968628\n","epoch 0, step: 84230/127656, loss: 0.013454767875373363\n","epoch 0, step: 84240/127656, loss: 0.3908647894859314\n","epoch 0, step: 84250/127656, loss: 0.0205092653632164\n","epoch 0, step: 84260/127656, loss: 0.009276105090975761\n","epoch 0, step: 84270/127656, loss: 0.01733909174799919\n","epoch 0, step: 84280/127656, loss: 0.013539008796215057\n","epoch 0, step: 84290/127656, loss: 0.014266932383179665\n","epoch 0, step: 84300/127656, loss: 0.013199868611991405\n","epoch 0, step: 84310/127656, loss: 0.3614268898963928\n","epoch 0, step: 84320/127656, loss: 0.0060737780295312405\n","epoch 0, step: 84330/127656, loss: 0.014703527092933655\n","epoch 0, step: 84340/127656, loss: 0.008958699181675911\n","epoch 0, step: 84350/127656, loss: 0.00992531143128872\n","epoch 0, step: 84360/127656, loss: 0.011371981352567673\n","epoch 0, step: 84370/127656, loss: 0.011350839398801327\n","epoch 0, step: 84380/127656, loss: 0.012253843247890472\n","epoch 0, step: 84390/127656, loss: 0.00972520001232624\n","epoch 0, step: 84400/127656, loss: 0.012668034061789513\n","epoch 0, step: 84410/127656, loss: 0.3963443636894226\n","epoch 0, step: 84420/127656, loss: 0.011782292276620865\n","epoch 0, step: 84430/127656, loss: 0.010803250595927238\n","epoch 0, step: 84440/127656, loss: 0.12793880701065063\n","epoch 0, step: 84450/127656, loss: 0.007498030550777912\n","epoch 0, step: 84460/127656, loss: 0.012281743809580803\n","epoch 0, step: 84470/127656, loss: 0.010906639508903027\n","epoch 0, step: 84480/127656, loss: 0.010609718970954418\n","epoch 0, step: 84490/127656, loss: 0.011431863531470299\n","epoch 0, step: 84500/127656, loss: 0.011596194468438625\n","epoch 0, step: 84510/127656, loss: 0.009148336946964264\n","epoch 0, step: 84520/127656, loss: 0.008200051262974739\n","epoch 0, step: 84530/127656, loss: 0.013923244550824165\n","epoch 0, step: 84540/127656, loss: 0.011724724434316158\n","epoch 0, step: 84550/127656, loss: 0.01183791272342205\n","epoch 0, step: 84560/127656, loss: 0.006049219518899918\n","epoch 0, step: 84570/127656, loss: 0.013635668903589249\n","epoch 0, step: 84580/127656, loss: 0.013397300615906715\n","epoch 0, step: 84590/127656, loss: 0.009545456618070602\n","epoch 0, step: 84600/127656, loss: 0.01301135029643774\n","epoch 0, step: 84610/127656, loss: 0.010079748928546906\n","epoch 0, step: 84620/127656, loss: 0.012555014342069626\n","epoch 0, step: 84630/127656, loss: 0.005483672954142094\n","epoch 0, step: 84640/127656, loss: 0.010114483535289764\n","epoch 0, step: 84650/127656, loss: 0.1243540346622467\n","epoch 0, step: 84660/127656, loss: 0.010373920202255249\n","epoch 0, step: 84670/127656, loss: 0.011710206046700478\n","epoch 0, step: 84680/127656, loss: 0.009925076737999916\n","epoch 0, step: 84690/127656, loss: 0.013217732310295105\n","epoch 0, step: 84700/127656, loss: 0.012270258739590645\n","epoch 0, step: 84710/127656, loss: 0.011943681165575981\n","epoch 0, step: 84720/127656, loss: 0.12843278050422668\n","epoch 0, step: 84730/127656, loss: 0.013547047041356564\n","epoch 0, step: 84740/127656, loss: 0.010641390457749367\n","epoch 0, step: 84750/127656, loss: 0.011790025047957897\n","epoch 0, step: 84760/127656, loss: 0.009108327329158783\n","epoch 0, step: 84770/127656, loss: 0.014436080120503902\n","epoch 0, step: 84780/127656, loss: 0.011453136801719666\n","epoch 0, step: 84790/127656, loss: 0.010431786999106407\n","epoch 0, step: 84800/127656, loss: 0.011930493637919426\n","epoch 0, step: 84810/127656, loss: 0.1338932067155838\n","epoch 0, step: 84820/127656, loss: 0.40098655223846436\n","epoch 0, step: 84830/127656, loss: 0.015148011967539787\n","epoch 0, step: 84840/127656, loss: 0.015419729053974152\n","epoch 0, step: 84850/127656, loss: 0.008076286874711514\n","epoch 0, step: 84860/127656, loss: 0.010159140452742577\n","epoch 0, step: 84870/127656, loss: 0.006661792751401663\n","epoch 0, step: 84880/127656, loss: 0.01090819388628006\n","epoch 0, step: 84890/127656, loss: 0.012901744805276394\n","epoch 0, step: 84900/127656, loss: 0.011144640855491161\n","epoch 0, step: 84910/127656, loss: 0.006050863768905401\n","epoch 0, step: 84920/127656, loss: 0.01007494144141674\n","epoch 0, step: 84930/127656, loss: 0.011611211113631725\n","epoch 0, step: 84940/127656, loss: 0.008311264216899872\n","epoch 0, step: 84950/127656, loss: 0.009799301624298096\n","epoch 0, step: 84960/127656, loss: 0.13754604756832123\n","epoch 0, step: 84970/127656, loss: 0.5121687650680542\n","epoch 0, step: 84980/127656, loss: 0.010229645296931267\n","epoch 0, step: 84990/127656, loss: 0.41717132925987244\n","epoch 0, step: 85000/127656, loss: 0.014856569468975067\n","epoch 0, step: 85010/127656, loss: 0.010811084881424904\n","epoch 0, step: 85020/127656, loss: 0.01136268861591816\n","epoch 0, step: 85030/127656, loss: 0.011787928640842438\n","epoch 0, step: 85040/127656, loss: 0.016804132610559464\n","epoch 0, step: 85050/127656, loss: 0.013590119779109955\n","epoch 0, step: 85060/127656, loss: 0.007893133908510208\n","epoch 0, step: 85070/127656, loss: 0.009712127968668938\n","epoch 0, step: 85080/127656, loss: 0.010104267857968807\n","epoch 0, step: 85090/127656, loss: 0.00817993376404047\n","epoch 0, step: 85100/127656, loss: 0.010591680184006691\n","epoch 0, step: 85110/127656, loss: 0.009956533089280128\n","epoch 0, step: 85120/127656, loss: 0.009497701190412045\n","epoch 0, step: 85130/127656, loss: 0.018974721431732178\n","epoch 0, step: 85140/127656, loss: 0.010485399514436722\n","epoch 0, step: 85150/127656, loss: 0.26641660928726196\n","epoch 0, step: 85160/127656, loss: 0.012128705158829689\n","epoch 0, step: 85170/127656, loss: 0.01089803408831358\n","epoch 0, step: 85180/127656, loss: 0.009335082024335861\n","epoch 0, step: 85190/127656, loss: 0.009599375538527966\n","epoch 0, step: 85200/127656, loss: 0.00826039258390665\n","epoch 0, step: 85210/127656, loss: 0.012887101620435715\n","epoch 0, step: 85220/127656, loss: 0.01127446349710226\n","epoch 0, step: 85230/127656, loss: 0.011663494631648064\n","epoch 0, step: 85240/127656, loss: 0.009445026516914368\n","epoch 0, step: 85250/127656, loss: 0.010489268228411674\n","epoch 0, step: 85260/127656, loss: 0.008922485634684563\n","epoch 0, step: 85270/127656, loss: 0.00914094503968954\n","epoch 0, step: 85280/127656, loss: 0.013813884928822517\n","epoch 0, step: 85290/127656, loss: 0.006766567938029766\n","epoch 0, step: 85300/127656, loss: 0.015471242368221283\n","epoch 0, step: 85310/127656, loss: 0.014135327190160751\n","epoch 0, step: 85320/127656, loss: 0.012111574411392212\n","epoch 0, step: 85330/127656, loss: 0.015275541692972183\n","epoch 0, step: 85340/127656, loss: 0.27702125906944275\n","epoch 0, step: 85350/127656, loss: 0.013141939416527748\n","epoch 0, step: 85360/127656, loss: 0.01333714835345745\n","epoch 0, step: 85370/127656, loss: 0.007472757250070572\n","epoch 0, step: 85380/127656, loss: 0.009409843012690544\n","epoch 0, step: 85390/127656, loss: 0.009622853249311447\n","epoch 0, step: 85400/127656, loss: 0.010151701979339123\n","epoch 0, step: 85410/127656, loss: 0.0073289647698402405\n","epoch 0, step: 85420/127656, loss: 0.011442124843597412\n","epoch 0, step: 85430/127656, loss: 0.2701111137866974\n","epoch 0, step: 85440/127656, loss: 0.008515979163348675\n","epoch 0, step: 85450/127656, loss: 0.3936646580696106\n","epoch 0, step: 85460/127656, loss: 0.12710043787956238\n","epoch 0, step: 85470/127656, loss: 0.010981510393321514\n","epoch 0, step: 85480/127656, loss: 0.01392508577555418\n","epoch 0, step: 85490/127656, loss: 0.008693862706422806\n","epoch 0, step: 85500/127656, loss: 0.009056711569428444\n","epoch 0, step: 85510/127656, loss: 0.005690691061317921\n","epoch 0, step: 85520/127656, loss: 0.006140746176242828\n","epoch 0, step: 85530/127656, loss: 0.011320634745061398\n","epoch 0, step: 85540/127656, loss: 0.014950166456401348\n","epoch 0, step: 85550/127656, loss: 0.010931393131613731\n","epoch 0, step: 85560/127656, loss: 0.008715043775737286\n","epoch 0, step: 85570/127656, loss: 0.011176428757607937\n","epoch 0, step: 85580/127656, loss: 0.014412540942430496\n","epoch 0, step: 85590/127656, loss: 0.010145510546863079\n","epoch 0, step: 85600/127656, loss: 0.014382022432982922\n","epoch 0, step: 85610/127656, loss: 0.011989830061793327\n","epoch 0, step: 85620/127656, loss: 0.014195360243320465\n","epoch 0, step: 85630/127656, loss: 0.009339991956949234\n","epoch 0, step: 85640/127656, loss: 0.011402532458305359\n","epoch 0, step: 85650/127656, loss: 0.014243426732718945\n","epoch 0, step: 85660/127656, loss: 0.011331102810800076\n","epoch 0, step: 85670/127656, loss: 0.011511172167956829\n","epoch 0, step: 85680/127656, loss: 0.009445389732718468\n","epoch 0, step: 85690/127656, loss: 0.13156040012836456\n","epoch 0, step: 85700/127656, loss: 0.011821000836789608\n","epoch 0, step: 85710/127656, loss: 0.012838402763009071\n","epoch 0, step: 85720/127656, loss: 0.013736983761191368\n","epoch 0, step: 85730/127656, loss: 0.008128700777888298\n","epoch 0, step: 85740/127656, loss: 0.009558476507663727\n","epoch 0, step: 85750/127656, loss: 0.016880277544260025\n","epoch 0, step: 85760/127656, loss: 0.007917160168290138\n","epoch 0, step: 85770/127656, loss: 0.014287104830145836\n","epoch 0, step: 85780/127656, loss: 0.25020667910575867\n","epoch 0, step: 85790/127656, loss: 0.007376017048954964\n","epoch 0, step: 85800/127656, loss: 0.012788220308721066\n","epoch 0, step: 85810/127656, loss: 0.010080218315124512\n","epoch 0, step: 85820/127656, loss: 0.008277006447315216\n","epoch 0, step: 85830/127656, loss: 0.007886259816586971\n","epoch 0, step: 85840/127656, loss: 0.010495822876691818\n","epoch 0, step: 85850/127656, loss: 0.011090332642197609\n","epoch 0, step: 85860/127656, loss: 0.015029028058052063\n","epoch 0, step: 85870/127656, loss: 0.011172501370310783\n","epoch 0, step: 85880/127656, loss: 0.00723958108574152\n","epoch 0, step: 85890/127656, loss: 0.13080446422100067\n","epoch 0, step: 85900/127656, loss: 0.006773896515369415\n","epoch 0, step: 85910/127656, loss: 0.013727678917348385\n","epoch 0, step: 85920/127656, loss: 0.013448541983962059\n","epoch 0, step: 85930/127656, loss: 0.3969208896160126\n","epoch 0, step: 85940/127656, loss: 0.00744748255237937\n","epoch 0, step: 85950/127656, loss: 0.010363822802901268\n","epoch 0, step: 85960/127656, loss: 0.008755096234381199\n","epoch 0, step: 85970/127656, loss: 0.012893701903522015\n","epoch 0, step: 85980/127656, loss: 0.6916396021842957\n","epoch 0, step: 85990/127656, loss: 0.011644656769931316\n","epoch 0, step: 86000/127656, loss: 0.010241731069982052\n","epoch 0, step: 86010/127656, loss: 0.01049977820366621\n","epoch 0, step: 86020/127656, loss: 0.012432888150215149\n","epoch 0, step: 86030/127656, loss: 0.008949256502091885\n","epoch 0, step: 86040/127656, loss: 0.011052398942410946\n","epoch 0, step: 86050/127656, loss: 0.011458838358521461\n","epoch 0, step: 86060/127656, loss: 0.009554633870720863\n","epoch 0, step: 86070/127656, loss: 0.013873916119337082\n","epoch 0, step: 86080/127656, loss: 0.009594894014298916\n","epoch 0, step: 86090/127656, loss: 0.014372508972883224\n","epoch 0, step: 86100/127656, loss: 0.012154808267951012\n","epoch 0, step: 86110/127656, loss: 0.13500642776489258\n","epoch 0, step: 86120/127656, loss: 0.3748818337917328\n","epoch 0, step: 86130/127656, loss: 0.013145467266440392\n","epoch 0, step: 86140/127656, loss: 0.012839491479098797\n","epoch 0, step: 86150/127656, loss: 0.022586744278669357\n","epoch 0, step: 86160/127656, loss: 0.02192613296210766\n","epoch 0, step: 86170/127656, loss: 0.012921977788209915\n","epoch 0, step: 86180/127656, loss: 0.010556250810623169\n","epoch 0, step: 86190/127656, loss: 0.008169354870915413\n","epoch 0, step: 86200/127656, loss: 0.015483736991882324\n","epoch 0, step: 86210/127656, loss: 0.013591828756034374\n","epoch 0, step: 86220/127656, loss: 0.007845637388527393\n","epoch 0, step: 86230/127656, loss: 0.006158368196338415\n","epoch 0, step: 86240/127656, loss: 0.009842580184340477\n","epoch 0, step: 86250/127656, loss: 0.01119792740792036\n","epoch 0, step: 86260/127656, loss: 0.008711183443665504\n","epoch 0, step: 86270/127656, loss: 0.007626928389072418\n","epoch 0, step: 86280/127656, loss: 0.008549407124519348\n","epoch 0, step: 86290/127656, loss: 0.13980546593666077\n","epoch 0, step: 86300/127656, loss: 0.011306174099445343\n","epoch 0, step: 86310/127656, loss: 0.008675791323184967\n","epoch 0, step: 86320/127656, loss: 0.01412989106029272\n","epoch 0, step: 86330/127656, loss: 0.018925640732049942\n","epoch 0, step: 86340/127656, loss: 0.01215450745075941\n","epoch 0, step: 86350/127656, loss: 0.5439563989639282\n","epoch 0, step: 86360/127656, loss: 0.007582391612231731\n","epoch 0, step: 86370/127656, loss: 0.015382467769086361\n","epoch 0, step: 86380/127656, loss: 0.5288326144218445\n","epoch 0, step: 86390/127656, loss: 0.01884070783853531\n","epoch 0, step: 86400/127656, loss: 0.009548251517117023\n","epoch 0, step: 86410/127656, loss: 0.011770486831665039\n","epoch 0, step: 86420/127656, loss: 0.2674596905708313\n","epoch 0, step: 86430/127656, loss: 0.0064688269048929214\n","epoch 0, step: 86440/127656, loss: 0.008663032203912735\n","epoch 0, step: 86450/127656, loss: 0.012791010551154613\n","epoch 0, step: 86460/127656, loss: 0.27152127027511597\n","epoch 0, step: 86470/127656, loss: 0.016053752973675728\n","epoch 0, step: 86480/127656, loss: 0.010056903585791588\n","epoch 0, step: 86490/127656, loss: 0.00884825736284256\n","epoch 0, step: 86500/127656, loss: 0.01389844249933958\n","epoch 0, step: 86510/127656, loss: 0.015252037905156612\n","epoch 0, step: 86520/127656, loss: 0.015259495005011559\n","epoch 0, step: 86530/127656, loss: 0.01686062291264534\n","epoch 0, step: 86540/127656, loss: 0.009634722024202347\n","epoch 0, step: 86550/127656, loss: 0.013930296525359154\n","epoch 0, step: 86560/127656, loss: 0.006764502730220556\n","epoch 0, step: 86570/127656, loss: 0.011634677648544312\n","epoch 0, step: 86580/127656, loss: 0.009653545916080475\n","epoch 0, step: 86590/127656, loss: 0.01054239273071289\n","epoch 0, step: 86600/127656, loss: 0.017524179071187973\n","epoch 0, step: 86610/127656, loss: 0.3975214958190918\n","epoch 0, step: 86620/127656, loss: 0.00765609135851264\n","epoch 0, step: 86630/127656, loss: 0.008877167478203773\n","epoch 0, step: 86640/127656, loss: 0.0068731121718883514\n","epoch 0, step: 86650/127656, loss: 0.012845559045672417\n","epoch 0, step: 86660/127656, loss: 0.009207094088196754\n","epoch 0, step: 86670/127656, loss: 0.014399274252355099\n","epoch 0, step: 86680/127656, loss: 0.01028367131948471\n","epoch 0, step: 86690/127656, loss: 0.006477612070739269\n","epoch 0, step: 86700/127656, loss: 0.012131216935813427\n","epoch 0, step: 86710/127656, loss: 0.009382902644574642\n","epoch 0, step: 86720/127656, loss: 0.018050406128168106\n","epoch 0, step: 86730/127656, loss: 0.01025022380053997\n","epoch 0, step: 86740/127656, loss: 0.007981078699231148\n","epoch 0, step: 86750/127656, loss: 0.010086297988891602\n","epoch 0, step: 86760/127656, loss: 0.009815596975386143\n","epoch 0, step: 86770/127656, loss: 0.020769745111465454\n","epoch 0, step: 86780/127656, loss: 0.007784787565469742\n","epoch 0, step: 86790/127656, loss: 0.006736062467098236\n","epoch 0, step: 86800/127656, loss: 0.010169983841478825\n","epoch 0, step: 86810/127656, loss: 0.013836185447871685\n","epoch 0, step: 86820/127656, loss: 0.006637775804847479\n","epoch 0, step: 86830/127656, loss: 0.13181236386299133\n","epoch 0, step: 86840/127656, loss: 0.24641363322734833\n","epoch 0, step: 86850/127656, loss: 0.008834843523800373\n","epoch 0, step: 86860/127656, loss: 0.015878727659583092\n","epoch 0, step: 86870/127656, loss: 0.0108352554962039\n","epoch 0, step: 86880/127656, loss: 0.008717121556401253\n","epoch 0, step: 86890/127656, loss: 0.01024050172418356\n","epoch 0, step: 86900/127656, loss: 0.01387475710362196\n","epoch 0, step: 86910/127656, loss: 0.009842894971370697\n","epoch 0, step: 86920/127656, loss: 0.01172966044396162\n","epoch 0, step: 86930/127656, loss: 0.008489628322422504\n","epoch 0, step: 86940/127656, loss: 0.007303627207875252\n","epoch 0, step: 86950/127656, loss: 0.013486424461007118\n","epoch 0, step: 86960/127656, loss: 0.009259161539375782\n","epoch 0, step: 86970/127656, loss: 0.012310508638620377\n","epoch 0, step: 86980/127656, loss: 0.0055546266958117485\n","epoch 0, step: 86990/127656, loss: 0.008160721510648727\n","epoch 0, step: 87000/127656, loss: 0.014242108911275864\n","epoch 0, step: 87010/127656, loss: 0.009981995448470116\n","epoch 0, step: 87020/127656, loss: 0.2754310369491577\n","epoch 0, step: 87030/127656, loss: 0.01388549618422985\n","epoch 0, step: 87040/127656, loss: 0.01361849531531334\n","epoch 0, step: 87050/127656, loss: 0.013415070250630379\n","epoch 0, step: 87060/127656, loss: 0.009324684739112854\n","epoch 0, step: 87070/127656, loss: 0.011671065352857113\n","epoch 0, step: 87080/127656, loss: 0.020978188142180443\n","epoch 0, step: 87090/127656, loss: 0.011989796534180641\n","epoch 0, step: 87100/127656, loss: 0.005903351120650768\n","epoch 0, step: 87110/127656, loss: 0.010160181671380997\n","epoch 0, step: 87120/127656, loss: 0.009041178971529007\n","epoch 0, step: 87130/127656, loss: 0.008411278948187828\n","epoch 0, step: 87140/127656, loss: 0.4126596450805664\n","epoch 0, step: 87150/127656, loss: 0.00819425843656063\n","epoch 0, step: 87160/127656, loss: 0.007256352808326483\n","epoch 0, step: 87170/127656, loss: 0.013880064710974693\n","epoch 0, step: 87180/127656, loss: 0.013569541275501251\n","epoch 0, step: 87190/127656, loss: 0.008963502943515778\n","epoch 0, step: 87200/127656, loss: 0.011266449466347694\n","epoch 0, step: 87210/127656, loss: 0.009049586951732635\n","epoch 0, step: 87220/127656, loss: 0.0070009357295930386\n","epoch 0, step: 87230/127656, loss: 0.00721855740994215\n","epoch 0, step: 87240/127656, loss: 0.012373343110084534\n","epoch 0, step: 87250/127656, loss: 0.013121902011334896\n","epoch 0, step: 87260/127656, loss: 0.008943776600062847\n","epoch 0, step: 87270/127656, loss: 0.010142137296497822\n","epoch 0, step: 87280/127656, loss: 0.012266556732356548\n","epoch 0, step: 87290/127656, loss: 0.008572516962885857\n","epoch 0, step: 87300/127656, loss: 0.010430041700601578\n","epoch 0, step: 87310/127656, loss: 0.007599851116538048\n","epoch 0, step: 87320/127656, loss: 0.009332234039902687\n","epoch 0, step: 87330/127656, loss: 0.007725957781076431\n","epoch 0, step: 87340/127656, loss: 0.02284892462193966\n","epoch 0, step: 87350/127656, loss: 0.009226874448359013\n","epoch 0, step: 87360/127656, loss: 0.009656265377998352\n","epoch 0, step: 87370/127656, loss: 0.00831509567797184\n","epoch 0, step: 87380/127656, loss: 0.015583233907818794\n","epoch 0, step: 87390/127656, loss: 0.009214109741151333\n","epoch 0, step: 87400/127656, loss: 0.015033412724733353\n","epoch 0, step: 87410/127656, loss: 0.013266671448946\n","epoch 0, step: 87420/127656, loss: 0.00954408012330532\n","epoch 0, step: 87430/127656, loss: 0.007239179220050573\n","epoch 0, step: 87440/127656, loss: 0.010454872623085976\n","epoch 0, step: 87450/127656, loss: 0.008613279089331627\n","epoch 0, step: 87460/127656, loss: 0.012698105536401272\n","epoch 0, step: 87470/127656, loss: 0.6654844880104065\n","epoch 0, step: 87480/127656, loss: 0.012677873484790325\n","epoch 0, step: 87490/127656, loss: 0.015470634214580059\n","epoch 0, step: 87500/127656, loss: 0.010877053253352642\n","epoch 0, step: 87510/127656, loss: 0.007525123655796051\n","epoch 0, step: 87520/127656, loss: 0.009274125099182129\n","epoch 0, step: 87530/127656, loss: 0.00965058896690607\n","epoch 0, step: 87540/127656, loss: 0.008693769574165344\n","epoch 0, step: 87550/127656, loss: 0.007326443679630756\n","epoch 0, step: 87560/127656, loss: 0.012441657483577728\n","epoch 0, step: 87570/127656, loss: 0.010739047080278397\n","epoch 0, step: 87580/127656, loss: 0.007800455205142498\n","epoch 0, step: 87590/127656, loss: 0.014977688901126385\n","epoch 0, step: 87600/127656, loss: 0.01406954787671566\n","epoch 0, step: 87610/127656, loss: 0.011610396206378937\n","epoch 0, step: 87620/127656, loss: 0.009746059775352478\n","epoch 0, step: 87630/127656, loss: 0.009828826412558556\n","epoch 0, step: 87640/127656, loss: 0.5263503789901733\n","epoch 0, step: 87650/127656, loss: 0.38947421312332153\n","epoch 0, step: 87660/127656, loss: 0.011179531924426556\n","epoch 0, step: 87670/127656, loss: 0.007642029784619808\n","epoch 0, step: 87680/127656, loss: 0.014281867071986198\n","epoch 0, step: 87690/127656, loss: 0.00840814970433712\n","epoch 0, step: 87700/127656, loss: 0.007200266234576702\n","epoch 0, step: 87710/127656, loss: 0.013192668557167053\n","epoch 0, step: 87720/127656, loss: 0.011166593991219997\n","epoch 0, step: 87730/127656, loss: 0.009407006204128265\n","epoch 0, step: 87740/127656, loss: 0.006720162462443113\n","epoch 0, step: 87750/127656, loss: 0.01409873366355896\n","epoch 0, step: 87760/127656, loss: 0.01000131480395794\n","epoch 0, step: 87770/127656, loss: 0.270542711019516\n","epoch 0, step: 87780/127656, loss: 0.015447570942342281\n","epoch 0, step: 87790/127656, loss: 0.01549021527171135\n","epoch 0, step: 87800/127656, loss: 0.012632458470761776\n","epoch 0, step: 87810/127656, loss: 0.0072946129366755486\n","epoch 0, step: 87820/127656, loss: 0.009446581825613976\n","epoch 0, step: 87830/127656, loss: 0.013694996014237404\n","epoch 0, step: 87840/127656, loss: 0.009738881140947342\n","epoch 0, step: 87850/127656, loss: 0.011280020698904991\n","epoch 0, step: 87860/127656, loss: 0.010076494887471199\n","epoch 0, step: 87870/127656, loss: 0.008444882929325104\n","epoch 0, step: 87880/127656, loss: 0.013277478516101837\n","epoch 0, step: 87890/127656, loss: 0.008494578301906586\n","epoch 0, step: 87900/127656, loss: 0.013424083590507507\n","epoch 0, step: 87910/127656, loss: 0.009724125266075134\n","epoch 0, step: 87920/127656, loss: 0.017175232991576195\n","epoch 0, step: 87930/127656, loss: 0.1466405838727951\n","epoch 0, step: 87940/127656, loss: 0.01221926137804985\n","epoch 0, step: 87950/127656, loss: 0.012879997491836548\n","epoch 0, step: 87960/127656, loss: 0.016206864267587662\n","epoch 0, step: 87970/127656, loss: 0.006450945511460304\n","epoch 0, step: 87980/127656, loss: 0.011610198765993118\n","epoch 0, step: 87990/127656, loss: 0.012548111379146576\n","epoch 0, step: 88000/127656, loss: 0.0118555948138237\n","epoch 0, step: 88010/127656, loss: 0.009260392747819424\n","epoch 0, step: 88020/127656, loss: 0.009209880605340004\n","epoch 0, step: 88030/127656, loss: 0.009047240950167179\n","epoch 0, step: 88040/127656, loss: 0.00922844372689724\n","epoch 0, step: 88050/127656, loss: 0.1316225379705429\n","epoch 0, step: 88060/127656, loss: 0.00717241270467639\n","epoch 0, step: 88070/127656, loss: 0.010069681331515312\n","epoch 0, step: 88080/127656, loss: 0.007515443488955498\n","epoch 0, step: 88090/127656, loss: 0.010262932628393173\n","epoch 0, step: 88100/127656, loss: 0.00946168601512909\n","epoch 0, step: 88110/127656, loss: 0.01076640747487545\n","epoch 0, step: 88120/127656, loss: 0.007336371578276157\n","epoch 0, step: 88130/127656, loss: 0.008256695233285427\n","epoch 0, step: 88140/127656, loss: 0.013724178075790405\n","epoch 0, step: 88150/127656, loss: 0.0096976887434721\n","epoch 0, step: 88160/127656, loss: 0.008955521509051323\n","epoch 0, step: 88170/127656, loss: 0.012515930458903313\n","epoch 0, step: 88180/127656, loss: 0.009031570516526699\n","epoch 0, step: 88190/127656, loss: 0.013469956815242767\n","epoch 0, step: 88200/127656, loss: 0.011263135820627213\n","epoch 0, step: 88210/127656, loss: 0.012494400143623352\n","epoch 0, step: 88220/127656, loss: 0.010813547298312187\n","epoch 0, step: 88230/127656, loss: 0.012405766174197197\n","epoch 0, step: 88240/127656, loss: 0.009775079786777496\n","epoch 0, step: 88250/127656, loss: 0.011009689420461655\n","epoch 0, step: 88260/127656, loss: 0.3826911449432373\n","epoch 0, step: 88270/127656, loss: 0.011312449350953102\n","epoch 0, step: 88280/127656, loss: 0.007711692713201046\n","epoch 0, step: 88290/127656, loss: 0.014780305325984955\n","epoch 0, step: 88300/127656, loss: 0.009940274059772491\n","epoch 0, step: 88310/127656, loss: 0.01342412643134594\n","epoch 0, step: 88320/127656, loss: 0.00495947478339076\n","epoch 0, step: 88330/127656, loss: 0.2851094901561737\n","epoch 0, step: 88340/127656, loss: 0.39779457449913025\n","epoch 0, step: 88350/127656, loss: 0.25052139163017273\n","epoch 0, step: 88360/127656, loss: 0.009107143618166447\n","epoch 0, step: 88370/127656, loss: 0.008672456257045269\n","epoch 0, step: 88380/127656, loss: 0.01497603114694357\n","epoch 0, step: 88390/127656, loss: 0.005472144111990929\n","epoch 0, step: 88400/127656, loss: 0.13807272911071777\n","epoch 0, step: 88410/127656, loss: 0.009480461478233337\n","epoch 0, step: 88420/127656, loss: 0.011896013282239437\n","epoch 0, step: 88430/127656, loss: 0.0061819227412343025\n","epoch 0, step: 88440/127656, loss: 0.008297058753669262\n","epoch 0, step: 88450/127656, loss: 0.01097111776471138\n","epoch 0, step: 88460/127656, loss: 0.014544768258929253\n","epoch 0, step: 88470/127656, loss: 0.010127737186849117\n","epoch 0, step: 88480/127656, loss: 0.01066560111939907\n","epoch 0, step: 88490/127656, loss: 0.5054194331169128\n","epoch 0, step: 88500/127656, loss: 0.41002652049064636\n","epoch 0, step: 88510/127656, loss: 0.01052098162472248\n","epoch 0, step: 88520/127656, loss: 0.009316377341747284\n","epoch 0, step: 88530/127656, loss: 0.009655850008130074\n","epoch 0, step: 88540/127656, loss: 0.009034458547830582\n","epoch 0, step: 88550/127656, loss: 0.016012828797101974\n","epoch 0, step: 88560/127656, loss: 0.014239421114325523\n","epoch 0, step: 88570/127656, loss: 0.009821820072829723\n","epoch 0, step: 88580/127656, loss: 0.017221910879015923\n","epoch 0, step: 88590/127656, loss: 0.009764466434717178\n","epoch 0, step: 88600/127656, loss: 0.006489581428468227\n","epoch 0, step: 88610/127656, loss: 0.13308586180210114\n","epoch 0, step: 88620/127656, loss: 0.015634186565876007\n","epoch 0, step: 88630/127656, loss: 0.009822624735534191\n","epoch 0, step: 88640/127656, loss: 0.009004224091768265\n","epoch 0, step: 88650/127656, loss: 0.012782356701791286\n","epoch 0, step: 88660/127656, loss: 0.01266402192413807\n","epoch 0, step: 88670/127656, loss: 0.5524712204933167\n","epoch 0, step: 88680/127656, loss: 0.007550378330051899\n","epoch 0, step: 88690/127656, loss: 0.014310033991932869\n","epoch 0, step: 88700/127656, loss: 0.008389868773519993\n","epoch 0, step: 88710/127656, loss: 0.016303131356835365\n","epoch 0, step: 88720/127656, loss: 0.011550985276699066\n","epoch 0, step: 88730/127656, loss: 0.009671812877058983\n","epoch 0, step: 88740/127656, loss: 0.009815978817641735\n","epoch 0, step: 88750/127656, loss: 0.007377623580396175\n","epoch 0, step: 88760/127656, loss: 0.011039524339139462\n","epoch 0, step: 88770/127656, loss: 0.00981165375560522\n","epoch 0, step: 88780/127656, loss: 0.5245131850242615\n","epoch 0, step: 88790/127656, loss: 0.010964429937303066\n","epoch 0, step: 88800/127656, loss: 0.011890815570950508\n","epoch 0, step: 88810/127656, loss: 0.007399100810289383\n","epoch 0, step: 88820/127656, loss: 0.0076367054134607315\n","epoch 0, step: 88830/127656, loss: 0.015926571562886238\n","epoch 0, step: 88840/127656, loss: 0.010638284496963024\n","epoch 0, step: 88850/127656, loss: 0.010759010910987854\n","epoch 0, step: 88860/127656, loss: 0.010994082316756248\n","epoch 0, step: 88870/127656, loss: 0.00929601676762104\n","epoch 0, step: 88880/127656, loss: 0.012685972265899181\n","epoch 0, step: 88890/127656, loss: 0.008375106379389763\n","epoch 0, step: 88900/127656, loss: 0.013866099528968334\n","epoch 0, step: 88910/127656, loss: 0.01673979125916958\n","epoch 0, step: 88920/127656, loss: 0.00920358020812273\n","epoch 0, step: 88930/127656, loss: 0.00760998111218214\n","epoch 0, step: 88940/127656, loss: 0.0099850594997406\n","epoch 0, step: 88950/127656, loss: 0.008671525865793228\n","epoch 0, step: 88960/127656, loss: 0.008958554826676846\n","epoch 0, step: 88970/127656, loss: 0.010443191044032574\n","epoch 0, step: 88980/127656, loss: 0.012920750305056572\n","epoch 0, step: 88990/127656, loss: 0.0065814172849059105\n","epoch 0, step: 89000/127656, loss: 0.01327657513320446\n","epoch 0, step: 89010/127656, loss: 0.00619689142331481\n","epoch 0, step: 89020/127656, loss: 0.010848499834537506\n","epoch 0, step: 89030/127656, loss: 0.011759730987250805\n","epoch 0, step: 89040/127656, loss: 0.011513113975524902\n","epoch 0, step: 89050/127656, loss: 0.01156024169176817\n","epoch 0, step: 89060/127656, loss: 0.4097329080104828\n","epoch 0, step: 89070/127656, loss: 0.012595961801707745\n","epoch 0, step: 89080/127656, loss: 0.011078506708145142\n","epoch 0, step: 89090/127656, loss: 0.015211373567581177\n","epoch 0, step: 89100/127656, loss: 0.011272910982370377\n","epoch 0, step: 89110/127656, loss: 0.012803545221686363\n","epoch 0, step: 89120/127656, loss: 0.011329695582389832\n","epoch 0, step: 89130/127656, loss: 0.010291161015629768\n","epoch 0, step: 89140/127656, loss: 0.010478022508323193\n","epoch 0, step: 89150/127656, loss: 0.2686465382575989\n","epoch 0, step: 89160/127656, loss: 0.12764598429203033\n","epoch 0, step: 89170/127656, loss: 0.25781604647636414\n","epoch 0, step: 89180/127656, loss: 0.00823243334889412\n","epoch 0, step: 89190/127656, loss: 0.1413859874010086\n","epoch 0, step: 89200/127656, loss: 0.014625857584178448\n","epoch 0, step: 89210/127656, loss: 0.6798812747001648\n","epoch 0, step: 89220/127656, loss: 0.0076932585798203945\n","epoch 0, step: 89230/127656, loss: 0.010723782703280449\n","epoch 0, step: 89240/127656, loss: 0.008843407966196537\n","epoch 0, step: 89250/127656, loss: 0.008904044516384602\n","epoch 0, step: 89260/127656, loss: 0.014590772800147533\n","epoch 0, step: 89270/127656, loss: 0.007048134692013264\n","epoch 0, step: 89280/127656, loss: 0.13186076283454895\n","epoch 0, step: 89290/127656, loss: 0.007410089951008558\n","epoch 0, step: 89300/127656, loss: 0.013317737728357315\n","epoch 0, step: 89310/127656, loss: 0.008514313958585262\n","epoch 0, step: 89320/127656, loss: 0.010232966393232346\n","epoch 0, step: 89330/127656, loss: 0.017807967960834503\n","epoch 0, step: 89340/127656, loss: 0.012091932818293571\n","epoch 0, step: 89350/127656, loss: 0.016232464462518692\n","epoch 0, step: 89360/127656, loss: 0.011599283665418625\n","epoch 0, step: 89370/127656, loss: 0.009883923456072807\n","epoch 0, step: 89380/127656, loss: 0.01467098668217659\n","epoch 0, step: 89390/127656, loss: 0.004970688372850418\n","epoch 0, step: 89400/127656, loss: 0.006904656067490578\n","epoch 0, step: 89410/127656, loss: 0.012515462003648281\n","epoch 0, step: 89420/127656, loss: 0.014630159363150597\n","epoch 0, step: 89430/127656, loss: 0.12831483781337738\n","epoch 0, step: 89440/127656, loss: 0.006601234432309866\n","epoch 0, step: 89450/127656, loss: 0.010042487643659115\n","epoch 0, step: 89460/127656, loss: 0.011161251924932003\n","epoch 0, step: 89470/127656, loss: 0.009999692440032959\n","epoch 0, step: 89480/127656, loss: 0.00996430404484272\n","epoch 0, step: 89490/127656, loss: 0.010828517377376556\n","epoch 0, step: 89500/127656, loss: 0.010107115842401981\n","epoch 0, step: 89510/127656, loss: 0.009174826554954052\n","epoch 0, step: 89520/127656, loss: 0.015478871762752533\n","epoch 0, step: 89530/127656, loss: 0.015117622911930084\n","epoch 0, step: 89540/127656, loss: 0.008136110380291939\n","epoch 0, step: 89550/127656, loss: 0.011225221678614616\n","epoch 0, step: 89560/127656, loss: 0.016974886879324913\n","epoch 0, step: 89570/127656, loss: 0.011121159419417381\n","epoch 0, step: 89580/127656, loss: 0.011129303835332394\n","epoch 0, step: 89590/127656, loss: 0.011748265475034714\n","epoch 0, step: 89600/127656, loss: 0.013642328791320324\n","epoch 0, step: 89610/127656, loss: 0.008903326466679573\n","epoch 0, step: 89620/127656, loss: 0.013348634354770184\n","epoch 0, step: 89630/127656, loss: 0.25729894638061523\n","epoch 0, step: 89640/127656, loss: 0.010942511260509491\n","epoch 0, step: 89650/127656, loss: 0.011763256043195724\n","epoch 0, step: 89660/127656, loss: 0.14151395857334137\n","epoch 0, step: 89670/127656, loss: 0.013443250209093094\n","epoch 0, step: 89680/127656, loss: 0.010477656498551369\n","epoch 0, step: 89690/127656, loss: 0.00876674149185419\n","epoch 0, step: 89700/127656, loss: 0.006732290145009756\n","epoch 0, step: 89710/127656, loss: 0.017817769199609756\n","epoch 0, step: 89720/127656, loss: 0.016913864761590958\n","epoch 0, step: 89730/127656, loss: 0.014034840278327465\n","epoch 0, step: 89740/127656, loss: 0.009409910067915916\n","epoch 0, step: 89750/127656, loss: 0.008634675294160843\n","epoch 0, step: 89760/127656, loss: 0.009747161529958248\n","epoch 0, step: 89770/127656, loss: 0.011586490087211132\n","epoch 0, step: 89780/127656, loss: 0.01903034746646881\n","epoch 0, step: 89790/127656, loss: 0.00756444688886404\n","epoch 0, step: 89800/127656, loss: 0.013697992078959942\n","epoch 0, step: 89810/127656, loss: 0.01069495640695095\n","epoch 0, step: 89820/127656, loss: 0.01181077491492033\n","epoch 0, step: 89830/127656, loss: 0.009744986891746521\n","epoch 0, step: 89840/127656, loss: 0.013237661682069302\n","epoch 0, step: 89850/127656, loss: 0.010983342304825783\n","epoch 0, step: 89860/127656, loss: 0.015880189836025238\n","epoch 0, step: 89870/127656, loss: 0.010157747194170952\n","epoch 0, step: 89880/127656, loss: 0.010284450836479664\n","epoch 0, step: 89890/127656, loss: 0.006886019837111235\n","epoch 0, step: 89900/127656, loss: 0.010040738619863987\n","epoch 0, step: 89910/127656, loss: 0.1421147882938385\n","epoch 0, step: 89920/127656, loss: 0.010815003886818886\n","epoch 0, step: 89930/127656, loss: 0.009520399384200573\n","epoch 0, step: 89940/127656, loss: 0.2735934257507324\n","epoch 0, step: 89950/127656, loss: 0.011506585404276848\n","epoch 0, step: 89960/127656, loss: 0.007406805641949177\n","epoch 0, step: 89970/127656, loss: 0.011247809045016766\n","epoch 0, step: 89980/127656, loss: 0.011536449193954468\n","epoch 0, step: 89990/127656, loss: 0.007556872442364693\n","epoch 0, step: 90000/127656, loss: 0.01218838058412075\n","epoch 0, step: 90010/127656, loss: 0.006666359957307577\n","epoch 0, step: 90020/127656, loss: 0.007840602658689022\n","epoch 0, step: 90030/127656, loss: 0.0070160143077373505\n","epoch 0, step: 90040/127656, loss: 0.012113804928958416\n","epoch 0, step: 90050/127656, loss: 0.011288149282336235\n","epoch 0, step: 90060/127656, loss: 0.00975409522652626\n","epoch 0, step: 90070/127656, loss: 0.00905776210129261\n","epoch 0, step: 90080/127656, loss: 0.013207150623202324\n","epoch 0, step: 90090/127656, loss: 0.00922500528395176\n","epoch 0, step: 90100/127656, loss: 0.27353721857070923\n","epoch 0, step: 90110/127656, loss: 0.5456575155258179\n","epoch 0, step: 90120/127656, loss: 0.00838121771812439\n","epoch 0, step: 90130/127656, loss: 0.00894513726234436\n","epoch 0, step: 90140/127656, loss: 0.009517950005829334\n","epoch 0, step: 90150/127656, loss: 0.01034625805914402\n","epoch 0, step: 90160/127656, loss: 0.01499092485755682\n","epoch 0, step: 90170/127656, loss: 0.007951975800096989\n","epoch 0, step: 90180/127656, loss: 0.017195751890540123\n","epoch 0, step: 90190/127656, loss: 0.007241433020681143\n","epoch 0, step: 90200/127656, loss: 0.009732326492667198\n","epoch 0, step: 90210/127656, loss: 0.01090339943766594\n","epoch 0, step: 90220/127656, loss: 0.013139676302671432\n","epoch 0, step: 90230/127656, loss: 0.009207228198647499\n","epoch 0, step: 90240/127656, loss: 0.009410507045686245\n","epoch 0, step: 90250/127656, loss: 0.4153633117675781\n","epoch 0, step: 90260/127656, loss: 0.014508336782455444\n","epoch 0, step: 90270/127656, loss: 0.007606080733239651\n","epoch 0, step: 90280/127656, loss: 0.007389680948108435\n","epoch 0, step: 90290/127656, loss: 0.40462589263916016\n","epoch 0, step: 90300/127656, loss: 0.0061141918413341045\n","epoch 0, step: 90310/127656, loss: 0.01123969815671444\n","epoch 0, step: 90320/127656, loss: 0.008879290893673897\n","epoch 0, step: 90330/127656, loss: 0.010652454569935799\n","epoch 0, step: 90340/127656, loss: 0.008085708133876324\n","epoch 0, step: 90350/127656, loss: 0.011671330779790878\n","epoch 0, step: 90360/127656, loss: 0.01009250245988369\n","epoch 0, step: 90370/127656, loss: 0.008416898548603058\n","epoch 0, step: 90380/127656, loss: 0.01014044601470232\n","epoch 0, step: 90390/127656, loss: 0.008396431803703308\n","epoch 0, step: 90400/127656, loss: 0.009085753001272678\n","epoch 0, step: 90410/127656, loss: 0.016741497442126274\n","epoch 0, step: 90420/127656, loss: 0.009958194568753242\n","epoch 0, step: 90430/127656, loss: 0.012217231094837189\n","epoch 0, step: 90440/127656, loss: 0.008850736543536186\n","epoch 0, step: 90450/127656, loss: 0.4095374047756195\n","epoch 0, step: 90460/127656, loss: 0.005750476382672787\n","epoch 0, step: 90470/127656, loss: 0.009698813781142235\n","epoch 0, step: 90480/127656, loss: 0.008453624323010445\n","epoch 0, step: 90490/127656, loss: 0.014987527392804623\n","epoch 0, step: 90500/127656, loss: 0.4010407328605652\n","epoch 0, step: 90510/127656, loss: 0.011505356058478355\n","epoch 0, step: 90520/127656, loss: 0.00994042120873928\n","epoch 0, step: 90530/127656, loss: 0.013766840100288391\n","epoch 0, step: 90540/127656, loss: 0.010468382388353348\n","epoch 0, step: 90550/127656, loss: 0.01630054973065853\n","epoch 0, step: 90560/127656, loss: 0.011444220319390297\n","epoch 0, step: 90570/127656, loss: 0.007845508866012096\n","epoch 0, step: 90580/127656, loss: 0.009578260593116283\n","epoch 0, step: 90590/127656, loss: 0.01252037100493908\n","epoch 0, step: 90600/127656, loss: 0.012242183089256287\n","epoch 0, step: 90610/127656, loss: 0.013109445571899414\n","epoch 0, step: 90620/127656, loss: 0.01470960583537817\n","epoch 0, step: 90630/127656, loss: 0.01159602776169777\n","epoch 0, step: 90640/127656, loss: 0.01595856063067913\n","epoch 0, step: 90650/127656, loss: 0.012550469487905502\n","epoch 0, step: 90660/127656, loss: 0.009738405235111713\n","epoch 0, step: 90670/127656, loss: 0.008779074065387249\n","epoch 0, step: 90680/127656, loss: 0.00795347522944212\n","epoch 0, step: 90690/127656, loss: 0.007517538033425808\n","epoch 0, step: 90700/127656, loss: 0.011937100440263748\n","epoch 0, step: 90710/127656, loss: 0.005477876868098974\n","epoch 0, step: 90720/127656, loss: 0.011792751960456371\n","epoch 0, step: 90730/127656, loss: 0.012079521082341671\n","epoch 0, step: 90740/127656, loss: 0.012253215536475182\n","epoch 0, step: 90750/127656, loss: 0.01328599825501442\n","epoch 0, step: 90760/127656, loss: 0.009148737415671349\n","epoch 0, step: 90770/127656, loss: 0.01468650996685028\n","epoch 0, step: 90780/127656, loss: 0.008040593937039375\n","epoch 0, step: 90790/127656, loss: 0.39937472343444824\n","epoch 0, step: 90800/127656, loss: 0.009366948157548904\n","epoch 0, step: 90810/127656, loss: 0.00784930307418108\n","epoch 0, step: 90820/127656, loss: 0.008508989587426186\n","epoch 0, step: 90830/127656, loss: 0.012400350533425808\n","epoch 0, step: 90840/127656, loss: 0.00765657564625144\n","epoch 0, step: 90850/127656, loss: 0.011897481046617031\n","epoch 0, step: 90860/127656, loss: 0.015480061993002892\n","epoch 0, step: 90870/127656, loss: 0.008240761235356331\n","epoch 0, step: 90880/127656, loss: 0.013784509152173996\n","epoch 0, step: 90890/127656, loss: 0.015177259221673012\n","epoch 0, step: 90900/127656, loss: 0.00976086501032114\n","epoch 0, step: 90910/127656, loss: 0.01141473837196827\n","epoch 0, step: 90920/127656, loss: 0.007796178571879864\n","epoch 0, step: 90930/127656, loss: 0.009580790065228939\n","epoch 0, step: 90940/127656, loss: 0.5476254224777222\n","epoch 0, step: 90950/127656, loss: 0.012424692511558533\n","epoch 0, step: 90960/127656, loss: 0.010471314191818237\n","epoch 0, step: 90970/127656, loss: 0.007301407866179943\n","epoch 0, step: 90980/127656, loss: 0.006259099580347538\n","epoch 0, step: 90990/127656, loss: 0.011911241337656975\n","epoch 0, step: 91000/127656, loss: 0.007215698715299368\n","epoch 0, step: 91010/127656, loss: 0.013010922819375992\n","epoch 0, step: 91020/127656, loss: 0.015309627167880535\n","epoch 0, step: 91030/127656, loss: 0.00885882880538702\n","epoch 0, step: 91040/127656, loss: 0.010792933404445648\n","epoch 0, step: 91050/127656, loss: 0.01847626082599163\n","epoch 0, step: 91060/127656, loss: 0.007923154160380363\n","epoch 0, step: 91070/127656, loss: 0.008038315922021866\n","epoch 0, step: 91080/127656, loss: 0.0075247748754918575\n","epoch 0, step: 91090/127656, loss: 0.3942040205001831\n","epoch 0, step: 91100/127656, loss: 0.012171527370810509\n","epoch 0, step: 91110/127656, loss: 0.006678231060504913\n","epoch 0, step: 91120/127656, loss: 0.009358447045087814\n","epoch 0, step: 91130/127656, loss: 0.016518592834472656\n","epoch 0, step: 91140/127656, loss: 0.12885500490665436\n","epoch 0, step: 91150/127656, loss: 0.008139533922076225\n","epoch 0, step: 91160/127656, loss: 0.011959951370954514\n","epoch 0, step: 91170/127656, loss: 0.010044218972325325\n","epoch 0, step: 91180/127656, loss: 0.008781272917985916\n","epoch 0, step: 91190/127656, loss: 0.011698882095515728\n","epoch 0, step: 91200/127656, loss: 0.01038863230496645\n","epoch 0, step: 91210/127656, loss: 0.015166286379098892\n","epoch 0, step: 91220/127656, loss: 0.009678302332758904\n","epoch 0, step: 91230/127656, loss: 0.011951340362429619\n","epoch 0, step: 91240/127656, loss: 0.00947902724146843\n","epoch 0, step: 91250/127656, loss: 0.013121211901307106\n","epoch 0, step: 91260/127656, loss: 0.0056251538917422295\n","epoch 0, step: 91270/127656, loss: 0.4316687285900116\n","epoch 0, step: 91280/127656, loss: 0.0071693360805511475\n","epoch 0, step: 91290/127656, loss: 0.013312730938196182\n","epoch 0, step: 91300/127656, loss: 0.008660832419991493\n","epoch 0, step: 91310/127656, loss: 0.009946017526090145\n","epoch 0, step: 91320/127656, loss: 0.008507904596626759\n","epoch 0, step: 91330/127656, loss: 0.006246358156204224\n","epoch 0, step: 91340/127656, loss: 0.01002985704690218\n","epoch 0, step: 91350/127656, loss: 0.01025201566517353\n","epoch 0, step: 91360/127656, loss: 0.012737376615405083\n","epoch 0, step: 91370/127656, loss: 0.00772342924028635\n","epoch 0, step: 91380/127656, loss: 0.013660263270139694\n","epoch 0, step: 91390/127656, loss: 0.008739212527871132\n","epoch 0, step: 91400/127656, loss: 0.6407941579818726\n","epoch 0, step: 91410/127656, loss: 0.008786000311374664\n","epoch 0, step: 91420/127656, loss: 0.014453744515776634\n","epoch 0, step: 91430/127656, loss: 0.00850245263427496\n","epoch 0, step: 91440/127656, loss: 0.257526695728302\n","epoch 0, step: 91450/127656, loss: 0.010751334950327873\n","epoch 0, step: 91460/127656, loss: 0.01123073510825634\n","epoch 0, step: 91470/127656, loss: 0.013025197200477123\n","epoch 0, step: 91480/127656, loss: 0.016467224806547165\n","epoch 0, step: 91490/127656, loss: 0.008966479450464249\n","epoch 0, step: 91500/127656, loss: 0.014198298566043377\n","epoch 0, step: 91510/127656, loss: 0.011676705442368984\n","epoch 0, step: 91520/127656, loss: 0.008065296337008476\n","epoch 0, step: 91530/127656, loss: 0.007857758551836014\n","epoch 0, step: 91540/127656, loss: 0.006686267908662558\n","epoch 0, step: 91550/127656, loss: 0.006949990056455135\n","epoch 0, step: 91560/127656, loss: 0.5366663336753845\n","epoch 0, step: 91570/127656, loss: 0.008922459557652473\n","epoch 0, step: 91580/127656, loss: 0.012048367410898209\n","epoch 0, step: 91590/127656, loss: 0.27109622955322266\n","epoch 0, step: 91600/127656, loss: 0.011425696313381195\n","epoch 0, step: 91610/127656, loss: 0.013444269075989723\n","epoch 0, step: 91620/127656, loss: 0.011056038551032543\n","epoch 0, step: 91630/127656, loss: 0.008170021697878838\n","epoch 0, step: 91640/127656, loss: 0.012726673856377602\n","epoch 0, step: 91650/127656, loss: 0.14596028625965118\n","epoch 0, step: 91660/127656, loss: 0.009900453500449657\n","epoch 0, step: 91670/127656, loss: 0.1385178565979004\n","epoch 0, step: 91680/127656, loss: 0.008090779185295105\n","epoch 0, step: 91690/127656, loss: 0.012326659634709358\n","epoch 0, step: 91700/127656, loss: 0.006551904603838921\n","epoch 0, step: 91710/127656, loss: 0.013593348674476147\n","epoch 0, step: 91720/127656, loss: 0.01363756787031889\n","epoch 0, step: 91730/127656, loss: 0.013233142904937267\n","epoch 0, step: 91740/127656, loss: 0.011703019961714745\n","epoch 0, step: 91750/127656, loss: 0.006919290404766798\n","epoch 0, step: 91760/127656, loss: 0.009141795337200165\n","epoch 0, step: 91770/127656, loss: 0.011015239171683788\n","epoch 0, step: 91780/127656, loss: 0.015228759497404099\n","epoch 0, step: 91790/127656, loss: 0.006699345074594021\n","epoch 0, step: 91800/127656, loss: 0.01354504656046629\n","epoch 0, step: 91810/127656, loss: 0.4038275182247162\n","epoch 0, step: 91820/127656, loss: 0.0074042631313204765\n","epoch 0, step: 91830/127656, loss: 0.015197686851024628\n","epoch 0, step: 91840/127656, loss: 0.00899905152618885\n","epoch 0, step: 91850/127656, loss: 0.007522878237068653\n","epoch 0, step: 91860/127656, loss: 0.011529787443578243\n","epoch 0, step: 91870/127656, loss: 0.011548100039362907\n","epoch 0, step: 91880/127656, loss: 0.2564712464809418\n","epoch 0, step: 91890/127656, loss: 0.0069266390055418015\n","epoch 0, step: 91900/127656, loss: 0.40634477138519287\n","epoch 0, step: 91910/127656, loss: 0.010653436183929443\n","epoch 0, step: 91920/127656, loss: 0.010650891810655594\n","epoch 0, step: 91930/127656, loss: 0.011679094284772873\n","epoch 0, step: 91940/127656, loss: 0.01318465918302536\n","epoch 0, step: 91950/127656, loss: 0.012234685942530632\n","epoch 0, step: 91960/127656, loss: 0.010489039123058319\n","epoch 0, step: 91970/127656, loss: 0.0108729787170887\n","epoch 0, step: 91980/127656, loss: 0.006537408567965031\n","epoch 0, step: 91990/127656, loss: 0.010004537180066109\n","epoch 0, step: 92000/127656, loss: 0.010770997032523155\n","epoch 0, step: 92010/127656, loss: 0.007453688886016607\n","epoch 0, step: 92020/127656, loss: 0.009379908442497253\n","epoch 0, step: 92030/127656, loss: 0.008485615253448486\n","epoch 0, step: 92040/127656, loss: 0.009564949199557304\n","epoch 0, step: 92050/127656, loss: 0.013023683801293373\n","epoch 0, step: 92060/127656, loss: 0.008372149430215359\n","epoch 0, step: 92070/127656, loss: 0.009673703461885452\n","epoch 0, step: 92080/127656, loss: 0.013353019021451473\n","epoch 0, step: 92090/127656, loss: 0.011093959212303162\n","epoch 0, step: 92100/127656, loss: 0.010257615707814693\n","epoch 0, step: 92110/127656, loss: 0.011012750677764416\n","epoch 0, step: 92120/127656, loss: 0.006581950467079878\n","epoch 0, step: 92130/127656, loss: 0.01574886217713356\n","epoch 0, step: 92140/127656, loss: 0.007015429437160492\n","epoch 0, step: 92150/127656, loss: 0.011170702986419201\n","epoch 0, step: 92160/127656, loss: 0.013676062226295471\n","epoch 0, step: 92170/127656, loss: 0.008155286312103271\n","epoch 0, step: 92180/127656, loss: 0.2801240086555481\n","epoch 0, step: 92190/127656, loss: 0.00614227494224906\n","epoch 0, step: 92200/127656, loss: 0.01416362076997757\n","epoch 0, step: 92210/127656, loss: 0.00956588052213192\n","epoch 0, step: 92220/127656, loss: 0.008492792025208473\n","epoch 0, step: 92230/127656, loss: 0.008002528920769691\n","epoch 0, step: 92240/127656, loss: 0.2570895254611969\n","epoch 0, step: 92250/127656, loss: 0.01081114262342453\n","epoch 0, step: 92260/127656, loss: 0.009452558122575283\n","epoch 0, step: 92270/127656, loss: 0.009611435234546661\n","epoch 0, step: 92280/127656, loss: 0.007780564017593861\n","epoch 0, step: 92290/127656, loss: 0.01187417283654213\n","epoch 0, step: 92300/127656, loss: 0.009908567182719707\n","epoch 0, step: 92310/127656, loss: 0.0073182471096515656\n","epoch 0, step: 92320/127656, loss: 0.010518409311771393\n","epoch 0, step: 92330/127656, loss: 0.007861126214265823\n","epoch 0, step: 92340/127656, loss: 0.01000004168599844\n","epoch 0, step: 92350/127656, loss: 0.00786096416413784\n","epoch 0, step: 92360/127656, loss: 0.014659775421023369\n","epoch 0, step: 92370/127656, loss: 0.012551600113511086\n","epoch 0, step: 92380/127656, loss: 0.010721846483647823\n","epoch 0, step: 92390/127656, loss: 0.014753639698028564\n","epoch 0, step: 92400/127656, loss: 0.00958410743623972\n","epoch 0, step: 92410/127656, loss: 0.01583462953567505\n","epoch 0, step: 92420/127656, loss: 0.007376685738563538\n","epoch 0, step: 92430/127656, loss: 0.009274189360439777\n","epoch 0, step: 92440/127656, loss: 0.008039294742047787\n","epoch 0, step: 92450/127656, loss: 0.010169166140258312\n","epoch 0, step: 92460/127656, loss: 0.014827563427388668\n","epoch 0, step: 92470/127656, loss: 0.013005366548895836\n","epoch 0, step: 92480/127656, loss: 0.010552319698035717\n","epoch 0, step: 92490/127656, loss: 0.00794224999845028\n","epoch 0, step: 92500/127656, loss: 0.14238713681697845\n","epoch 0, step: 92510/127656, loss: 0.008081133477389812\n","epoch 0, step: 92520/127656, loss: 0.011272571980953217\n","epoch 0, step: 92530/127656, loss: 0.008380142971873283\n","epoch 0, step: 92540/127656, loss: 0.007354130037128925\n","epoch 0, step: 92550/127656, loss: 0.008799346163868904\n","epoch 0, step: 92560/127656, loss: 0.009013819508254528\n","epoch 0, step: 92570/127656, loss: 0.006105945445597172\n","epoch 0, step: 92580/127656, loss: 0.011568037793040276\n","epoch 0, step: 92590/127656, loss: 0.011826969683170319\n","epoch 0, step: 92600/127656, loss: 0.017061453312635422\n","epoch 0, step: 92610/127656, loss: 0.010313051752746105\n","epoch 0, step: 92620/127656, loss: 0.010177969932556152\n","epoch 0, step: 92630/127656, loss: 0.007629783824086189\n","epoch 0, step: 92640/127656, loss: 0.007878612726926804\n","epoch 0, step: 92650/127656, loss: 0.007975243031978607\n","epoch 0, step: 92660/127656, loss: 0.27160072326660156\n","epoch 0, step: 92670/127656, loss: 0.008655684068799019\n","epoch 0, step: 92680/127656, loss: 0.011787168681621552\n","epoch 0, step: 92690/127656, loss: 0.006667163223028183\n","epoch 0, step: 92700/127656, loss: 0.00940113328397274\n","epoch 0, step: 92710/127656, loss: 0.0076078986749053\n","epoch 0, step: 92720/127656, loss: 0.008088895119726658\n","epoch 0, step: 92730/127656, loss: 0.006195692345499992\n","epoch 0, step: 92740/127656, loss: 0.011339995078742504\n","epoch 0, step: 92750/127656, loss: 0.007307767402380705\n","epoch 0, step: 92760/127656, loss: 0.01060052402317524\n","epoch 0, step: 92770/127656, loss: 0.015304267406463623\n","epoch 0, step: 92780/127656, loss: 0.01294986717402935\n","epoch 0, step: 92790/127656, loss: 0.010705615393817425\n","epoch 0, step: 92800/127656, loss: 0.2713630199432373\n","epoch 0, step: 92810/127656, loss: 0.008303378708660603\n","epoch 0, step: 92820/127656, loss: 0.010301418602466583\n","epoch 0, step: 92830/127656, loss: 0.009522432461380959\n","epoch 0, step: 92840/127656, loss: 0.009790859185159206\n","epoch 0, step: 92850/127656, loss: 0.007695053704082966\n","epoch 0, step: 92860/127656, loss: 0.0076839211396873\n","epoch 0, step: 92870/127656, loss: 0.011153928935527802\n","epoch 0, step: 92880/127656, loss: 0.008520230650901794\n","epoch 0, step: 92890/127656, loss: 0.011555418372154236\n","epoch 0, step: 92900/127656, loss: 0.00768530135974288\n","epoch 0, step: 92910/127656, loss: 0.008338573388755322\n","epoch 0, step: 92920/127656, loss: 0.13275137543678284\n","epoch 0, step: 92930/127656, loss: 0.010943343862891197\n","epoch 0, step: 92940/127656, loss: 0.00967884436249733\n","epoch 0, step: 92950/127656, loss: 0.019111700356006622\n","epoch 0, step: 92960/127656, loss: 0.4021691083908081\n","epoch 0, step: 92970/127656, loss: 0.007926649414002895\n","epoch 0, step: 92980/127656, loss: 0.009742887690663338\n","epoch 0, step: 92990/127656, loss: 0.011842979118227959\n","epoch 0, step: 93000/127656, loss: 0.011007381603121758\n","epoch 0, step: 93010/127656, loss: 0.01426423154771328\n","epoch 0, step: 93020/127656, loss: 0.009672285988926888\n","epoch 0, step: 93030/127656, loss: 0.00755461398512125\n","epoch 0, step: 93040/127656, loss: 0.00907384604215622\n","epoch 0, step: 93050/127656, loss: 0.005796082317829132\n","epoch 0, step: 93060/127656, loss: 0.010897359810769558\n","epoch 0, step: 93070/127656, loss: 0.009774763137102127\n","epoch 0, step: 93080/127656, loss: 0.01127296220511198\n","epoch 0, step: 93090/127656, loss: 0.006387609988451004\n","epoch 0, step: 93100/127656, loss: 0.010306760668754578\n","epoch 0, step: 93110/127656, loss: 0.006713024340569973\n","epoch 0, step: 93120/127656, loss: 0.010439693927764893\n","epoch 0, step: 93130/127656, loss: 0.01130989845842123\n","epoch 0, step: 93140/127656, loss: 0.009986380115151405\n","epoch 0, step: 93150/127656, loss: 0.0073658740147948265\n","epoch 0, step: 93160/127656, loss: 0.006549998186528683\n","epoch 0, step: 93170/127656, loss: 0.010240964591503143\n","epoch 0, step: 93180/127656, loss: 0.01187838427722454\n","epoch 0, step: 93190/127656, loss: 0.013156230561435223\n","epoch 0, step: 93200/127656, loss: 0.00877329707145691\n","epoch 0, step: 93210/127656, loss: 0.008786916732788086\n","epoch 0, step: 93220/127656, loss: 0.009058341383934021\n","epoch 0, step: 93230/127656, loss: 0.01053658314049244\n","epoch 0, step: 93240/127656, loss: 0.007717416621744633\n","epoch 0, step: 93250/127656, loss: 0.006615092512220144\n","epoch 0, step: 93260/127656, loss: 0.009376429952681065\n","epoch 0, step: 93270/127656, loss: 0.010316611267626286\n","epoch 0, step: 93280/127656, loss: 0.13984417915344238\n","epoch 0, step: 93290/127656, loss: 0.0069151511415839195\n","epoch 0, step: 93300/127656, loss: 0.007981939241290092\n","epoch 0, step: 93310/127656, loss: 0.4092908501625061\n","epoch 0, step: 93320/127656, loss: 0.009862694889307022\n","epoch 0, step: 93330/127656, loss: 0.013199596665799618\n","epoch 0, step: 93340/127656, loss: 0.010736430063843727\n","epoch 0, step: 93350/127656, loss: 0.008720677345991135\n","epoch 0, step: 93360/127656, loss: 0.009928150102496147\n","epoch 0, step: 93370/127656, loss: 0.014130724593997002\n","epoch 0, step: 93380/127656, loss: 0.006585285067558289\n","epoch 0, step: 93390/127656, loss: 0.011613357812166214\n","epoch 0, step: 93400/127656, loss: 0.008932151831686497\n","epoch 0, step: 93410/127656, loss: 0.01086733303964138\n","epoch 0, step: 93420/127656, loss: 0.006477431394159794\n","epoch 0, step: 93430/127656, loss: 0.009495720267295837\n","epoch 0, step: 93440/127656, loss: 0.011705167591571808\n","epoch 0, step: 93450/127656, loss: 0.009567979723215103\n","epoch 0, step: 93460/127656, loss: 0.00846181996166706\n","epoch 0, step: 93470/127656, loss: 0.01311442255973816\n","epoch 0, step: 93480/127656, loss: 0.008267736993730068\n","epoch 0, step: 93490/127656, loss: 0.0077613359317183495\n","epoch 0, step: 93500/127656, loss: 0.0077362386509776115\n","epoch 0, step: 93510/127656, loss: 0.012638586573302746\n","epoch 0, step: 93520/127656, loss: 0.01176072284579277\n","epoch 0, step: 93530/127656, loss: 0.007673266809433699\n","epoch 0, step: 93540/127656, loss: 0.2449062466621399\n","epoch 0, step: 93550/127656, loss: 0.011268964037299156\n","epoch 0, step: 93560/127656, loss: 0.00862060859799385\n","epoch 0, step: 93570/127656, loss: 0.012588722631335258\n","epoch 0, step: 93580/127656, loss: 0.1352478712797165\n","epoch 0, step: 93590/127656, loss: 0.015769200399518013\n","epoch 0, step: 93600/127656, loss: 0.007232924457639456\n","epoch 0, step: 93610/127656, loss: 0.010638495907187462\n","epoch 0, step: 93620/127656, loss: 0.012750616297125816\n","epoch 0, step: 93630/127656, loss: 0.014562736265361309\n","epoch 0, step: 93640/127656, loss: 0.008206595666706562\n","epoch 0, step: 93650/127656, loss: 0.009129881858825684\n","epoch 0, step: 93660/127656, loss: 0.008767958730459213\n","epoch 0, step: 93670/127656, loss: 0.011372709646821022\n","epoch 0, step: 93680/127656, loss: 0.016449134796857834\n","epoch 0, step: 93690/127656, loss: 0.007483784109354019\n","epoch 0, step: 93700/127656, loss: 0.006628747098147869\n","epoch 0, step: 93710/127656, loss: 0.012824888341128826\n","epoch 0, step: 93720/127656, loss: 0.01418729405850172\n","epoch 0, step: 93730/127656, loss: 0.13991600275039673\n","epoch 0, step: 93740/127656, loss: 0.1297837197780609\n","epoch 0, step: 93750/127656, loss: 0.00793381780385971\n","epoch 0, step: 93760/127656, loss: 0.0062262676656246185\n","epoch 0, step: 93770/127656, loss: 0.011560987681150436\n","epoch 0, step: 93780/127656, loss: 0.011600266210734844\n","epoch 0, step: 93790/127656, loss: 0.008876968175172806\n","epoch 0, step: 93800/127656, loss: 0.009202847257256508\n","epoch 0, step: 93810/127656, loss: 0.008651948533952236\n","epoch 0, step: 93820/127656, loss: 0.0063223158940672874\n","epoch 0, step: 93830/127656, loss: 0.007303616963326931\n","epoch 0, step: 93840/127656, loss: 0.006619495805352926\n","epoch 0, step: 93850/127656, loss: 0.009004396386444569\n","epoch 0, step: 93860/127656, loss: 0.008298377506434917\n","epoch 0, step: 93870/127656, loss: 0.009906185790896416\n","epoch 0, step: 93880/127656, loss: 0.13061223924160004\n","epoch 0, step: 93890/127656, loss: 0.013301247730851173\n","epoch 0, step: 93900/127656, loss: 0.00649214256554842\n","epoch 0, step: 93910/127656, loss: 0.00911305658519268\n","epoch 0, step: 93920/127656, loss: 0.01241810992360115\n","epoch 0, step: 93930/127656, loss: 0.011581189930438995\n","epoch 0, step: 93940/127656, loss: 0.006672978401184082\n","epoch 0, step: 93950/127656, loss: 0.41358682513237\n","epoch 0, step: 93960/127656, loss: 0.008523957803845406\n","epoch 0, step: 93970/127656, loss: 0.014553092420101166\n","epoch 0, step: 93980/127656, loss: 0.008157005533576012\n","epoch 0, step: 93990/127656, loss: 0.008167878724634647\n","epoch 0, step: 94000/127656, loss: 0.011214061640202999\n","epoch 0, step: 94010/127656, loss: 0.014289960265159607\n","epoch 0, step: 94020/127656, loss: 0.008331741206347942\n","epoch 0, step: 94030/127656, loss: 0.008719708770513535\n","epoch 0, step: 94040/127656, loss: 0.008286106400191784\n","epoch 0, step: 94050/127656, loss: 0.025270599871873856\n","epoch 0, step: 94060/127656, loss: 0.008600165136158466\n","epoch 0, step: 94070/127656, loss: 0.01499833445996046\n","epoch 0, step: 94080/127656, loss: 0.011965127661824226\n","epoch 0, step: 94090/127656, loss: 0.010408791713416576\n","epoch 0, step: 94100/127656, loss: 0.010053873062133789\n","epoch 0, step: 94110/127656, loss: 0.012684589251875877\n","epoch 0, step: 94120/127656, loss: 0.009021610021591187\n","epoch 0, step: 94130/127656, loss: 0.011651507578790188\n","epoch 0, step: 94140/127656, loss: 0.15301892161369324\n","epoch 0, step: 94150/127656, loss: 0.006136453710496426\n","epoch 0, step: 94160/127656, loss: 0.00943897757679224\n","epoch 0, step: 94170/127656, loss: 0.008221718482673168\n","epoch 0, step: 94180/127656, loss: 0.008938427083194256\n","epoch 0, step: 94190/127656, loss: 0.01072668470442295\n","epoch 0, step: 94200/127656, loss: 0.014905940741300583\n","epoch 0, step: 94210/127656, loss: 0.012113027274608612\n","epoch 0, step: 94220/127656, loss: 0.12684321403503418\n","epoch 0, step: 94230/127656, loss: 0.010328019969165325\n","epoch 0, step: 94240/127656, loss: 0.011101747862994671\n","epoch 0, step: 94250/127656, loss: 0.012130219489336014\n","epoch 0, step: 94260/127656, loss: 0.006554144434630871\n","epoch 0, step: 94270/127656, loss: 0.010500309988856316\n","epoch 0, step: 94280/127656, loss: 0.0056399451568722725\n","epoch 0, step: 94290/127656, loss: 0.010491974651813507\n","epoch 0, step: 94300/127656, loss: 0.008420739322900772\n","epoch 0, step: 94310/127656, loss: 0.009823041036725044\n","epoch 0, step: 94320/127656, loss: 0.010936291888356209\n","epoch 0, step: 94330/127656, loss: 0.009801413863897324\n","epoch 0, step: 94340/127656, loss: 0.00767944660037756\n","epoch 0, step: 94350/127656, loss: 0.005627316888421774\n","epoch 0, step: 94360/127656, loss: 0.01592094637453556\n","epoch 0, step: 94370/127656, loss: 0.00917140208184719\n","epoch 0, step: 94380/127656, loss: 0.008474918082356453\n","epoch 0, step: 94390/127656, loss: 0.011283627711236477\n","epoch 0, step: 94400/127656, loss: 0.01912786439061165\n","epoch 0, step: 94410/127656, loss: 0.01089649647474289\n","epoch 0, step: 94420/127656, loss: 0.013334129005670547\n","epoch 0, step: 94430/127656, loss: 0.006878437474370003\n","epoch 0, step: 94440/127656, loss: 0.007235411088913679\n","epoch 0, step: 94450/127656, loss: 0.013808990828692913\n","epoch 0, step: 94460/127656, loss: 0.010000284761190414\n","epoch 0, step: 94470/127656, loss: 0.009679487906396389\n","epoch 0, step: 94480/127656, loss: 0.012425134889781475\n","epoch 0, step: 94490/127656, loss: 0.008354924619197845\n","epoch 0, step: 94500/127656, loss: 0.009104069322347641\n","epoch 0, step: 94510/127656, loss: 0.008990682661533356\n","epoch 0, step: 94520/127656, loss: 0.01262187771499157\n","epoch 0, step: 94530/127656, loss: 0.0067228274419903755\n","epoch 0, step: 94540/127656, loss: 0.007645537611097097\n","epoch 0, step: 94550/127656, loss: 0.006423622369766235\n","epoch 0, step: 94560/127656, loss: 0.006611091550439596\n","epoch 0, step: 94570/127656, loss: 0.009636669419705868\n","epoch 0, step: 94580/127656, loss: 0.009322351776063442\n","epoch 0, step: 94590/127656, loss: 0.012911461293697357\n","epoch 0, step: 94600/127656, loss: 0.009449716657400131\n","epoch 0, step: 94610/127656, loss: 0.01194154005497694\n","epoch 0, step: 94620/127656, loss: 0.0068784672766923904\n","epoch 0, step: 94630/127656, loss: 0.01565183326601982\n","epoch 0, step: 94640/127656, loss: 0.008685769513249397\n","epoch 0, step: 94650/127656, loss: 0.1273646056652069\n","epoch 0, step: 94660/127656, loss: 0.012804522179067135\n","epoch 0, step: 94670/127656, loss: 0.13205166161060333\n","epoch 0, step: 94680/127656, loss: 0.009499303996562958\n","epoch 0, step: 94690/127656, loss: 0.009798742830753326\n","epoch 0, step: 94700/127656, loss: 0.009215298108756542\n","epoch 0, step: 94710/127656, loss: 0.010529815219342709\n","epoch 0, step: 94720/127656, loss: 0.00809075403958559\n","epoch 0, step: 94730/127656, loss: 0.007893707603216171\n","epoch 0, step: 94740/127656, loss: 0.2628174126148224\n","epoch 0, step: 94750/127656, loss: 0.014475300908088684\n","epoch 0, step: 94760/127656, loss: 0.007105666678398848\n","epoch 0, step: 94770/127656, loss: 0.008899491280317307\n","epoch 0, step: 94780/127656, loss: 0.007339312694966793\n","epoch 0, step: 94790/127656, loss: 0.010016186162829399\n","epoch 0, step: 94800/127656, loss: 0.014020363800227642\n","epoch 0, step: 94810/127656, loss: 0.005153861828148365\n","epoch 0, step: 94820/127656, loss: 0.012200355529785156\n","epoch 0, step: 94830/127656, loss: 0.012405911460518837\n","epoch 0, step: 94840/127656, loss: 0.007342716213315725\n","epoch 0, step: 94850/127656, loss: 0.011963861994445324\n","epoch 0, step: 94860/127656, loss: 0.13273468613624573\n","epoch 0, step: 94870/127656, loss: 0.009970203042030334\n","epoch 0, step: 94880/127656, loss: 0.14527112245559692\n","epoch 0, step: 94890/127656, loss: 0.009873520582914352\n","epoch 0, step: 94900/127656, loss: 0.010812513530254364\n","epoch 0, step: 94910/127656, loss: 0.3794282078742981\n","epoch 0, step: 94920/127656, loss: 0.01297431718558073\n","epoch 0, step: 94930/127656, loss: 0.14324577152729034\n","epoch 0, step: 94940/127656, loss: 0.00645058136433363\n","epoch 0, step: 94950/127656, loss: 0.008347907103598118\n","epoch 0, step: 94960/127656, loss: 0.010187674313783646\n","epoch 0, step: 94970/127656, loss: 0.4152947664260864\n","epoch 0, step: 94980/127656, loss: 0.016628321260213852\n","epoch 0, step: 94990/127656, loss: 0.14354507625102997\n","epoch 0, step: 95000/127656, loss: 0.39466676115989685\n","epoch 0, step: 95010/127656, loss: 0.007557201199233532\n","epoch 0, step: 95020/127656, loss: 0.007966877892613411\n","epoch 0, step: 95030/127656, loss: 0.014239415526390076\n","epoch 0, step: 95040/127656, loss: 0.010209424421191216\n","epoch 0, step: 95050/127656, loss: 0.010877260938286781\n","epoch 0, step: 95060/127656, loss: 0.012821482494473457\n","epoch 0, step: 95070/127656, loss: 0.010084539651870728\n","epoch 0, step: 95080/127656, loss: 0.006651062052696943\n","epoch 0, step: 95090/127656, loss: 0.008854839019477367\n","epoch 0, step: 95100/127656, loss: 0.013040591031312943\n","epoch 0, step: 95110/127656, loss: 0.007393486797809601\n","epoch 0, step: 95120/127656, loss: 0.019048023968935013\n","epoch 0, step: 95130/127656, loss: 0.5342800617218018\n","epoch 0, step: 95140/127656, loss: 0.007123743649572134\n","epoch 0, step: 95150/127656, loss: 0.011003091931343079\n","epoch 0, step: 95160/127656, loss: 0.006933572702109814\n","epoch 0, step: 95170/127656, loss: 0.011529478244483471\n","epoch 0, step: 95180/127656, loss: 0.013500755652785301\n","epoch 0, step: 95190/127656, loss: 0.0066928863525390625\n","epoch 0, step: 95200/127656, loss: 0.01092742383480072\n","epoch 0, step: 95210/127656, loss: 0.12594623863697052\n","epoch 0, step: 95220/127656, loss: 0.010315503925085068\n","epoch 0, step: 95230/127656, loss: 0.007524145767092705\n","epoch 0, step: 95240/127656, loss: 0.2773544192314148\n","epoch 0, step: 95250/127656, loss: 0.14196562767028809\n","epoch 0, step: 95260/127656, loss: 0.010669967159628868\n","epoch 0, step: 95270/127656, loss: 0.007228352129459381\n","epoch 0, step: 95280/127656, loss: 0.008601321838796139\n","epoch 0, step: 95290/127656, loss: 0.007769462652504444\n","epoch 0, step: 95300/127656, loss: 0.27409183979034424\n","epoch 0, step: 95310/127656, loss: 0.009509391151368618\n","epoch 0, step: 95320/127656, loss: 0.005916929803788662\n","epoch 0, step: 95330/127656, loss: 0.14049094915390015\n","epoch 0, step: 95340/127656, loss: 0.011811799369752407\n","epoch 0, step: 95350/127656, loss: 0.006573214195668697\n","epoch 0, step: 95360/127656, loss: 0.00600204523652792\n","epoch 0, step: 95370/127656, loss: 0.5512921214103699\n","epoch 0, step: 95380/127656, loss: 0.00760792288929224\n","epoch 0, step: 95390/127656, loss: 0.014169996604323387\n","epoch 0, step: 95400/127656, loss: 0.009218467399477959\n","epoch 0, step: 95410/127656, loss: 0.008152233436703682\n","epoch 0, step: 95420/127656, loss: 0.011074598878622055\n","epoch 0, step: 95430/127656, loss: 0.008373452350497246\n","epoch 0, step: 95440/127656, loss: 0.01068338006734848\n","epoch 0, step: 95450/127656, loss: 0.015646934509277344\n","epoch 0, step: 95460/127656, loss: 0.008622669614851475\n","epoch 0, step: 95470/127656, loss: 0.00569020863622427\n","epoch 0, step: 95480/127656, loss: 0.010896513238549232\n","epoch 0, step: 95490/127656, loss: 0.007944343611598015\n","epoch 0, step: 95500/127656, loss: 0.010830683633685112\n","epoch 0, step: 95510/127656, loss: 0.277423232793808\n","epoch 0, step: 95520/127656, loss: 0.2766629755496979\n","epoch 0, step: 95530/127656, loss: 0.1381140500307083\n","epoch 0, step: 95540/127656, loss: 0.015205900184810162\n","epoch 0, step: 95550/127656, loss: 0.13435906171798706\n","epoch 0, step: 95560/127656, loss: 0.00777576956897974\n","epoch 0, step: 95570/127656, loss: 0.012114521116018295\n","epoch 0, step: 95580/127656, loss: 0.1297140121459961\n","epoch 0, step: 95590/127656, loss: 0.011493313126266003\n","epoch 0, step: 95600/127656, loss: 0.009659163653850555\n","epoch 0, step: 95610/127656, loss: 0.012535709887742996\n","epoch 0, step: 95620/127656, loss: 0.011055314913392067\n","epoch 0, step: 95630/127656, loss: 0.008190568536520004\n","epoch 0, step: 95640/127656, loss: 0.007693859748542309\n","epoch 0, step: 95650/127656, loss: 0.007172952406108379\n","epoch 0, step: 95660/127656, loss: 0.009197098203003407\n","epoch 0, step: 95670/127656, loss: 0.007342488970607519\n","epoch 0, step: 95680/127656, loss: 0.008519785478711128\n","epoch 0, step: 95690/127656, loss: 0.012924056500196457\n","epoch 0, step: 95700/127656, loss: 0.011704374104738235\n","epoch 0, step: 95710/127656, loss: 0.01291816495358944\n","epoch 0, step: 95720/127656, loss: 0.006140401586890221\n","epoch 0, step: 95730/127656, loss: 0.8048602938652039\n","epoch 0, step: 95740/127656, loss: 0.00987870991230011\n","epoch 0, step: 95750/127656, loss: 0.009821462444961071\n","epoch 0, step: 95760/127656, loss: 0.013561559841036797\n","epoch 0, step: 95770/127656, loss: 0.012533064000308514\n","epoch 0, step: 95780/127656, loss: 0.012317124754190445\n","epoch 0, step: 95790/127656, loss: 0.009964782744646072\n","epoch 0, step: 95800/127656, loss: 0.011465460062026978\n","epoch 0, step: 95810/127656, loss: 0.13796068727970123\n","epoch 0, step: 95820/127656, loss: 0.008835667744278908\n","epoch 0, step: 95830/127656, loss: 0.007747130002826452\n","epoch 0, step: 95840/127656, loss: 0.01886129379272461\n","epoch 0, step: 95850/127656, loss: 0.008162861689925194\n","epoch 0, step: 95860/127656, loss: 0.008677052333950996\n","epoch 0, step: 95870/127656, loss: 0.010639364831149578\n","epoch 0, step: 95880/127656, loss: 0.01057715155184269\n","epoch 0, step: 95890/127656, loss: 0.13911589980125427\n","epoch 0, step: 95900/127656, loss: 0.012991263531148434\n","epoch 0, step: 95910/127656, loss: 0.008209148421883583\n","epoch 0, step: 95920/127656, loss: 0.010102402418851852\n","epoch 0, step: 95930/127656, loss: 0.012447278015315533\n","epoch 0, step: 95940/127656, loss: 0.1427692323923111\n","epoch 0, step: 95950/127656, loss: 0.009323853068053722\n","epoch 0, step: 95960/127656, loss: 0.1294001042842865\n","epoch 0, step: 95970/127656, loss: 0.2741767168045044\n","epoch 0, step: 95980/127656, loss: 0.14810779690742493\n","epoch 0, step: 95990/127656, loss: 0.011828607879579067\n","epoch 0, step: 96000/127656, loss: 0.009139037691056728\n","epoch 0, step: 96010/127656, loss: 0.009171328507363796\n","epoch 0, step: 96020/127656, loss: 0.010303189978003502\n","epoch 0, step: 96030/127656, loss: 0.007078216411173344\n","epoch 0, step: 96040/127656, loss: 0.0049506742507219315\n","epoch 0, step: 96050/127656, loss: 0.007287055253982544\n","epoch 0, step: 96060/127656, loss: 0.010503548197448254\n","epoch 0, step: 96070/127656, loss: 0.009784931316971779\n","epoch 0, step: 96080/127656, loss: 0.007939968258142471\n","epoch 0, step: 96090/127656, loss: 0.3821895122528076\n","epoch 0, step: 96100/127656, loss: 0.12923002243041992\n","epoch 0, step: 96110/127656, loss: 0.009754359722137451\n","epoch 0, step: 96120/127656, loss: 0.015093604102730751\n","epoch 0, step: 96130/127656, loss: 0.008999606594443321\n","epoch 0, step: 96140/127656, loss: 0.00882871262729168\n","epoch 0, step: 96150/127656, loss: 0.00984338577836752\n","epoch 0, step: 96160/127656, loss: 0.008154395967721939\n","epoch 0, step: 96170/127656, loss: 0.0061814747750759125\n","epoch 0, step: 96180/127656, loss: 0.004388284869492054\n","epoch 0, step: 96190/127656, loss: 0.01371081918478012\n","epoch 0, step: 96200/127656, loss: 0.01005193404853344\n","epoch 0, step: 96210/127656, loss: 0.007905460894107819\n","epoch 0, step: 96220/127656, loss: 0.007937893271446228\n","epoch 0, step: 96230/127656, loss: 0.019412077963352203\n","epoch 0, step: 96240/127656, loss: 0.007226872257888317\n","epoch 0, step: 96250/127656, loss: 0.008559256792068481\n","epoch 0, step: 96260/127656, loss: 0.01150733232498169\n","epoch 0, step: 96270/127656, loss: 0.01441238448023796\n","epoch 0, step: 96280/127656, loss: 0.006229551509022713\n","epoch 0, step: 96290/127656, loss: 0.006419407203793526\n","epoch 0, step: 96300/127656, loss: 0.012384402565658092\n","epoch 0, step: 96310/127656, loss: 0.008405140601098537\n","epoch 0, step: 96320/127656, loss: 0.011940445750951767\n","epoch 0, step: 96330/127656, loss: 0.01392302941530943\n","epoch 0, step: 96340/127656, loss: 0.01392848789691925\n","epoch 0, step: 96350/127656, loss: 0.013942728750407696\n","epoch 0, step: 96360/127656, loss: 0.013751751743257046\n","epoch 0, step: 96370/127656, loss: 0.006967358291149139\n","epoch 0, step: 96380/127656, loss: 0.00712237786501646\n","epoch 0, step: 96390/127656, loss: 0.5045777559280396\n","epoch 0, step: 96400/127656, loss: 0.011936190538108349\n","epoch 0, step: 96410/127656, loss: 0.014294186607003212\n","epoch 0, step: 96420/127656, loss: 0.395879328250885\n","epoch 0, step: 96430/127656, loss: 0.008113101124763489\n","epoch 0, step: 96440/127656, loss: 0.011348716914653778\n","epoch 0, step: 96450/127656, loss: 0.0092505794018507\n","epoch 0, step: 96460/127656, loss: 0.005429816897958517\n","epoch 0, step: 96470/127656, loss: 0.010516713373363018\n","epoch 0, step: 96480/127656, loss: 0.007519676350057125\n","epoch 0, step: 96490/127656, loss: 0.011349080130457878\n","epoch 0, step: 96500/127656, loss: 0.007444753777235746\n","epoch 0, step: 96510/127656, loss: 0.006081037223339081\n","epoch 0, step: 96520/127656, loss: 0.011690422892570496\n","epoch 0, step: 96530/127656, loss: 0.01143073569983244\n","epoch 0, step: 96540/127656, loss: 0.01056953426450491\n","epoch 0, step: 96550/127656, loss: 0.00894922949373722\n","epoch 0, step: 96560/127656, loss: 0.2818758189678192\n","epoch 0, step: 96570/127656, loss: 0.010051432996988297\n","epoch 0, step: 96580/127656, loss: 0.014972716569900513\n","epoch 0, step: 96590/127656, loss: 0.011604229919612408\n","epoch 0, step: 96600/127656, loss: 0.00791596807539463\n","epoch 0, step: 96610/127656, loss: 0.013669921085238457\n","epoch 0, step: 96620/127656, loss: 0.007326346822082996\n","epoch 0, step: 96630/127656, loss: 0.013384330086410046\n","epoch 0, step: 96640/127656, loss: 0.014290145598351955\n","epoch 0, step: 96650/127656, loss: 0.008781690150499344\n","epoch 0, step: 96660/127656, loss: 0.009074077010154724\n","epoch 0, step: 96670/127656, loss: 0.0073207952082157135\n","epoch 0, step: 96680/127656, loss: 0.010989516042172909\n","epoch 0, step: 96690/127656, loss: 0.018979204818606377\n","epoch 0, step: 96700/127656, loss: 0.007224114146083593\n","epoch 0, step: 96710/127656, loss: 0.005358241032809019\n","epoch 0, step: 96720/127656, loss: 0.005244077183306217\n","epoch 0, step: 96730/127656, loss: 0.007392018102109432\n","epoch 0, step: 96740/127656, loss: 0.007750685326755047\n","epoch 0, step: 96750/127656, loss: 0.008315734565258026\n","epoch 0, step: 96760/127656, loss: 0.008671272546052933\n","epoch 0, step: 96770/127656, loss: 0.007825635373592377\n","epoch 0, step: 96780/127656, loss: 0.009445007890462875\n","epoch 0, step: 96790/127656, loss: 0.011663614772260189\n","epoch 0, step: 96800/127656, loss: 0.009508291259407997\n","epoch 0, step: 96810/127656, loss: 0.5193424224853516\n","epoch 0, step: 96820/127656, loss: 0.005892433226108551\n","epoch 0, step: 96830/127656, loss: 0.00960357766598463\n","epoch 0, step: 96840/127656, loss: 0.00789971835911274\n","epoch 0, step: 96850/127656, loss: 0.012646779417991638\n","epoch 0, step: 96860/127656, loss: 0.006142813712358475\n","epoch 0, step: 96870/127656, loss: 0.012900936417281628\n","epoch 0, step: 96880/127656, loss: 0.008814819157123566\n","epoch 0, step: 96890/127656, loss: 0.00897211953997612\n","epoch 0, step: 96900/127656, loss: 0.007973197847604752\n","epoch 0, step: 96910/127656, loss: 0.007355082780122757\n","epoch 0, step: 96920/127656, loss: 0.5208954811096191\n","epoch 0, step: 96930/127656, loss: 0.015728263184428215\n","epoch 0, step: 96940/127656, loss: 0.37358468770980835\n","epoch 0, step: 96950/127656, loss: 0.008257241919636726\n","epoch 0, step: 96960/127656, loss: 0.005955158732831478\n","epoch 0, step: 96970/127656, loss: 0.009646711871027946\n","epoch 0, step: 96980/127656, loss: 0.013176126405596733\n","epoch 0, step: 96990/127656, loss: 0.006518342532217503\n","epoch 0, step: 97000/127656, loss: 0.00884997844696045\n","epoch 0, step: 97010/127656, loss: 0.008786162361502647\n","epoch 0, step: 97020/127656, loss: 0.5796050429344177\n","epoch 0, step: 97030/127656, loss: 0.007234133780002594\n","epoch 0, step: 97040/127656, loss: 0.4225863814353943\n","epoch 0, step: 97050/127656, loss: 0.010451356880366802\n","epoch 0, step: 97060/127656, loss: 0.01202835887670517\n","epoch 0, step: 97070/127656, loss: 0.009053900837898254\n","epoch 0, step: 97080/127656, loss: 0.010595942847430706\n","epoch 0, step: 97090/127656, loss: 0.007814290001988411\n","epoch 0, step: 97100/127656, loss: 0.011717235669493675\n","epoch 0, step: 97110/127656, loss: 0.008480449207127094\n","epoch 0, step: 97120/127656, loss: 0.011140869930386543\n","epoch 0, step: 97130/127656, loss: 0.009929618798196316\n","epoch 0, step: 97140/127656, loss: 0.010050135664641857\n","epoch 0, step: 97150/127656, loss: 0.014151059091091156\n","epoch 0, step: 97160/127656, loss: 0.008889548480510712\n","epoch 0, step: 97170/127656, loss: 0.3964274823665619\n","epoch 0, step: 97180/127656, loss: 0.010360217653214931\n","epoch 0, step: 97190/127656, loss: 0.010551678948104382\n","epoch 0, step: 97200/127656, loss: 0.00917510874569416\n","epoch 0, step: 97210/127656, loss: 0.00929991528391838\n","epoch 0, step: 97220/127656, loss: 0.009694291278719902\n","epoch 0, step: 97230/127656, loss: 0.2589408755302429\n","epoch 0, step: 97240/127656, loss: 0.1399187445640564\n","epoch 0, step: 97250/127656, loss: 0.008887030184268951\n","epoch 0, step: 97260/127656, loss: 0.01274256594479084\n","epoch 0, step: 97270/127656, loss: 0.008582331240177155\n","epoch 0, step: 97280/127656, loss: 0.00751919811591506\n","epoch 0, step: 97290/127656, loss: 0.0068163685500621796\n","epoch 0, step: 97300/127656, loss: 0.008777787908911705\n","epoch 0, step: 97310/127656, loss: 0.12529659271240234\n","epoch 0, step: 97320/127656, loss: 0.012398889288306236\n","epoch 0, step: 97330/127656, loss: 0.14079269766807556\n","epoch 0, step: 97340/127656, loss: 0.009075832553207874\n","epoch 0, step: 97350/127656, loss: 0.005905258469283581\n","epoch 0, step: 97360/127656, loss: 0.007621139287948608\n","epoch 0, step: 97370/127656, loss: 0.006287341937422752\n","epoch 0, step: 97380/127656, loss: 0.011930837295949459\n","epoch 0, step: 97390/127656, loss: 0.00870523601770401\n","epoch 0, step: 97400/127656, loss: 0.011518923565745354\n","epoch 0, step: 97410/127656, loss: 0.008981592021882534\n","epoch 0, step: 97420/127656, loss: 0.00641916086897254\n","epoch 0, step: 97430/127656, loss: 0.009977992624044418\n","epoch 0, step: 97440/127656, loss: 0.0077621592208743095\n","epoch 0, step: 97450/127656, loss: 0.008017057552933693\n","epoch 0, step: 97460/127656, loss: 0.006607828661799431\n","epoch 0, step: 97470/127656, loss: 0.009371452033519745\n","epoch 0, step: 97480/127656, loss: 0.010983813554048538\n","epoch 0, step: 97490/127656, loss: 0.00970272347331047\n","epoch 0, step: 97500/127656, loss: 0.009694565087556839\n","epoch 0, step: 97510/127656, loss: 0.14523416757583618\n","epoch 0, step: 97520/127656, loss: 0.011673140339553356\n","epoch 0, step: 97530/127656, loss: 0.005732445977628231\n","epoch 0, step: 97540/127656, loss: 0.008263765834271908\n","epoch 0, step: 97550/127656, loss: 0.014879189431667328\n","epoch 0, step: 97560/127656, loss: 0.007739376276731491\n","epoch 0, step: 97570/127656, loss: 0.013072437606751919\n","epoch 0, step: 97580/127656, loss: 0.01167276781052351\n","epoch 0, step: 97590/127656, loss: 0.009700956754386425\n","epoch 0, step: 97600/127656, loss: 0.011446749791502953\n","epoch 0, step: 97610/127656, loss: 0.008246669545769691\n","epoch 0, step: 97620/127656, loss: 0.00908453669399023\n","epoch 0, step: 97630/127656, loss: 0.006287219934165478\n","epoch 0, step: 97640/127656, loss: 0.010761957615613937\n","epoch 0, step: 97650/127656, loss: 0.13372620940208435\n","epoch 0, step: 97660/127656, loss: 0.14544764161109924\n","epoch 0, step: 97670/127656, loss: 0.007530983071774244\n","epoch 0, step: 97680/127656, loss: 0.01320702861994505\n","epoch 0, step: 97690/127656, loss: 0.011141963303089142\n","epoch 0, step: 97700/127656, loss: 0.004053701180964708\n","epoch 0, step: 97710/127656, loss: 0.008169787935912609\n","epoch 0, step: 97720/127656, loss: 0.014660975895822048\n","epoch 0, step: 97730/127656, loss: 0.009091086685657501\n","epoch 0, step: 97740/127656, loss: 0.13388501107692719\n","epoch 0, step: 97750/127656, loss: 0.008484077639877796\n","epoch 0, step: 97760/127656, loss: 0.005333637353032827\n","epoch 0, step: 97770/127656, loss: 0.008782899007201195\n","epoch 0, step: 97780/127656, loss: 0.013350638560950756\n","epoch 0, step: 97790/127656, loss: 0.010659994557499886\n","epoch 0, step: 97800/127656, loss: 0.013159855268895626\n","epoch 0, step: 97810/127656, loss: 0.0069365957751870155\n","epoch 0, step: 97820/127656, loss: 0.007005307357758284\n","epoch 0, step: 97830/127656, loss: 0.009594915434718132\n","epoch 0, step: 97840/127656, loss: 0.0033899869304150343\n","epoch 0, step: 97850/127656, loss: 0.007087647449225187\n","epoch 0, step: 97860/127656, loss: 0.009119945578277111\n","epoch 0, step: 97870/127656, loss: 0.011577161960303783\n","epoch 0, step: 97880/127656, loss: 0.27910932898521423\n","epoch 0, step: 97890/127656, loss: 0.01111677847802639\n","epoch 0, step: 97900/127656, loss: 0.008540420792996883\n","epoch 0, step: 97910/127656, loss: 0.012150762602686882\n","epoch 0, step: 97920/127656, loss: 0.012552338652312756\n","epoch 0, step: 97930/127656, loss: 0.006699285935610533\n","epoch 0, step: 97940/127656, loss: 0.009799725376069546\n","epoch 0, step: 97950/127656, loss: 0.007393138017505407\n","epoch 0, step: 97960/127656, loss: 0.010095058009028435\n","epoch 0, step: 97970/127656, loss: 0.010939552448689938\n","epoch 0, step: 97980/127656, loss: 0.00829862616956234\n","epoch 0, step: 97990/127656, loss: 0.013217585161328316\n","epoch 0, step: 98000/127656, loss: 0.008509436622262001\n","epoch 0, step: 98010/127656, loss: 0.012598225846886635\n","epoch 0, step: 98020/127656, loss: 0.006242201663553715\n","epoch 0, step: 98030/127656, loss: 0.00821674894541502\n","epoch 0, step: 98040/127656, loss: 0.3777562379837036\n","epoch 0, step: 98050/127656, loss: 0.008906367234885693\n","epoch 0, step: 98060/127656, loss: 0.006936906836926937\n","epoch 0, step: 98070/127656, loss: 0.12796126306056976\n","epoch 0, step: 98080/127656, loss: 0.010677685961127281\n","epoch 0, step: 98090/127656, loss: 0.006029549054801464\n","epoch 0, step: 98100/127656, loss: 0.5093017816543579\n","epoch 0, step: 98110/127656, loss: 0.007876662537455559\n","epoch 0, step: 98120/127656, loss: 0.011516308411955833\n","epoch 0, step: 98130/127656, loss: 0.01150632556527853\n","epoch 0, step: 98140/127656, loss: 0.0093391640111804\n","epoch 0, step: 98150/127656, loss: 0.010042168200016022\n","epoch 0, step: 98160/127656, loss: 0.011659021489322186\n","epoch 0, step: 98170/127656, loss: 0.14636515080928802\n","epoch 0, step: 98180/127656, loss: 0.008941465057432652\n","epoch 0, step: 98190/127656, loss: 0.010837639681994915\n","epoch 0, step: 98200/127656, loss: 0.012559419497847557\n","epoch 0, step: 98210/127656, loss: 0.00765260448679328\n","epoch 0, step: 98220/127656, loss: 0.01140020601451397\n","epoch 0, step: 98230/127656, loss: 0.009373540058732033\n","epoch 0, step: 98240/127656, loss: 0.009025780484080315\n","epoch 0, step: 98250/127656, loss: 0.2667429447174072\n","epoch 0, step: 98260/127656, loss: 0.5581225752830505\n","epoch 0, step: 98270/127656, loss: 0.009142973460257053\n","epoch 0, step: 98280/127656, loss: 0.015635769814252853\n","epoch 0, step: 98290/127656, loss: 0.009691612794995308\n","epoch 0, step: 98300/127656, loss: 0.009189244359731674\n","epoch 0, step: 98310/127656, loss: 0.011373120360076427\n","epoch 0, step: 98320/127656, loss: 0.007189227268099785\n","epoch 0, step: 98330/127656, loss: 0.014987332746386528\n","epoch 0, step: 98340/127656, loss: 0.007587318774312735\n","epoch 0, step: 98350/127656, loss: 0.007297347765415907\n","epoch 0, step: 98360/127656, loss: 0.005613122135400772\n","epoch 0, step: 98370/127656, loss: 0.009088285267353058\n","epoch 0, step: 98380/127656, loss: 0.011572450399398804\n","epoch 0, step: 98390/127656, loss: 0.011639407835900784\n","epoch 0, step: 98400/127656, loss: 0.008879099041223526\n","epoch 0, step: 98410/127656, loss: 0.010373787954449654\n","epoch 0, step: 98420/127656, loss: 0.4229833781719208\n","epoch 0, step: 98430/127656, loss: 0.010351983830332756\n","epoch 0, step: 98440/127656, loss: 0.008327817544341087\n","epoch 0, step: 98450/127656, loss: 0.009821109473705292\n","epoch 0, step: 98460/127656, loss: 0.008878123015165329\n","epoch 0, step: 98470/127656, loss: 0.01103189866989851\n","epoch 0, step: 98480/127656, loss: 0.008531034924089909\n","epoch 0, step: 98490/127656, loss: 0.005893006920814514\n","epoch 0, step: 98500/127656, loss: 0.009251037612557411\n","epoch 0, step: 98510/127656, loss: 0.2733795642852783\n","epoch 0, step: 98520/127656, loss: 0.011454089544713497\n","epoch 0, step: 98530/127656, loss: 0.009920787997543812\n","epoch 0, step: 98540/127656, loss: 0.0064682429656386375\n","epoch 0, step: 98550/127656, loss: 0.01096310280263424\n","epoch 0, step: 98560/127656, loss: 0.006963895633816719\n","epoch 0, step: 98570/127656, loss: 0.008972261101007462\n","epoch 0, step: 98580/127656, loss: 0.009375005960464478\n","epoch 0, step: 98590/127656, loss: 0.009654929861426353\n","epoch 0, step: 98600/127656, loss: 0.00979476049542427\n","epoch 0, step: 98610/127656, loss: 0.01000901311635971\n","epoch 0, step: 98620/127656, loss: 0.009558921679854393\n","epoch 0, step: 98630/127656, loss: 0.0057007865980267525\n","epoch 0, step: 98640/127656, loss: 0.009130198508501053\n","epoch 0, step: 98650/127656, loss: 0.014448600821197033\n","epoch 0, step: 98660/127656, loss: 0.007479480467736721\n","epoch 0, step: 98670/127656, loss: 0.005141526460647583\n","epoch 0, step: 98680/127656, loss: 0.011254003271460533\n","epoch 0, step: 98690/127656, loss: 0.011482677422463894\n","epoch 0, step: 98700/127656, loss: 0.0065015098080039024\n","epoch 0, step: 98710/127656, loss: 0.008375410921871662\n","epoch 0, step: 98720/127656, loss: 0.4295794367790222\n","epoch 0, step: 98730/127656, loss: 0.010428717359900475\n","epoch 0, step: 98740/127656, loss: 0.00832636933773756\n","epoch 0, step: 98750/127656, loss: 0.009974716231226921\n","epoch 0, step: 98760/127656, loss: 0.009153160266578197\n","epoch 0, step: 98770/127656, loss: 0.007679092697799206\n","epoch 0, step: 98780/127656, loss: 0.017244739457964897\n","epoch 0, step: 98790/127656, loss: 0.009087460115551949\n","epoch 0, step: 98800/127656, loss: 0.009579497389495373\n","epoch 0, step: 98810/127656, loss: 0.01059778779745102\n","epoch 0, step: 98820/127656, loss: 0.009921465069055557\n","epoch 0, step: 98830/127656, loss: 0.011387297883629799\n","epoch 0, step: 98840/127656, loss: 0.00684854481369257\n","epoch 0, step: 98850/127656, loss: 0.011880978010594845\n","epoch 0, step: 98860/127656, loss: 0.007791526615619659\n","epoch 0, step: 98870/127656, loss: 0.00616822112351656\n","epoch 0, step: 98880/127656, loss: 0.010355050675570965\n","epoch 0, step: 98890/127656, loss: 0.007334083318710327\n","epoch 0, step: 98900/127656, loss: 0.008804554119706154\n","epoch 0, step: 98910/127656, loss: 0.5534334182739258\n","epoch 0, step: 98920/127656, loss: 0.011635726317763329\n","epoch 0, step: 98930/127656, loss: 0.007304886355996132\n","epoch 0, step: 98940/127656, loss: 0.008505081757903099\n","epoch 0, step: 98950/127656, loss: 0.012415824458003044\n","epoch 0, step: 98960/127656, loss: 0.01743757352232933\n","epoch 0, step: 98970/127656, loss: 0.5597736239433289\n","epoch 0, step: 98980/127656, loss: 0.00807339046150446\n","epoch 0, step: 98990/127656, loss: 0.00579974427819252\n","epoch 0, step: 99000/127656, loss: 0.010192318819463253\n","epoch 0, step: 99010/127656, loss: 0.2839474678039551\n","epoch 0, step: 99020/127656, loss: 0.3945375084877014\n","epoch 0, step: 99030/127656, loss: 0.01078515499830246\n","epoch 0, step: 99040/127656, loss: 0.018057670444250107\n","epoch 0, step: 99050/127656, loss: 0.008484965190291405\n","epoch 0, step: 99060/127656, loss: 0.009780334308743477\n","epoch 0, step: 99070/127656, loss: 0.009952791035175323\n","epoch 0, step: 99080/127656, loss: 0.012415256351232529\n","epoch 0, step: 99090/127656, loss: 0.009112315252423286\n","epoch 0, step: 99100/127656, loss: 0.010543002746999264\n","epoch 0, step: 99110/127656, loss: 0.00807240977883339\n","epoch 0, step: 99120/127656, loss: 0.007667239755392075\n","epoch 0, step: 99130/127656, loss: 0.009007716551423073\n","epoch 0, step: 99140/127656, loss: 0.010463888756930828\n","epoch 0, step: 99150/127656, loss: 0.003712903242558241\n","epoch 0, step: 99160/127656, loss: 0.006706336047500372\n","epoch 0, step: 99170/127656, loss: 0.010199422016739845\n","epoch 0, step: 99180/127656, loss: 0.011070474982261658\n","epoch 0, step: 99190/127656, loss: 0.012300686910748482\n","epoch 0, step: 99200/127656, loss: 0.011475643143057823\n","epoch 0, step: 99210/127656, loss: 0.008214371278882027\n","epoch 0, step: 99220/127656, loss: 0.5428485870361328\n","epoch 0, step: 99230/127656, loss: 0.01059205736964941\n","epoch 0, step: 99240/127656, loss: 0.01015520840883255\n","epoch 0, step: 99250/127656, loss: 0.009921768680214882\n","epoch 0, step: 99260/127656, loss: 0.008123030886054039\n","epoch 0, step: 99270/127656, loss: 0.014644881710410118\n","epoch 0, step: 99280/127656, loss: 0.26598817110061646\n","epoch 0, step: 99290/127656, loss: 0.0072544896975159645\n","epoch 0, step: 99300/127656, loss: 0.006265274249017239\n","epoch 0, step: 99310/127656, loss: 0.011491014622151852\n","epoch 0, step: 99320/127656, loss: 0.007744172587990761\n","epoch 0, step: 99330/127656, loss: 0.014043038710951805\n","epoch 0, step: 99340/127656, loss: 0.008566400036215782\n","epoch 0, step: 99350/127656, loss: 0.010035084560513496\n","epoch 0, step: 99360/127656, loss: 0.41419312357902527\n","epoch 0, step: 99370/127656, loss: 0.009779538959264755\n","epoch 0, step: 99380/127656, loss: 0.009441129863262177\n","epoch 0, step: 99390/127656, loss: 0.009813293814659119\n","epoch 0, step: 99400/127656, loss: 0.00801037810742855\n","epoch 0, step: 99410/127656, loss: 0.01266392320394516\n","epoch 0, step: 99420/127656, loss: 0.007191315293312073\n","epoch 0, step: 99430/127656, loss: 0.012499495409429073\n","epoch 0, step: 99440/127656, loss: 0.1339361071586609\n","epoch 0, step: 99450/127656, loss: 0.010055886581540108\n","epoch 0, step: 99460/127656, loss: 0.007548964582383633\n","epoch 0, step: 99470/127656, loss: 0.011078661307692528\n","epoch 0, step: 99480/127656, loss: 0.007799703162163496\n","epoch 0, step: 99490/127656, loss: 0.014260846190154552\n","epoch 0, step: 99500/127656, loss: 0.14269620180130005\n","epoch 0, step: 99510/127656, loss: 0.00840378925204277\n","epoch 0, step: 99520/127656, loss: 0.012836865149438381\n","epoch 0, step: 99530/127656, loss: 0.00933697447180748\n","epoch 0, step: 99540/127656, loss: 0.007749506738036871\n","epoch 0, step: 99550/127656, loss: 0.012596270069479942\n","epoch 0, step: 99560/127656, loss: 0.00833069533109665\n","epoch 0, step: 99570/127656, loss: 0.0076795644126832485\n","epoch 0, step: 99580/127656, loss: 0.010570804588496685\n","epoch 0, step: 99590/127656, loss: 0.015176268294453621\n","epoch 0, step: 99600/127656, loss: 0.010283012874424458\n","epoch 0, step: 99610/127656, loss: 0.011051572859287262\n","epoch 0, step: 99620/127656, loss: 0.008593015372753143\n","epoch 0, step: 99630/127656, loss: 0.0068018147721886635\n","epoch 0, step: 99640/127656, loss: 0.007357331924140453\n","epoch 0, step: 99650/127656, loss: 0.007534536998718977\n","epoch 0, step: 99660/127656, loss: 0.009193740785121918\n","epoch 0, step: 99670/127656, loss: 0.008694063872098923\n","epoch 0, step: 99680/127656, loss: 0.007048199884593487\n","epoch 0, step: 99690/127656, loss: 0.006676127202808857\n","epoch 0, step: 99700/127656, loss: 0.007992718368768692\n","epoch 0, step: 99710/127656, loss: 0.01202190201729536\n","epoch 0, step: 99720/127656, loss: 0.008945874869823456\n","epoch 0, step: 99730/127656, loss: 0.011098239570856094\n","epoch 0, step: 99740/127656, loss: 0.013937133364379406\n","epoch 0, step: 99750/127656, loss: 0.012058703228831291\n","epoch 0, step: 99760/127656, loss: 0.009895573370158672\n","epoch 0, step: 99770/127656, loss: 0.1402725726366043\n","epoch 0, step: 99780/127656, loss: 0.006976163946092129\n","epoch 0, step: 99790/127656, loss: 0.006445775739848614\n","epoch 0, step: 99800/127656, loss: 0.0070447493344545364\n","epoch 0, step: 99810/127656, loss: 0.00765692163258791\n","epoch 0, step: 99820/127656, loss: 0.010807494632899761\n","epoch 0, step: 99830/127656, loss: 0.015726475045084953\n","epoch 0, step: 99840/127656, loss: 0.00900376308709383\n","epoch 0, step: 99850/127656, loss: 0.013236275874078274\n","epoch 0, step: 99860/127656, loss: 0.13841331005096436\n","epoch 0, step: 99870/127656, loss: 0.2664831578731537\n","epoch 0, step: 99880/127656, loss: 0.0057343486696481705\n","epoch 0, step: 99890/127656, loss: 0.007240756414830685\n","epoch 0, step: 99900/127656, loss: 0.006968513131141663\n","epoch 0, step: 99910/127656, loss: 0.013947194442152977\n","epoch 0, step: 99920/127656, loss: 0.009627833031117916\n","epoch 0, step: 99930/127656, loss: 0.007668397389352322\n","epoch 0, step: 99940/127656, loss: 0.0069794729351997375\n","epoch 0, step: 99950/127656, loss: 0.010987428948283195\n","epoch 0, step: 99960/127656, loss: 0.008852019906044006\n","epoch 0, step: 99970/127656, loss: 0.009982874616980553\n","epoch 0, step: 99980/127656, loss: 0.006549895741045475\n","epoch 0, step: 99990/127656, loss: 0.009056754410266876\n","epoch 0, step: 100000/127656, loss: 0.007893450558185577\n","epoch 0, step: 100010/127656, loss: 0.01118413358926773\n","epoch 0, step: 100020/127656, loss: 0.013669883832335472\n","epoch 0, step: 100030/127656, loss: 0.006346989423036575\n","epoch 0, step: 100040/127656, loss: 0.010155822150409222\n","epoch 0, step: 100050/127656, loss: 0.007481766398996115\n","epoch 0, step: 100060/127656, loss: 0.008610136806964874\n","epoch 0, step: 100070/127656, loss: 0.012640811502933502\n","epoch 0, step: 100080/127656, loss: 0.010402638465166092\n","epoch 0, step: 100090/127656, loss: 0.011926298961043358\n","epoch 0, step: 100100/127656, loss: 0.006162197329103947\n","epoch 0, step: 100110/127656, loss: 0.008108028210699558\n","epoch 0, step: 100120/127656, loss: 0.00792701169848442\n","epoch 0, step: 100130/127656, loss: 0.005552919115871191\n","epoch 0, step: 100140/127656, loss: 0.009862787090241909\n","epoch 0, step: 100150/127656, loss: 0.011819997802376747\n","epoch 0, step: 100160/127656, loss: 0.010311642661690712\n","epoch 0, step: 100170/127656, loss: 0.017162449657917023\n","epoch 0, step: 100180/127656, loss: 0.008044587448239326\n","epoch 0, step: 100190/127656, loss: 0.007543361745774746\n","epoch 0, step: 100200/127656, loss: 0.006998931523412466\n","epoch 0, step: 100210/127656, loss: 0.01903773657977581\n","epoch 0, step: 100220/127656, loss: 0.007341818418353796\n","epoch 0, step: 100230/127656, loss: 0.010360188782215118\n","epoch 0, step: 100240/127656, loss: 0.27754342555999756\n","epoch 0, step: 100250/127656, loss: 0.007824895903468132\n","epoch 0, step: 100260/127656, loss: 0.011316992342472076\n","epoch 0, step: 100270/127656, loss: 0.005628635641187429\n","epoch 0, step: 100280/127656, loss: 0.4097597599029541\n","epoch 0, step: 100290/127656, loss: 0.007742529734969139\n","epoch 0, step: 100300/127656, loss: 0.007662850432097912\n","epoch 0, step: 100310/127656, loss: 0.010453696362674236\n","epoch 0, step: 100320/127656, loss: 0.00816621445119381\n","epoch 0, step: 100330/127656, loss: 0.0067199221812188625\n","epoch 0, step: 100340/127656, loss: 0.010300315916538239\n","epoch 0, step: 100350/127656, loss: 0.006228931248188019\n","epoch 0, step: 100360/127656, loss: 0.013320687226951122\n","epoch 0, step: 100370/127656, loss: 0.008414039388298988\n","epoch 0, step: 100380/127656, loss: 0.009005993604660034\n","epoch 0, step: 100390/127656, loss: 0.00862053968012333\n","epoch 0, step: 100400/127656, loss: 0.01173519715666771\n","epoch 0, step: 100410/127656, loss: 0.007944872602820396\n","epoch 0, step: 100420/127656, loss: 0.006551242433488369\n","epoch 0, step: 100430/127656, loss: 0.008887071162462234\n","epoch 0, step: 100440/127656, loss: 0.006673878990113735\n","epoch 0, step: 100450/127656, loss: 0.0063287802040576935\n","epoch 0, step: 100460/127656, loss: 0.007773471996188164\n","epoch 0, step: 100470/127656, loss: 0.01226803194731474\n","epoch 0, step: 100480/127656, loss: 0.007691953331232071\n","epoch 0, step: 100490/127656, loss: 0.1511746644973755\n","epoch 0, step: 100500/127656, loss: 0.009932342916727066\n","epoch 0, step: 100510/127656, loss: 0.011320937424898148\n","epoch 0, step: 100520/127656, loss: 0.006133453454822302\n","epoch 0, step: 100530/127656, loss: 0.004953343421220779\n","epoch 0, step: 100540/127656, loss: 0.009420911781489849\n","epoch 0, step: 100550/127656, loss: 0.011610030196607113\n","epoch 0, step: 100560/127656, loss: 0.008316988125443459\n","epoch 0, step: 100570/127656, loss: 0.1340000033378601\n","epoch 0, step: 100580/127656, loss: 0.008902298286557198\n","epoch 0, step: 100590/127656, loss: 0.012891404330730438\n","epoch 0, step: 100600/127656, loss: 0.408783495426178\n","epoch 0, step: 100610/127656, loss: 0.007270186208188534\n","epoch 0, step: 100620/127656, loss: 0.008520931005477905\n","epoch 0, step: 100630/127656, loss: 0.13280445337295532\n","epoch 0, step: 100640/127656, loss: 0.009733395650982857\n","epoch 0, step: 100650/127656, loss: 0.006630342453718185\n","epoch 0, step: 100660/127656, loss: 0.009805982932448387\n","epoch 0, step: 100670/127656, loss: 0.29360780119895935\n","epoch 0, step: 100680/127656, loss: 0.011606452986598015\n","epoch 0, step: 100690/127656, loss: 0.007930431514978409\n","epoch 0, step: 100700/127656, loss: 0.009873848408460617\n","epoch 0, step: 100710/127656, loss: 0.00978028029203415\n","epoch 0, step: 100720/127656, loss: 0.01219404861330986\n","epoch 0, step: 100730/127656, loss: 0.01153077743947506\n","epoch 0, step: 100740/127656, loss: 0.0045722974464297295\n","epoch 0, step: 100750/127656, loss: 0.005637232214212418\n","epoch 0, step: 100760/127656, loss: 0.008461755700409412\n","epoch 0, step: 100770/127656, loss: 0.01126539334654808\n","epoch 0, step: 100780/127656, loss: 0.010440431535243988\n","epoch 0, step: 100790/127656, loss: 0.008523466065526009\n","epoch 0, step: 100800/127656, loss: 0.011860187165439129\n","epoch 0, step: 100810/127656, loss: 0.1291242092847824\n","epoch 0, step: 100820/127656, loss: 0.012470029294490814\n","epoch 0, step: 100830/127656, loss: 0.008022790774703026\n","epoch 0, step: 100840/127656, loss: 0.008434886112809181\n","epoch 0, step: 100850/127656, loss: 0.01499475073069334\n","epoch 0, step: 100860/127656, loss: 0.00972179975360632\n","epoch 0, step: 100870/127656, loss: 0.4139387607574463\n","epoch 0, step: 100880/127656, loss: 0.2798954248428345\n","epoch 0, step: 100890/127656, loss: 0.005644659511744976\n","epoch 0, step: 100900/127656, loss: 0.009547974914312363\n","epoch 0, step: 100910/127656, loss: 0.014397363178431988\n","epoch 0, step: 100920/127656, loss: 0.01096244528889656\n","epoch 0, step: 100930/127656, loss: 0.010878283530473709\n","epoch 0, step: 100940/127656, loss: 0.009373709559440613\n","epoch 0, step: 100950/127656, loss: 0.28361114859580994\n","epoch 0, step: 100960/127656, loss: 0.01008974015712738\n","epoch 0, step: 100970/127656, loss: 0.00617807824164629\n","epoch 0, step: 100980/127656, loss: 0.007728061638772488\n","epoch 0, step: 100990/127656, loss: 0.00987011194229126\n","epoch 0, step: 101000/127656, loss: 0.006421424448490143\n","epoch 0, step: 101010/127656, loss: 0.0062123751267790794\n","epoch 0, step: 101020/127656, loss: 0.008398069068789482\n","epoch 0, step: 101030/127656, loss: 0.3869938254356384\n","epoch 0, step: 101040/127656, loss: 0.005424977745860815\n","epoch 0, step: 101050/127656, loss: 0.008108248002827168\n","epoch 0, step: 101060/127656, loss: 0.007917994633316994\n","epoch 0, step: 101070/127656, loss: 0.008979935199022293\n","epoch 0, step: 101080/127656, loss: 0.006568229757249355\n","epoch 0, step: 101090/127656, loss: 0.014348792843520641\n","epoch 0, step: 101100/127656, loss: 0.005926894024014473\n","epoch 0, step: 101110/127656, loss: 0.007768144831061363\n","epoch 0, step: 101120/127656, loss: 0.010611589066684246\n","epoch 0, step: 101130/127656, loss: 0.01129552349448204\n","epoch 0, step: 101140/127656, loss: 0.005361396353691816\n","epoch 0, step: 101150/127656, loss: 0.0065281083807349205\n","epoch 0, step: 101160/127656, loss: 0.009575854986906052\n","epoch 0, step: 101170/127656, loss: 0.012633170001208782\n","epoch 0, step: 101180/127656, loss: 0.0078052799217402935\n","epoch 0, step: 101190/127656, loss: 0.006878342479467392\n","epoch 0, step: 101200/127656, loss: 0.011677011847496033\n","epoch 0, step: 101210/127656, loss: 0.011711913160979748\n","epoch 0, step: 101220/127656, loss: 0.26514092087745667\n","epoch 0, step: 101230/127656, loss: 0.007479407824575901\n","epoch 0, step: 101240/127656, loss: 0.2930983901023865\n","epoch 0, step: 101250/127656, loss: 0.009201345965266228\n","epoch 0, step: 101260/127656, loss: 0.008045095950365067\n","epoch 0, step: 101270/127656, loss: 0.27158719301223755\n","epoch 0, step: 101280/127656, loss: 0.00526770856231451\n","epoch 0, step: 101290/127656, loss: 0.007702288217842579\n","epoch 0, step: 101300/127656, loss: 0.010543206706643105\n","epoch 0, step: 101310/127656, loss: 0.005089982878416777\n","epoch 0, step: 101320/127656, loss: 0.006249380763620138\n","epoch 0, step: 101330/127656, loss: 0.01431337557733059\n","epoch 0, step: 101340/127656, loss: 0.010268221609294415\n","epoch 0, step: 101350/127656, loss: 0.00754854641854763\n","epoch 0, step: 101360/127656, loss: 0.014415948651731014\n","epoch 0, step: 101370/127656, loss: 0.008050383999943733\n","epoch 0, step: 101380/127656, loss: 0.015462893061339855\n","epoch 0, step: 101390/127656, loss: 0.40318048000335693\n","epoch 0, step: 101400/127656, loss: 0.01299003791064024\n","epoch 0, step: 101410/127656, loss: 0.006162741687148809\n","epoch 0, step: 101420/127656, loss: 0.00638122484087944\n","epoch 0, step: 101430/127656, loss: 0.012116453610360622\n","epoch 0, step: 101440/127656, loss: 0.00657528406009078\n","epoch 0, step: 101450/127656, loss: 0.006764660589396954\n","epoch 0, step: 101460/127656, loss: 0.011855006217956543\n","epoch 0, step: 101470/127656, loss: 0.012664500623941422\n","epoch 0, step: 101480/127656, loss: 0.009301329031586647\n","epoch 0, step: 101490/127656, loss: 0.01163377147167921\n","epoch 0, step: 101500/127656, loss: 0.009325764141976833\n","epoch 0, step: 101510/127656, loss: 0.0068588401190936565\n","epoch 0, step: 101520/127656, loss: 0.008851997554302216\n","epoch 0, step: 101530/127656, loss: 0.008514619432389736\n","epoch 0, step: 101540/127656, loss: 0.00891028344631195\n","epoch 0, step: 101550/127656, loss: 0.006970684044063091\n","epoch 0, step: 101560/127656, loss: 0.006677315570414066\n","epoch 0, step: 101570/127656, loss: 0.00507021602243185\n","epoch 0, step: 101580/127656, loss: 0.014675488695502281\n","epoch 0, step: 101590/127656, loss: 0.005292384885251522\n","epoch 0, step: 101600/127656, loss: 0.012366054579615593\n","epoch 0, step: 101610/127656, loss: 0.004727967083454132\n","epoch 0, step: 101620/127656, loss: 0.00881766527891159\n","epoch 0, step: 101630/127656, loss: 0.011556239798665047\n","epoch 0, step: 101640/127656, loss: 0.0075390879064798355\n","epoch 0, step: 101650/127656, loss: 0.008384985849261284\n","epoch 0, step: 101660/127656, loss: 0.005935445427894592\n","epoch 0, step: 101670/127656, loss: 0.008082946762442589\n","epoch 0, step: 101680/127656, loss: 0.0110468789935112\n","epoch 0, step: 101690/127656, loss: 0.006632796488702297\n","epoch 0, step: 101700/127656, loss: 0.007239182945340872\n","epoch 0, step: 101710/127656, loss: 0.009834406897425652\n","epoch 0, step: 101720/127656, loss: 0.006591279059648514\n","epoch 0, step: 101730/127656, loss: 0.010796982795000076\n","epoch 0, step: 101740/127656, loss: 0.009923841804265976\n","epoch 0, step: 101750/127656, loss: 0.008619057945907116\n","epoch 0, step: 101760/127656, loss: 0.008379518985748291\n","epoch 0, step: 101770/127656, loss: 0.012188058346509933\n","epoch 0, step: 101780/127656, loss: 0.011554747819900513\n","epoch 0, step: 101790/127656, loss: 0.014289855025708675\n","epoch 0, step: 101800/127656, loss: 0.007116235792636871\n","epoch 0, step: 101810/127656, loss: 0.00547755416482687\n","epoch 0, step: 101820/127656, loss: 0.008249710313975811\n","epoch 0, step: 101830/127656, loss: 0.007371954154223204\n","epoch 0, step: 101840/127656, loss: 0.007801603525876999\n","epoch 0, step: 101850/127656, loss: 0.00854675006121397\n","epoch 0, step: 101860/127656, loss: 0.13076581060886383\n","epoch 0, step: 101870/127656, loss: 0.4132842719554901\n","epoch 0, step: 101880/127656, loss: 0.008463375270366669\n","epoch 0, step: 101890/127656, loss: 0.0069089918397367\n","epoch 0, step: 101900/127656, loss: 0.005345598794519901\n","epoch 0, step: 101910/127656, loss: 0.006950804498046637\n","epoch 0, step: 101920/127656, loss: 0.009489119984209538\n","epoch 0, step: 101930/127656, loss: 0.5471968054771423\n","epoch 0, step: 101940/127656, loss: 0.41687819361686707\n","epoch 0, step: 101950/127656, loss: 0.2915031611919403\n","epoch 0, step: 101960/127656, loss: 0.008303700014948845\n","epoch 0, step: 101970/127656, loss: 0.008161686360836029\n","epoch 0, step: 101980/127656, loss: 0.007988578639924526\n","epoch 0, step: 101990/127656, loss: 0.01237151212990284\n","epoch 0, step: 102000/127656, loss: 0.007715763058513403\n","epoch 0, step: 102010/127656, loss: 0.1438450813293457\n","epoch 0, step: 102020/127656, loss: 0.0061490703374147415\n","epoch 0, step: 102030/127656, loss: 0.007608321961015463\n","epoch 0, step: 102040/127656, loss: 0.00717010535299778\n","epoch 0, step: 102050/127656, loss: 0.007231666706502438\n","epoch 0, step: 102060/127656, loss: 0.009871250949800014\n","epoch 0, step: 102070/127656, loss: 0.009319725446403027\n","epoch 0, step: 102080/127656, loss: 0.009467119351029396\n","epoch 0, step: 102090/127656, loss: 0.008555995300412178\n","epoch 0, step: 102100/127656, loss: 0.008311880752444267\n","epoch 0, step: 102110/127656, loss: 0.14193932712078094\n","epoch 0, step: 102120/127656, loss: 0.007174579426646233\n","epoch 0, step: 102130/127656, loss: 0.009732530452311039\n","epoch 0, step: 102140/127656, loss: 0.40206295251846313\n","epoch 0, step: 102150/127656, loss: 0.01192144863307476\n","epoch 0, step: 102160/127656, loss: 0.00646620150655508\n","epoch 0, step: 102170/127656, loss: 0.008453740738332272\n","epoch 0, step: 102180/127656, loss: 0.00674488116055727\n","epoch 0, step: 102190/127656, loss: 0.37089240550994873\n","epoch 0, step: 102200/127656, loss: 0.009430645033717155\n","epoch 0, step: 102210/127656, loss: 0.011620472185313702\n","epoch 0, step: 102220/127656, loss: 0.23279738426208496\n","epoch 0, step: 102230/127656, loss: 0.01251595001667738\n","epoch 0, step: 102240/127656, loss: 0.7148115038871765\n","epoch 0, step: 102250/127656, loss: 0.010737570002675056\n","epoch 0, step: 102260/127656, loss: 0.006637544836848974\n","epoch 0, step: 102270/127656, loss: 0.5392100214958191\n","epoch 0, step: 102280/127656, loss: 0.00734788877889514\n","epoch 0, step: 102290/127656, loss: 0.009408248588442802\n","epoch 0, step: 102300/127656, loss: 0.006709420587867498\n","epoch 0, step: 102310/127656, loss: 0.008849707432091236\n","epoch 0, step: 102320/127656, loss: 0.010055682621896267\n","epoch 0, step: 102330/127656, loss: 0.01009467151015997\n","epoch 0, step: 102340/127656, loss: 0.006307839881628752\n","epoch 0, step: 102350/127656, loss: 0.011310242116451263\n","epoch 0, step: 102360/127656, loss: 0.009798101149499416\n","epoch 0, step: 102370/127656, loss: 0.14884047210216522\n","epoch 0, step: 102380/127656, loss: 0.007504883222281933\n","epoch 0, step: 102390/127656, loss: 0.006788204424083233\n","epoch 0, step: 102400/127656, loss: 0.0083521269261837\n","epoch 0, step: 102410/127656, loss: 0.00632048724219203\n","epoch 0, step: 102420/127656, loss: 0.006892765872180462\n","epoch 0, step: 102430/127656, loss: 0.007338082417845726\n","epoch 0, step: 102440/127656, loss: 0.014183047227561474\n","epoch 0, step: 102450/127656, loss: 0.008335290476679802\n","epoch 0, step: 102460/127656, loss: 0.006372980773448944\n","epoch 0, step: 102470/127656, loss: 0.2815856337547302\n","epoch 0, step: 102480/127656, loss: 0.0069208405911922455\n","epoch 0, step: 102490/127656, loss: 0.007327098399400711\n","epoch 0, step: 102500/127656, loss: 0.007205672096461058\n","epoch 0, step: 102510/127656, loss: 0.008350426331162453\n","epoch 0, step: 102520/127656, loss: 0.008997723460197449\n","epoch 0, step: 102530/127656, loss: 0.00979667529463768\n","epoch 0, step: 102540/127656, loss: 0.00903274118900299\n","epoch 0, step: 102550/127656, loss: 0.010140558704733849\n","epoch 0, step: 102560/127656, loss: 0.013555167242884636\n","epoch 0, step: 102570/127656, loss: 0.007626441773027182\n","epoch 0, step: 102580/127656, loss: 0.008401229977607727\n","epoch 0, step: 102590/127656, loss: 0.011941539123654366\n","epoch 0, step: 102600/127656, loss: 0.012914979830384254\n","epoch 0, step: 102610/127656, loss: 0.008926913142204285\n","epoch 0, step: 102620/127656, loss: 0.00633036345243454\n","epoch 0, step: 102630/127656, loss: 0.010661065578460693\n","epoch 0, step: 102640/127656, loss: 0.009377294220030308\n","epoch 0, step: 102650/127656, loss: 0.008786472491919994\n","epoch 0, step: 102660/127656, loss: 0.016109123826026917\n","epoch 0, step: 102670/127656, loss: 0.01291070505976677\n","epoch 0, step: 102680/127656, loss: 0.009320966899394989\n","epoch 0, step: 102690/127656, loss: 0.008403928019106388\n","epoch 0, step: 102700/127656, loss: 0.008741321042180061\n","epoch 0, step: 102710/127656, loss: 0.008466985076665878\n","epoch 0, step: 102720/127656, loss: 0.012183677405118942\n","epoch 0, step: 102730/127656, loss: 0.009096062742173672\n","epoch 0, step: 102740/127656, loss: 0.27637311816215515\n","epoch 0, step: 102750/127656, loss: 0.007266975939273834\n","epoch 0, step: 102760/127656, loss: 0.1311783492565155\n","epoch 0, step: 102770/127656, loss: 0.007578772958368063\n","epoch 0, step: 102780/127656, loss: 0.008030812256038189\n","epoch 0, step: 102790/127656, loss: 0.00740297045558691\n","epoch 0, step: 102800/127656, loss: 0.007674807216972113\n","epoch 0, step: 102810/127656, loss: 0.010586987249553204\n","epoch 0, step: 102820/127656, loss: 0.015149436891078949\n","epoch 0, step: 102830/127656, loss: 0.007060042582452297\n","epoch 0, step: 102840/127656, loss: 0.007584746927022934\n","epoch 0, step: 102850/127656, loss: 0.010290898382663727\n","epoch 0, step: 102860/127656, loss: 0.2811373174190521\n","epoch 0, step: 102870/127656, loss: 0.012120332568883896\n","epoch 0, step: 102880/127656, loss: 0.010329559445381165\n","epoch 0, step: 102890/127656, loss: 0.00839516893029213\n","epoch 0, step: 102900/127656, loss: 0.009982172399759293\n","epoch 0, step: 102910/127656, loss: 0.014813887886703014\n","epoch 0, step: 102920/127656, loss: 0.007087183650583029\n","epoch 0, step: 102930/127656, loss: 0.007922444492578506\n","epoch 0, step: 102940/127656, loss: 0.5478740930557251\n","epoch 0, step: 102950/127656, loss: 0.007450708653777838\n","epoch 0, step: 102960/127656, loss: 0.013649633154273033\n","epoch 0, step: 102970/127656, loss: 0.011583397164940834\n","epoch 0, step: 102980/127656, loss: 0.010066162794828415\n","epoch 0, step: 102990/127656, loss: 0.008299081586301327\n","epoch 0, step: 103000/127656, loss: 0.007917536422610283\n","epoch 0, step: 103010/127656, loss: 0.006656366400420666\n","epoch 0, step: 103020/127656, loss: 0.008652984164655209\n","epoch 0, step: 103030/127656, loss: 0.008634725585579872\n","epoch 0, step: 103040/127656, loss: 0.012660233303904533\n","epoch 0, step: 103050/127656, loss: 0.4034397006034851\n","epoch 0, step: 103060/127656, loss: 0.28179866075515747\n","epoch 0, step: 103070/127656, loss: 0.007274231873452663\n","epoch 0, step: 103080/127656, loss: 0.012861124239861965\n","epoch 0, step: 103090/127656, loss: 0.00906368624418974\n","epoch 0, step: 103100/127656, loss: 0.011736447922885418\n","epoch 0, step: 103110/127656, loss: 0.010380210354924202\n","epoch 0, step: 103120/127656, loss: 0.008923194371163845\n","epoch 0, step: 103130/127656, loss: 0.00997350737452507\n","epoch 0, step: 103140/127656, loss: 0.006422055885195732\n","epoch 0, step: 103150/127656, loss: 0.006890011951327324\n","epoch 0, step: 103160/127656, loss: 0.008823166601359844\n","epoch 0, step: 103170/127656, loss: 0.0057344939559698105\n","epoch 0, step: 103180/127656, loss: 0.0067416648380458355\n","epoch 0, step: 103190/127656, loss: 0.006298370659351349\n","epoch 0, step: 103200/127656, loss: 0.12279938906431198\n","epoch 0, step: 103210/127656, loss: 0.009415214881300926\n","epoch 0, step: 103220/127656, loss: 0.014733552932739258\n","epoch 0, step: 103230/127656, loss: 0.010668507777154446\n","epoch 0, step: 103240/127656, loss: 0.01188359409570694\n","epoch 0, step: 103250/127656, loss: 0.00706305168569088\n","epoch 0, step: 103260/127656, loss: 0.006845301948487759\n","epoch 0, step: 103270/127656, loss: 0.005603024736046791\n","epoch 0, step: 103280/127656, loss: 0.27693355083465576\n","epoch 0, step: 103290/127656, loss: 0.007984638214111328\n","epoch 0, step: 103300/127656, loss: 0.015047113411128521\n","epoch 0, step: 103310/127656, loss: 0.011026907712221146\n","epoch 0, step: 103320/127656, loss: 0.008454076945781708\n","epoch 0, step: 103330/127656, loss: 0.005200919695198536\n","epoch 0, step: 103340/127656, loss: 0.005542231258004904\n","epoch 0, step: 103350/127656, loss: 0.008169540204107761\n","epoch 0, step: 103360/127656, loss: 0.13774725794792175\n","epoch 0, step: 103370/127656, loss: 0.012133024632930756\n","epoch 0, step: 103380/127656, loss: 0.009396625682711601\n","epoch 0, step: 103390/127656, loss: 0.010271246545016766\n","epoch 0, step: 103400/127656, loss: 0.008758008480072021\n","epoch 0, step: 103410/127656, loss: 0.0073915207758545876\n","epoch 0, step: 103420/127656, loss: 0.5462491512298584\n","epoch 0, step: 103430/127656, loss: 0.009321055375039577\n","epoch 0, step: 103440/127656, loss: 0.009742175228893757\n","epoch 0, step: 103450/127656, loss: 0.00841483660042286\n","epoch 0, step: 103460/127656, loss: 0.00823397096246481\n","epoch 0, step: 103470/127656, loss: 0.00895465537905693\n","epoch 0, step: 103480/127656, loss: 0.009946143254637718\n","epoch 0, step: 103490/127656, loss: 0.010090570896863937\n","epoch 0, step: 103500/127656, loss: 0.0066331676207482815\n","epoch 0, step: 103510/127656, loss: 0.5416123270988464\n","epoch 0, step: 103520/127656, loss: 0.00948852114379406\n","epoch 0, step: 103530/127656, loss: 0.011612764559686184\n","epoch 0, step: 103540/127656, loss: 0.010974309407174587\n","epoch 0, step: 103550/127656, loss: 0.0076454500667750835\n","epoch 0, step: 103560/127656, loss: 0.4149717092514038\n","epoch 0, step: 103570/127656, loss: 0.010804551653563976\n","epoch 0, step: 103580/127656, loss: 0.006385135464370251\n","epoch 0, step: 103590/127656, loss: 0.006540441419929266\n","epoch 0, step: 103600/127656, loss: 0.006914913654327393\n","epoch 0, step: 103610/127656, loss: 0.009813631884753704\n","epoch 0, step: 103620/127656, loss: 0.008321553468704224\n","epoch 0, step: 103630/127656, loss: 0.008285857737064362\n","epoch 0, step: 103640/127656, loss: 0.13844484090805054\n","epoch 0, step: 103650/127656, loss: 0.005567312240600586\n","epoch 0, step: 103660/127656, loss: 0.006054950412362814\n","epoch 0, step: 103670/127656, loss: 0.008912384510040283\n","epoch 0, step: 103680/127656, loss: 0.009190605953335762\n","epoch 0, step: 103690/127656, loss: 0.008978712372481823\n","epoch 0, step: 103700/127656, loss: 0.006797389127314091\n","epoch 0, step: 103710/127656, loss: 0.009450191631913185\n","epoch 0, step: 103720/127656, loss: 0.00683395704254508\n","epoch 0, step: 103730/127656, loss: 0.00864475592970848\n","epoch 0, step: 103740/127656, loss: 0.005876750685274601\n","epoch 0, step: 103750/127656, loss: 0.006413035094738007\n","epoch 0, step: 103760/127656, loss: 0.010126803070306778\n","epoch 0, step: 103770/127656, loss: 0.0065980106592178345\n","epoch 0, step: 103780/127656, loss: 0.009610163979232311\n","epoch 0, step: 103790/127656, loss: 0.7127615809440613\n","epoch 0, step: 103800/127656, loss: 0.27528202533721924\n","epoch 0, step: 103810/127656, loss: 0.007956719025969505\n","epoch 0, step: 103820/127656, loss: 0.008706402033567429\n","epoch 0, step: 103830/127656, loss: 0.00844244472682476\n","epoch 0, step: 103840/127656, loss: 0.006987429689615965\n","epoch 0, step: 103850/127656, loss: 0.004823700524866581\n","epoch 0, step: 103860/127656, loss: 0.00840239692479372\n","epoch 0, step: 103870/127656, loss: 0.008149420842528343\n","epoch 0, step: 103880/127656, loss: 0.005829836241900921\n","epoch 0, step: 103890/127656, loss: 0.011243059299886227\n","epoch 0, step: 103900/127656, loss: 0.006971509661525488\n","epoch 0, step: 103910/127656, loss: 0.017929676920175552\n","epoch 0, step: 103920/127656, loss: 0.009432036429643631\n","epoch 0, step: 103930/127656, loss: 0.010709023103117943\n","epoch 0, step: 103940/127656, loss: 0.014893664978444576\n","epoch 0, step: 103950/127656, loss: 0.010527539066970348\n","epoch 0, step: 103960/127656, loss: 0.006452057044953108\n","epoch 0, step: 103970/127656, loss: 0.007153861224651337\n","epoch 0, step: 103980/127656, loss: 0.006808508187532425\n","epoch 0, step: 103990/127656, loss: 0.0066300611943006516\n","epoch 0, step: 104000/127656, loss: 0.011283314786851406\n","epoch 0, step: 104010/127656, loss: 0.008901203982532024\n","epoch 0, step: 104020/127656, loss: 0.006955855991691351\n","epoch 0, step: 104030/127656, loss: 0.0068895649164915085\n","epoch 0, step: 104040/127656, loss: 0.008896231651306152\n","epoch 0, step: 104050/127656, loss: 0.28034234046936035\n","epoch 0, step: 104060/127656, loss: 0.008335070684552193\n","epoch 0, step: 104070/127656, loss: 0.006722623482346535\n","epoch 0, step: 104080/127656, loss: 0.012961901724338531\n","epoch 0, step: 104090/127656, loss: 0.004150270950049162\n","epoch 0, step: 104100/127656, loss: 0.008574269711971283\n","epoch 0, step: 104110/127656, loss: 0.010426930151879787\n","epoch 0, step: 104120/127656, loss: 0.11431502550840378\n","epoch 0, step: 104130/127656, loss: 0.009503411129117012\n","epoch 0, step: 104140/127656, loss: 0.006329350173473358\n","epoch 0, step: 104150/127656, loss: 0.007200689520686865\n","epoch 0, step: 104160/127656, loss: 0.015509938821196556\n","epoch 0, step: 104170/127656, loss: 0.008914629928767681\n","epoch 0, step: 104180/127656, loss: 0.0073830075562000275\n","epoch 0, step: 104190/127656, loss: 0.009444091469049454\n","epoch 0, step: 104200/127656, loss: 0.010707994922995567\n","epoch 0, step: 104210/127656, loss: 0.14045818150043488\n","epoch 0, step: 104220/127656, loss: 0.007486134767532349\n","epoch 0, step: 104230/127656, loss: 0.00888622272759676\n","epoch 0, step: 104240/127656, loss: 0.008725501596927643\n","epoch 0, step: 104250/127656, loss: 0.009545567445456982\n","epoch 0, step: 104260/127656, loss: 0.007351583801209927\n","epoch 0, step: 104270/127656, loss: 0.007004675921052694\n","epoch 0, step: 104280/127656, loss: 0.14173710346221924\n","epoch 0, step: 104290/127656, loss: 0.007150971796363592\n","epoch 0, step: 104300/127656, loss: 0.009714677929878235\n","epoch 0, step: 104310/127656, loss: 0.005337137263268232\n","epoch 0, step: 104320/127656, loss: 0.009100120514631271\n","epoch 0, step: 104330/127656, loss: 0.008723610080778599\n","epoch 0, step: 104340/127656, loss: 0.009215665981173515\n","epoch 0, step: 104350/127656, loss: 0.009631795808672905\n","epoch 0, step: 104360/127656, loss: 0.006547120399773121\n","epoch 0, step: 104370/127656, loss: 0.00673869252204895\n","epoch 0, step: 104380/127656, loss: 0.005543986335396767\n","epoch 0, step: 104390/127656, loss: 0.0081389881670475\n","epoch 0, step: 104400/127656, loss: 0.41644486784935\n","epoch 0, step: 104410/127656, loss: 0.006110839545726776\n","epoch 0, step: 104420/127656, loss: 0.012364941649138927\n","epoch 0, step: 104430/127656, loss: 0.01233171857893467\n","epoch 0, step: 104440/127656, loss: 0.0064483932219445705\n","epoch 0, step: 104450/127656, loss: 0.13885711133480072\n","epoch 0, step: 104460/127656, loss: 0.010415379889309406\n","epoch 0, step: 104470/127656, loss: 0.010789426043629646\n","epoch 0, step: 104480/127656, loss: 0.006117853801697493\n","epoch 0, step: 104490/127656, loss: 0.006361445412039757\n","epoch 0, step: 104500/127656, loss: 0.009688407182693481\n","epoch 0, step: 104510/127656, loss: 0.006433201488107443\n","epoch 0, step: 104520/127656, loss: 0.01279289461672306\n","epoch 0, step: 104530/127656, loss: 0.009743638336658478\n","epoch 0, step: 104540/127656, loss: 0.012936515733599663\n","epoch 0, step: 104550/127656, loss: 0.007295613177120686\n","epoch 0, step: 104560/127656, loss: 0.00907735712826252\n","epoch 0, step: 104570/127656, loss: 0.009645242244005203\n","epoch 0, step: 104580/127656, loss: 0.008379779756069183\n","epoch 0, step: 104590/127656, loss: 0.00940128043293953\n","epoch 0, step: 104600/127656, loss: 0.14947724342346191\n","epoch 0, step: 104610/127656, loss: 0.012192799709737301\n","epoch 0, step: 104620/127656, loss: 0.005684698931872845\n","epoch 0, step: 104630/127656, loss: 0.560978889465332\n","epoch 0, step: 104640/127656, loss: 0.005386480130255222\n","epoch 0, step: 104650/127656, loss: 0.0064525599591434\n","epoch 0, step: 104660/127656, loss: 0.008631227537989616\n","epoch 0, step: 104670/127656, loss: 0.013058789074420929\n","epoch 0, step: 104680/127656, loss: 0.008037585765123367\n","epoch 0, step: 104690/127656, loss: 0.011218063533306122\n","epoch 0, step: 104700/127656, loss: 0.39585959911346436\n","epoch 0, step: 104710/127656, loss: 0.009915532544255257\n","epoch 0, step: 104720/127656, loss: 0.011333370581269264\n","epoch 0, step: 104730/127656, loss: 0.008465724997222424\n","epoch 0, step: 104740/127656, loss: 0.006864367052912712\n","epoch 0, step: 104750/127656, loss: 0.01060826238244772\n","epoch 0, step: 104760/127656, loss: 0.007400189992040396\n","epoch 0, step: 104770/127656, loss: 0.008142709732055664\n","epoch 0, step: 104780/127656, loss: 0.009186796844005585\n","epoch 0, step: 104790/127656, loss: 0.008434103801846504\n","epoch 0, step: 104800/127656, loss: 0.009293309412896633\n","epoch 0, step: 104810/127656, loss: 0.007512438576668501\n","epoch 0, step: 104820/127656, loss: 0.009162496775388718\n","epoch 0, step: 104830/127656, loss: 0.009038703516125679\n","epoch 0, step: 104840/127656, loss: 0.008357768878340721\n","epoch 0, step: 104850/127656, loss: 0.011677050963044167\n","epoch 0, step: 104860/127656, loss: 0.00709488894790411\n","epoch 0, step: 104870/127656, loss: 0.41938143968582153\n","epoch 0, step: 104880/127656, loss: 0.007783216889947653\n","epoch 0, step: 104890/127656, loss: 0.1471838802099228\n","epoch 0, step: 104900/127656, loss: 0.00803998950868845\n","epoch 0, step: 104910/127656, loss: 0.01688849739730358\n","epoch 0, step: 104920/127656, loss: 0.008066593669354916\n","epoch 0, step: 104930/127656, loss: 0.011035725474357605\n","epoch 0, step: 104940/127656, loss: 0.013886402361094952\n","epoch 0, step: 104950/127656, loss: 0.007335287518799305\n","epoch 0, step: 104960/127656, loss: 0.1426296830177307\n","epoch 0, step: 104970/127656, loss: 0.004847708158195019\n","epoch 0, step: 104980/127656, loss: 0.009200770407915115\n","epoch 0, step: 104990/127656, loss: 0.008818516507744789\n","epoch 0, step: 105000/127656, loss: 0.006687474902719259\n","epoch 0, step: 105010/127656, loss: 0.009434388019144535\n","epoch 0, step: 105020/127656, loss: 0.008225355297327042\n","epoch 0, step: 105030/127656, loss: 0.007594512775540352\n","epoch 0, step: 105040/127656, loss: 0.005193193443119526\n","epoch 0, step: 105050/127656, loss: 0.015371732413768768\n","epoch 0, step: 105060/127656, loss: 0.006806445308029652\n","epoch 0, step: 105070/127656, loss: 0.005806718021631241\n","epoch 0, step: 105080/127656, loss: 0.007037244271486998\n","epoch 0, step: 105090/127656, loss: 0.006988428067415953\n","epoch 0, step: 105100/127656, loss: 0.007671716623008251\n","epoch 0, step: 105110/127656, loss: 0.010918864980340004\n","epoch 0, step: 105120/127656, loss: 0.00740955863147974\n","epoch 0, step: 105130/127656, loss: 0.008933542296290398\n","epoch 0, step: 105140/127656, loss: 0.010970774106681347\n","epoch 0, step: 105150/127656, loss: 0.008648239076137543\n","epoch 0, step: 105160/127656, loss: 0.007340861484408379\n","epoch 0, step: 105170/127656, loss: 0.006155941169708967\n","epoch 0, step: 105180/127656, loss: 0.0079214908182621\n","epoch 0, step: 105190/127656, loss: 0.536320686340332\n","epoch 0, step: 105200/127656, loss: 0.00812464114278555\n","epoch 0, step: 105210/127656, loss: 0.009764897637069225\n","epoch 0, step: 105220/127656, loss: 0.5433914065361023\n","epoch 0, step: 105230/127656, loss: 0.009912895038723946\n","epoch 0, step: 105240/127656, loss: 0.008339553140103817\n","epoch 0, step: 105250/127656, loss: 0.010708974674344063\n","epoch 0, step: 105260/127656, loss: 0.012251308187842369\n","epoch 0, step: 105270/127656, loss: 0.011260554194450378\n","epoch 0, step: 105280/127656, loss: 0.554694414138794\n","epoch 0, step: 105290/127656, loss: 0.00757815083488822\n","epoch 0, step: 105300/127656, loss: 0.00642506405711174\n","epoch 0, step: 105310/127656, loss: 0.01047573983669281\n","epoch 0, step: 105320/127656, loss: 0.005214547272771597\n","epoch 0, step: 105330/127656, loss: 0.013780302368104458\n","epoch 0, step: 105340/127656, loss: 0.007732947822660208\n","epoch 0, step: 105350/127656, loss: 0.009812675416469574\n","epoch 0, step: 105360/127656, loss: 0.01391569059342146\n","epoch 0, step: 105370/127656, loss: 0.011891933158040047\n","epoch 0, step: 105380/127656, loss: 0.008153422735631466\n","epoch 0, step: 105390/127656, loss: 0.011156714521348476\n","epoch 0, step: 105400/127656, loss: 0.013984757475554943\n","epoch 0, step: 105410/127656, loss: 0.014711705967783928\n","epoch 0, step: 105420/127656, loss: 0.014800844714045525\n","epoch 0, step: 105430/127656, loss: 0.008203510195016861\n","epoch 0, step: 105440/127656, loss: 0.00591462105512619\n","epoch 0, step: 105450/127656, loss: 0.012902002781629562\n","epoch 0, step: 105460/127656, loss: 0.005117666907608509\n","epoch 0, step: 105470/127656, loss: 0.3924787938594818\n","epoch 0, step: 105480/127656, loss: 0.006346925627440214\n","epoch 0, step: 105490/127656, loss: 0.009834504686295986\n","epoch 0, step: 105500/127656, loss: 0.009329244494438171\n","epoch 0, step: 105510/127656, loss: 0.007727633696049452\n","epoch 0, step: 105520/127656, loss: 0.011294672265648842\n","epoch 0, step: 105530/127656, loss: 0.007391798309981823\n","epoch 0, step: 105540/127656, loss: 0.011140568181872368\n","epoch 0, step: 105550/127656, loss: 0.011033445596694946\n","epoch 0, step: 105560/127656, loss: 0.006737920455634594\n","epoch 0, step: 105570/127656, loss: 0.009957417845726013\n","epoch 0, step: 105580/127656, loss: 0.008909481577575207\n","epoch 0, step: 105590/127656, loss: 0.00750287901610136\n","epoch 0, step: 105600/127656, loss: 0.01157553680241108\n","epoch 0, step: 105610/127656, loss: 0.00605386309325695\n","epoch 0, step: 105620/127656, loss: 0.01647416315972805\n","epoch 0, step: 105630/127656, loss: 0.008044149726629257\n","epoch 0, step: 105640/127656, loss: 0.010309687815606594\n","epoch 0, step: 105650/127656, loss: 0.01197693683207035\n","epoch 0, step: 105660/127656, loss: 0.017167028039693832\n","epoch 0, step: 105670/127656, loss: 0.013085311278700829\n","epoch 0, step: 105680/127656, loss: 0.007056200411170721\n","epoch 0, step: 105690/127656, loss: 0.007639977149665356\n","epoch 0, step: 105700/127656, loss: 0.009407691657543182\n","epoch 0, step: 105710/127656, loss: 0.01676824688911438\n","epoch 0, step: 105720/127656, loss: 0.006229767110198736\n","epoch 0, step: 105730/127656, loss: 0.1413000226020813\n","epoch 0, step: 105740/127656, loss: 0.0077516864985227585\n","epoch 0, step: 105750/127656, loss: 0.007960986346006393\n","epoch 0, step: 105760/127656, loss: 0.13039405643939972\n","epoch 0, step: 105770/127656, loss: 0.011229793541133404\n","epoch 0, step: 105780/127656, loss: 0.013616137206554413\n","epoch 0, step: 105790/127656, loss: 0.011256794445216656\n","epoch 0, step: 105800/127656, loss: 0.007667665369808674\n","epoch 0, step: 105810/127656, loss: 0.008214184083044529\n","epoch 0, step: 105820/127656, loss: 0.007924607023596764\n","epoch 0, step: 105830/127656, loss: 0.008942895568907261\n","epoch 0, step: 105840/127656, loss: 0.0092688649892807\n","epoch 0, step: 105850/127656, loss: 0.011483261361718178\n","epoch 0, step: 105860/127656, loss: 0.1411440521478653\n","epoch 0, step: 105870/127656, loss: 0.011488288640975952\n","epoch 0, step: 105880/127656, loss: 0.008157618343830109\n","epoch 0, step: 105890/127656, loss: 0.008641091175377369\n","epoch 0, step: 105900/127656, loss: 0.008738058619201183\n","epoch 0, step: 105910/127656, loss: 0.008058912120759487\n","epoch 0, step: 105920/127656, loss: 0.009679054841399193\n","epoch 0, step: 105930/127656, loss: 0.008855100721120834\n","epoch 0, step: 105940/127656, loss: 0.006387128960341215\n","epoch 0, step: 105950/127656, loss: 0.011726207099854946\n","epoch 0, step: 105960/127656, loss: 0.006760809570550919\n","epoch 0, step: 105970/127656, loss: 0.008279869332909584\n","epoch 0, step: 105980/127656, loss: 0.00598508957773447\n","epoch 0, step: 105990/127656, loss: 0.007185990922152996\n","epoch 0, step: 106000/127656, loss: 0.006268120836466551\n","epoch 0, step: 106010/127656, loss: 0.00894839596003294\n","epoch 0, step: 106020/127656, loss: 0.009555809199810028\n","epoch 0, step: 106030/127656, loss: 0.015668068081140518\n","epoch 0, step: 106040/127656, loss: 0.01040571741759777\n","epoch 0, step: 106050/127656, loss: 0.007188640534877777\n","epoch 0, step: 106060/127656, loss: 0.00729519035667181\n","epoch 0, step: 106070/127656, loss: 0.019714387133717537\n","epoch 0, step: 106080/127656, loss: 0.006305876187980175\n","epoch 0, step: 106090/127656, loss: 0.009140977635979652\n","epoch 0, step: 106100/127656, loss: 0.007445893716067076\n","epoch 0, step: 106110/127656, loss: 0.009950609877705574\n","epoch 0, step: 106120/127656, loss: 0.011092930100858212\n","epoch 0, step: 106130/127656, loss: 0.00808046292513609\n","epoch 0, step: 106140/127656, loss: 0.00467307586222887\n","epoch 0, step: 106150/127656, loss: 0.007525247987359762\n","epoch 0, step: 106160/127656, loss: 0.008304232731461525\n","epoch 0, step: 106170/127656, loss: 0.009074444882571697\n","epoch 0, step: 106180/127656, loss: 0.006000843830406666\n","epoch 0, step: 106190/127656, loss: 0.00715121254324913\n","epoch 0, step: 106200/127656, loss: 0.009445182979106903\n","epoch 0, step: 106210/127656, loss: 0.005667401477694511\n","epoch 0, step: 106220/127656, loss: 0.1356370598077774\n","epoch 0, step: 106230/127656, loss: 0.007312409114092588\n","epoch 0, step: 106240/127656, loss: 0.006718716584146023\n","epoch 0, step: 106250/127656, loss: 0.01330347266048193\n","epoch 0, step: 106260/127656, loss: 0.010483622550964355\n","epoch 0, step: 106270/127656, loss: 0.004807776305824518\n","epoch 0, step: 106280/127656, loss: 0.009707637131214142\n","epoch 0, step: 106290/127656, loss: 0.00869026966392994\n","epoch 0, step: 106300/127656, loss: 0.013099687173962593\n","epoch 0, step: 106310/127656, loss: 0.006598459556698799\n","epoch 0, step: 106320/127656, loss: 0.006637300364673138\n","epoch 0, step: 106330/127656, loss: 0.0058173611760139465\n","epoch 0, step: 106340/127656, loss: 0.006879735272377729\n","epoch 0, step: 106350/127656, loss: 0.006973049603402615\n","epoch 0, step: 106360/127656, loss: 0.007468727882951498\n","epoch 0, step: 106370/127656, loss: 0.011737791821360588\n","epoch 0, step: 106380/127656, loss: 0.009848632849752903\n","epoch 0, step: 106390/127656, loss: 0.007303030230104923\n","epoch 0, step: 106400/127656, loss: 0.006670243106782436\n","epoch 0, step: 106410/127656, loss: 0.005119537003338337\n","epoch 0, step: 106420/127656, loss: 0.014074137434363365\n","epoch 0, step: 106430/127656, loss: 0.13442832231521606\n","epoch 0, step: 106440/127656, loss: 0.015839800238609314\n","epoch 0, step: 106450/127656, loss: 0.006968379020690918\n","epoch 0, step: 106460/127656, loss: 0.012382905930280685\n","epoch 0, step: 106470/127656, loss: 0.005761332809925079\n","epoch 0, step: 106480/127656, loss: 0.006047224160283804\n","epoch 0, step: 106490/127656, loss: 0.004429425112903118\n","epoch 0, step: 106500/127656, loss: 0.010213153436779976\n","epoch 0, step: 106510/127656, loss: 0.009512541815638542\n","epoch 0, step: 106520/127656, loss: 0.007946494966745377\n","epoch 0, step: 106530/127656, loss: 0.008118107914924622\n","epoch 0, step: 106540/127656, loss: 0.008126073516905308\n","epoch 0, step: 106550/127656, loss: 0.011463019996881485\n","epoch 0, step: 106560/127656, loss: 0.009366386570036411\n","epoch 0, step: 106570/127656, loss: 0.005131865851581097\n","epoch 0, step: 106580/127656, loss: 0.00664482032880187\n","epoch 0, step: 106590/127656, loss: 0.009477379731833935\n","epoch 0, step: 106600/127656, loss: 0.007775434292852879\n","epoch 0, step: 106610/127656, loss: 0.00539809837937355\n","epoch 0, step: 106620/127656, loss: 0.00927434116601944\n","epoch 0, step: 106630/127656, loss: 0.007326880004256964\n","epoch 0, step: 106640/127656, loss: 0.010232258588075638\n","epoch 0, step: 106650/127656, loss: 0.01020512543618679\n","epoch 0, step: 106660/127656, loss: 0.14134368300437927\n","epoch 0, step: 106670/127656, loss: 0.008404385298490524\n","epoch 0, step: 106680/127656, loss: 0.006495211273431778\n","epoch 0, step: 106690/127656, loss: 0.006771968211978674\n","epoch 0, step: 106700/127656, loss: 0.015163264237344265\n","epoch 0, step: 106710/127656, loss: 0.5619624853134155\n","epoch 0, step: 106720/127656, loss: 0.006536740344017744\n","epoch 0, step: 106730/127656, loss: 0.006229600869119167\n","epoch 0, step: 106740/127656, loss: 0.011038648895919323\n","epoch 0, step: 106750/127656, loss: 0.01181526854634285\n","epoch 0, step: 106760/127656, loss: 0.011100078001618385\n","epoch 0, step: 106770/127656, loss: 0.012788484804332256\n","epoch 0, step: 106780/127656, loss: 0.007683266885578632\n","epoch 0, step: 106790/127656, loss: 0.010540246963500977\n","epoch 0, step: 106800/127656, loss: 0.007287844084203243\n","epoch 0, step: 106810/127656, loss: 0.01070336066186428\n","epoch 0, step: 106820/127656, loss: 0.00646001473069191\n","epoch 0, step: 106830/127656, loss: 0.00852209236472845\n","epoch 0, step: 106840/127656, loss: 0.006215140223503113\n","epoch 0, step: 106850/127656, loss: 0.008459657430648804\n","epoch 0, step: 106860/127656, loss: 0.012186279520392418\n","epoch 0, step: 106870/127656, loss: 0.14703869819641113\n","epoch 0, step: 106880/127656, loss: 0.5353588461875916\n","epoch 0, step: 106890/127656, loss: 0.00705909077078104\n","epoch 0, step: 106900/127656, loss: 0.008604342117905617\n","epoch 0, step: 106910/127656, loss: 0.006484159268438816\n","epoch 0, step: 106920/127656, loss: 0.24950914084911346\n","epoch 0, step: 106930/127656, loss: 0.008276574313640594\n","epoch 0, step: 106940/127656, loss: 0.008363550528883934\n","epoch 0, step: 106950/127656, loss: 0.009666981175541878\n","epoch 0, step: 106960/127656, loss: 0.2782193422317505\n","epoch 0, step: 106970/127656, loss: 0.005895313806831837\n","epoch 0, step: 106980/127656, loss: 0.003996599465608597\n","epoch 0, step: 106990/127656, loss: 0.011140238493680954\n","epoch 0, step: 107000/127656, loss: 0.009393356740474701\n","epoch 0, step: 107010/127656, loss: 0.008424702100455761\n","epoch 0, step: 107020/127656, loss: 0.01043411623686552\n","epoch 0, step: 107030/127656, loss: 0.011251936666667461\n","epoch 0, step: 107040/127656, loss: 0.00684878695756197\n","epoch 0, step: 107050/127656, loss: 0.006792456842958927\n","epoch 0, step: 107060/127656, loss: 0.00624912790954113\n","epoch 0, step: 107070/127656, loss: 0.012644722126424313\n","epoch 0, step: 107080/127656, loss: 0.013428030535578728\n","epoch 0, step: 107090/127656, loss: 0.5529409646987915\n","epoch 0, step: 107100/127656, loss: 0.13913995027542114\n","epoch 0, step: 107110/127656, loss: 0.008455948904156685\n","epoch 0, step: 107120/127656, loss: 0.007397571112960577\n","epoch 0, step: 107130/127656, loss: 0.007352937012910843\n","epoch 0, step: 107140/127656, loss: 0.005757666192948818\n","epoch 0, step: 107150/127656, loss: 0.01092077698558569\n","epoch 0, step: 107160/127656, loss: 0.27836447954177856\n","epoch 0, step: 107170/127656, loss: 0.005910755600780249\n","epoch 0, step: 107180/127656, loss: 0.008069602772593498\n","epoch 0, step: 107190/127656, loss: 0.52897709608078\n","epoch 0, step: 107200/127656, loss: 0.29411494731903076\n","epoch 0, step: 107210/127656, loss: 0.0059244511649012566\n","epoch 0, step: 107220/127656, loss: 0.008916272781789303\n","epoch 0, step: 107230/127656, loss: 0.00913891103118658\n","epoch 0, step: 107240/127656, loss: 0.0064345854334533215\n","epoch 0, step: 107250/127656, loss: 0.007319701369851828\n","epoch 0, step: 107260/127656, loss: 0.007572320755571127\n","epoch 0, step: 107270/127656, loss: 0.012586338445544243\n","epoch 0, step: 107280/127656, loss: 0.004813222214579582\n","epoch 0, step: 107290/127656, loss: 0.00713661452755332\n","epoch 0, step: 107300/127656, loss: 0.007744554430246353\n","epoch 0, step: 107310/127656, loss: 0.008729927241802216\n","epoch 0, step: 107320/127656, loss: 0.01395490299910307\n","epoch 0, step: 107330/127656, loss: 0.006090774200856686\n","epoch 0, step: 107340/127656, loss: 0.0056463442742824554\n","epoch 0, step: 107350/127656, loss: 0.012736817821860313\n","epoch 0, step: 107360/127656, loss: 0.0069039687514305115\n","epoch 0, step: 107370/127656, loss: 0.00860514398664236\n","epoch 0, step: 107380/127656, loss: 0.007285236846655607\n","epoch 0, step: 107390/127656, loss: 0.004377052187919617\n","epoch 0, step: 107400/127656, loss: 0.005711864680051804\n","epoch 0, step: 107410/127656, loss: 0.016770362854003906\n","epoch 0, step: 107420/127656, loss: 0.6922770738601685\n","epoch 0, step: 107430/127656, loss: 0.006680767517536879\n","epoch 0, step: 107440/127656, loss: 0.007891843095421791\n","epoch 0, step: 107450/127656, loss: 0.00720598828047514\n","epoch 0, step: 107460/127656, loss: 0.010337075218558311\n","epoch 0, step: 107470/127656, loss: 0.012416113168001175\n","epoch 0, step: 107480/127656, loss: 0.4254845976829529\n","epoch 0, step: 107490/127656, loss: 0.005269106477499008\n","epoch 0, step: 107500/127656, loss: 0.009801195934414864\n","epoch 0, step: 107510/127656, loss: 0.01680833473801613\n","epoch 0, step: 107520/127656, loss: 0.006180425174534321\n","epoch 0, step: 107530/127656, loss: 0.013801328837871552\n","epoch 0, step: 107540/127656, loss: 0.00851505808532238\n","epoch 0, step: 107550/127656, loss: 0.007502963300794363\n","epoch 0, step: 107560/127656, loss: 0.0049682846292853355\n","epoch 0, step: 107570/127656, loss: 0.012090951204299927\n","epoch 0, step: 107580/127656, loss: 0.005883423145860434\n","epoch 0, step: 107590/127656, loss: 0.006754211150109768\n","epoch 0, step: 107600/127656, loss: 0.00491432985290885\n","epoch 0, step: 107610/127656, loss: 0.00997200794517994\n","epoch 0, step: 107620/127656, loss: 0.01255411934107542\n","epoch 0, step: 107630/127656, loss: 0.005247167311608791\n","epoch 0, step: 107640/127656, loss: 0.009108196943998337\n","epoch 0, step: 107650/127656, loss: 0.009406938217580318\n","epoch 0, step: 107660/127656, loss: 0.005770324729382992\n","epoch 0, step: 107670/127656, loss: 0.006061555817723274\n","epoch 0, step: 107680/127656, loss: 0.12350412458181381\n","epoch 0, step: 107690/127656, loss: 0.007605158723890781\n","epoch 0, step: 107700/127656, loss: 0.006979568861424923\n","epoch 0, step: 107710/127656, loss: 0.011415074579417706\n","epoch 0, step: 107720/127656, loss: 0.006613294593989849\n","epoch 0, step: 107730/127656, loss: 0.007462409324944019\n","epoch 0, step: 107740/127656, loss: 0.007579663302749395\n","epoch 0, step: 107750/127656, loss: 0.006836801301687956\n","epoch 0, step: 107760/127656, loss: 0.013459116220474243\n","epoch 0, step: 107770/127656, loss: 0.0069411094300448895\n","epoch 0, step: 107780/127656, loss: 0.01074739545583725\n","epoch 0, step: 107790/127656, loss: 0.0063103921711444855\n","epoch 0, step: 107800/127656, loss: 0.2527298331260681\n","epoch 0, step: 107810/127656, loss: 0.009572278708219528\n","epoch 0, step: 107820/127656, loss: 0.2637535333633423\n","epoch 0, step: 107830/127656, loss: 0.005457606166601181\n","epoch 0, step: 107840/127656, loss: 0.004913415294140577\n","epoch 0, step: 107850/127656, loss: 0.013545237481594086\n","epoch 0, step: 107860/127656, loss: 0.2777969241142273\n","epoch 0, step: 107870/127656, loss: 0.013426460325717926\n","epoch 0, step: 107880/127656, loss: 0.006771960761398077\n","epoch 0, step: 107890/127656, loss: 0.14533081650733948\n","epoch 0, step: 107900/127656, loss: 0.010598903521895409\n","epoch 0, step: 107910/127656, loss: 0.014428285881876945\n","epoch 0, step: 107920/127656, loss: 0.27723708748817444\n","epoch 0, step: 107930/127656, loss: 0.006697534583508968\n","epoch 0, step: 107940/127656, loss: 0.004139239899814129\n","epoch 0, step: 107950/127656, loss: 0.00946219265460968\n","epoch 0, step: 107960/127656, loss: 0.011234506964683533\n","epoch 0, step: 107970/127656, loss: 0.011398887261748314\n","epoch 0, step: 107980/127656, loss: 0.011028054170310497\n","epoch 0, step: 107990/127656, loss: 0.009510512463748455\n","epoch 0, step: 108000/127656, loss: 0.0054201423190534115\n","epoch 0, step: 108010/127656, loss: 0.007795504294335842\n","epoch 0, step: 108020/127656, loss: 0.005379789508879185\n","epoch 0, step: 108030/127656, loss: 0.007002921774983406\n","epoch 0, step: 108040/127656, loss: 0.01032315194606781\n","epoch 0, step: 108050/127656, loss: 0.006494856905192137\n","epoch 0, step: 108060/127656, loss: 0.00899360328912735\n","epoch 0, step: 108070/127656, loss: 0.006376908626407385\n","epoch 0, step: 108080/127656, loss: 0.006355868186801672\n","epoch 0, step: 108090/127656, loss: 0.006457343231886625\n","epoch 0, step: 108100/127656, loss: 0.26791995763778687\n","epoch 0, step: 108110/127656, loss: 0.14196327328681946\n","epoch 0, step: 108120/127656, loss: 0.011313087306916714\n","epoch 0, step: 108130/127656, loss: 0.008563875220716\n","epoch 0, step: 108140/127656, loss: 0.00854279100894928\n","epoch 0, step: 108150/127656, loss: 0.015018782578408718\n","epoch 0, step: 108160/127656, loss: 0.008458464406430721\n","epoch 0, step: 108170/127656, loss: 0.00645740982145071\n","epoch 0, step: 108180/127656, loss: 0.2753041386604309\n","epoch 0, step: 108190/127656, loss: 0.008159669116139412\n","epoch 0, step: 108200/127656, loss: 0.006943870335817337\n","epoch 0, step: 108210/127656, loss: 0.12540027499198914\n","epoch 0, step: 108220/127656, loss: 0.13263541460037231\n","epoch 0, step: 108230/127656, loss: 0.0071097626350820065\n","epoch 0, step: 108240/127656, loss: 0.010082810185849667\n","epoch 0, step: 108250/127656, loss: 0.005952045321464539\n","epoch 0, step: 108260/127656, loss: 0.009152032434940338\n","epoch 0, step: 108270/127656, loss: 0.009022925049066544\n","epoch 0, step: 108280/127656, loss: 0.00932922400534153\n","epoch 0, step: 108290/127656, loss: 0.008687702007591724\n","epoch 0, step: 108300/127656, loss: 0.011538928374648094\n","epoch 0, step: 108310/127656, loss: 0.00754608865827322\n","epoch 0, step: 108320/127656, loss: 0.012831347994506359\n","epoch 0, step: 108330/127656, loss: 0.006774467416107655\n","epoch 0, step: 108340/127656, loss: 0.41117405891418457\n","epoch 0, step: 108350/127656, loss: 0.009941661730408669\n","epoch 0, step: 108360/127656, loss: 0.5413657426834106\n","epoch 0, step: 108370/127656, loss: 0.2830391824245453\n","epoch 0, step: 108380/127656, loss: 0.010613447055220604\n","epoch 0, step: 108390/127656, loss: 0.008722788654267788\n","epoch 0, step: 108400/127656, loss: 0.004176394082605839\n","epoch 0, step: 108410/127656, loss: 0.01156873069703579\n","epoch 0, step: 108420/127656, loss: 0.008644890040159225\n","epoch 0, step: 108430/127656, loss: 0.0077915191650390625\n","epoch 0, step: 108440/127656, loss: 0.007661443203687668\n","epoch 0, step: 108450/127656, loss: 0.015509223565459251\n","epoch 0, step: 108460/127656, loss: 0.006998802535235882\n","epoch 0, step: 108470/127656, loss: 0.008706746622920036\n","epoch 0, step: 108480/127656, loss: 0.0064642056822776794\n","epoch 0, step: 108490/127656, loss: 0.009105067700147629\n","epoch 0, step: 108500/127656, loss: 0.009658660739660263\n","epoch 0, step: 108510/127656, loss: 0.008360859006643295\n","epoch 0, step: 108520/127656, loss: 0.011331717483699322\n","epoch 0, step: 108530/127656, loss: 0.010586592368781567\n","epoch 0, step: 108540/127656, loss: 0.011914974078536034\n","epoch 0, step: 108550/127656, loss: 0.011119897477328777\n","epoch 0, step: 108560/127656, loss: 0.006117332726716995\n","epoch 0, step: 108570/127656, loss: 0.005901793949306011\n","epoch 0, step: 108580/127656, loss: 0.006901036947965622\n","epoch 0, step: 108590/127656, loss: 0.004538980312645435\n","epoch 0, step: 108600/127656, loss: 0.006928734481334686\n","epoch 0, step: 108610/127656, loss: 0.010590456426143646\n","epoch 0, step: 108620/127656, loss: 0.1433076709508896\n","epoch 0, step: 108630/127656, loss: 0.013800546526908875\n","epoch 0, step: 108640/127656, loss: 0.0068536968901753426\n","epoch 0, step: 108650/127656, loss: 0.010926144197583199\n","epoch 0, step: 108660/127656, loss: 0.008831070736050606\n","epoch 0, step: 108670/127656, loss: 0.005011134780943394\n","epoch 0, step: 108680/127656, loss: 0.007177535444498062\n","epoch 0, step: 108690/127656, loss: 0.5366151928901672\n","epoch 0, step: 108700/127656, loss: 0.005749938543885946\n","epoch 0, step: 108710/127656, loss: 0.006127486005425453\n","epoch 0, step: 108720/127656, loss: 0.00650666281580925\n","epoch 0, step: 108730/127656, loss: 0.0067773605696856976\n","epoch 0, step: 108740/127656, loss: 0.012100402265787125\n","epoch 0, step: 108750/127656, loss: 0.004837073385715485\n","epoch 0, step: 108760/127656, loss: 0.009035633876919746\n","epoch 0, step: 108770/127656, loss: 0.006641821004450321\n","epoch 0, step: 108780/127656, loss: 0.008273919112980366\n","epoch 0, step: 108790/127656, loss: 0.005713101476430893\n","epoch 0, step: 108800/127656, loss: 0.009398611262440681\n","epoch 0, step: 108810/127656, loss: 0.006053673103451729\n","epoch 0, step: 108820/127656, loss: 0.006865831092000008\n","epoch 0, step: 108830/127656, loss: 0.012475268915295601\n","epoch 0, step: 108840/127656, loss: 0.009748397395014763\n","epoch 0, step: 108850/127656, loss: 0.010101917199790478\n","epoch 0, step: 108860/127656, loss: 0.004708798602223396\n","epoch 0, step: 108870/127656, loss: 0.008473951369524002\n","epoch 0, step: 108880/127656, loss: 0.009625776670873165\n","epoch 0, step: 108890/127656, loss: 0.011464344337582588\n","epoch 0, step: 108900/127656, loss: 0.5362352728843689\n","epoch 0, step: 108910/127656, loss: 0.010149477981030941\n","epoch 0, step: 108920/127656, loss: 0.005332100670784712\n","epoch 0, step: 108930/127656, loss: 0.012174963019788265\n","epoch 0, step: 108940/127656, loss: 0.00849908497184515\n","epoch 0, step: 108950/127656, loss: 0.014815477654337883\n","epoch 0, step: 108960/127656, loss: 0.4230581223964691\n","epoch 0, step: 108970/127656, loss: 0.005731484852731228\n","epoch 0, step: 108980/127656, loss: 0.006250937934964895\n","epoch 0, step: 108990/127656, loss: 0.010582812130451202\n","epoch 0, step: 109000/127656, loss: 0.00989572238177061\n","epoch 0, step: 109010/127656, loss: 0.006264876574277878\n","epoch 0, step: 109020/127656, loss: 0.006801222451031208\n","epoch 0, step: 109030/127656, loss: 0.12102027982473373\n","epoch 0, step: 109040/127656, loss: 0.009584333747625351\n","epoch 0, step: 109050/127656, loss: 0.00746487220749259\n","epoch 0, step: 109060/127656, loss: 0.5647060871124268\n","epoch 0, step: 109070/127656, loss: 0.009332872927188873\n","epoch 0, step: 109080/127656, loss: 0.008355354890227318\n","epoch 0, step: 109090/127656, loss: 0.01001809537410736\n","epoch 0, step: 109100/127656, loss: 0.011293388903141022\n","epoch 0, step: 109110/127656, loss: 0.1440947949886322\n","epoch 0, step: 109120/127656, loss: 0.00683873426169157\n","epoch 0, step: 109130/127656, loss: 0.011450476013123989\n","epoch 0, step: 109140/127656, loss: 0.010585272684693336\n","epoch 0, step: 109150/127656, loss: 0.0076819430105388165\n","epoch 0, step: 109160/127656, loss: 0.012276235967874527\n","epoch 0, step: 109170/127656, loss: 0.008214006200432777\n","epoch 0, step: 109180/127656, loss: 0.007704656571149826\n","epoch 0, step: 109190/127656, loss: 0.012107966467738152\n","epoch 0, step: 109200/127656, loss: 0.010130074806511402\n","epoch 0, step: 109210/127656, loss: 0.013128362596035004\n","epoch 0, step: 109220/127656, loss: 0.011522967368364334\n","epoch 0, step: 109230/127656, loss: 0.007766111753880978\n","epoch 0, step: 109240/127656, loss: 0.005097311455756426\n","epoch 0, step: 109250/127656, loss: 0.0071225883439183235\n","epoch 0, step: 109260/127656, loss: 0.00934454333037138\n","epoch 0, step: 109270/127656, loss: 0.011402716860175133\n","epoch 0, step: 109280/127656, loss: 0.010123570449650288\n","epoch 0, step: 109290/127656, loss: 0.008518422022461891\n","epoch 0, step: 109300/127656, loss: 0.008712265640497208\n","epoch 0, step: 109310/127656, loss: 0.006412172690033913\n","epoch 0, step: 109320/127656, loss: 0.007625860162079334\n","epoch 0, step: 109330/127656, loss: 0.01086115837097168\n","epoch 0, step: 109340/127656, loss: 0.008046520873904228\n","epoch 0, step: 109350/127656, loss: 0.006444280501455069\n","epoch 0, step: 109360/127656, loss: 0.13684070110321045\n","epoch 0, step: 109370/127656, loss: 0.005507674068212509\n","epoch 0, step: 109380/127656, loss: 0.01163163036108017\n","epoch 0, step: 109390/127656, loss: 0.006825994234532118\n","epoch 0, step: 109400/127656, loss: 0.01498120091855526\n","epoch 0, step: 109410/127656, loss: 0.13828986883163452\n","epoch 0, step: 109420/127656, loss: 0.009746967814862728\n","epoch 0, step: 109430/127656, loss: 0.008322082459926605\n","epoch 0, step: 109440/127656, loss: 0.00792594626545906\n","epoch 0, step: 109450/127656, loss: 0.008427203632891178\n","epoch 0, step: 109460/127656, loss: 0.007145047187805176\n","epoch 0, step: 109470/127656, loss: 0.007622461766004562\n","epoch 0, step: 109480/127656, loss: 0.011236841790378094\n","epoch 0, step: 109490/127656, loss: 0.008012698963284492\n","epoch 0, step: 109500/127656, loss: 0.42020997405052185\n","epoch 0, step: 109510/127656, loss: 0.007827929221093655\n","epoch 0, step: 109520/127656, loss: 0.008479633368551731\n","epoch 0, step: 109530/127656, loss: 0.012271764688193798\n","epoch 0, step: 109540/127656, loss: 0.006635948084294796\n","epoch 0, step: 109550/127656, loss: 0.008544053882360458\n","epoch 0, step: 109560/127656, loss: 0.0075035663321614265\n","epoch 0, step: 109570/127656, loss: 0.010120210237801075\n","epoch 0, step: 109580/127656, loss: 0.008399546146392822\n","epoch 0, step: 109590/127656, loss: 0.007844217121601105\n","epoch 0, step: 109600/127656, loss: 0.011564596556127071\n","epoch 0, step: 109610/127656, loss: 0.007626955863088369\n","epoch 0, step: 109620/127656, loss: 0.007581233978271484\n","epoch 0, step: 109630/127656, loss: 0.010109021328389645\n","epoch 0, step: 109640/127656, loss: 0.012404612265527248\n","epoch 0, step: 109650/127656, loss: 0.006341859698295593\n","epoch 0, step: 109660/127656, loss: 0.1273275464773178\n","epoch 0, step: 109670/127656, loss: 0.0054361517541110516\n","epoch 0, step: 109680/127656, loss: 0.008112628012895584\n","epoch 0, step: 109690/127656, loss: 0.011113705113530159\n","epoch 0, step: 109700/127656, loss: 0.011113940738141537\n","epoch 0, step: 109710/127656, loss: 0.39717864990234375\n","epoch 0, step: 109720/127656, loss: 0.008278529159724712\n","epoch 0, step: 109730/127656, loss: 0.0061446912586688995\n","epoch 0, step: 109740/127656, loss: 0.012271209619939327\n","epoch 0, step: 109750/127656, loss: 0.008964797481894493\n","epoch 0, step: 109760/127656, loss: 0.010936367325484753\n","epoch 0, step: 109770/127656, loss: 0.011254390701651573\n","epoch 0, step: 109780/127656, loss: 0.005237847100943327\n","epoch 0, step: 109790/127656, loss: 0.011357693932950497\n","epoch 0, step: 109800/127656, loss: 0.14431807398796082\n","epoch 0, step: 109810/127656, loss: 0.010365060530602932\n","epoch 0, step: 109820/127656, loss: 0.005590915214270353\n","epoch 0, step: 109830/127656, loss: 0.00838493462651968\n","epoch 0, step: 109840/127656, loss: 0.008310304023325443\n","epoch 0, step: 109850/127656, loss: 0.007448102347552776\n","epoch 0, step: 109860/127656, loss: 0.004020231775939465\n","epoch 0, step: 109870/127656, loss: 0.4087086319923401\n","epoch 0, step: 109880/127656, loss: 0.005733917932957411\n","epoch 0, step: 109890/127656, loss: 0.00601551029831171\n","epoch 0, step: 109900/127656, loss: 0.010614579543471336\n","epoch 0, step: 109910/127656, loss: 0.010731343179941177\n","epoch 0, step: 109920/127656, loss: 0.00940726324915886\n","epoch 0, step: 109930/127656, loss: 0.005459889769554138\n","epoch 0, step: 109940/127656, loss: 0.012792102061212063\n","epoch 0, step: 109950/127656, loss: 0.008312324061989784\n","epoch 0, step: 109960/127656, loss: 0.010344717651605606\n","epoch 0, step: 109970/127656, loss: 0.005029515828937292\n","epoch 0, step: 109980/127656, loss: 0.0098055899143219\n","epoch 0, step: 109990/127656, loss: 0.006074875593185425\n","epoch 0, step: 110000/127656, loss: 0.00718847056850791\n","epoch 0, step: 110010/127656, loss: 0.010823993943631649\n","epoch 0, step: 110020/127656, loss: 0.011427107267081738\n","epoch 0, step: 110030/127656, loss: 0.008785305544734001\n","epoch 0, step: 110040/127656, loss: 0.2735021412372589\n","epoch 0, step: 110050/127656, loss: 0.39687228202819824\n","epoch 0, step: 110060/127656, loss: 0.007715312298387289\n","epoch 0, step: 110070/127656, loss: 0.00801791064441204\n","epoch 0, step: 110080/127656, loss: 0.2632003426551819\n","epoch 0, step: 110090/127656, loss: 0.006521513219922781\n","epoch 0, step: 110100/127656, loss: 0.007029639557003975\n","epoch 0, step: 110110/127656, loss: 0.010693714022636414\n","epoch 0, step: 110120/127656, loss: 0.007245148532092571\n","epoch 0, step: 110130/127656, loss: 0.009150070138275623\n","epoch 0, step: 110140/127656, loss: 0.005244959145784378\n","epoch 0, step: 110150/127656, loss: 0.0052755726501345634\n","epoch 0, step: 110160/127656, loss: 0.009616597555577755\n","epoch 0, step: 110170/127656, loss: 0.007888548076152802\n","epoch 0, step: 110180/127656, loss: 0.006699401885271072\n","epoch 0, step: 110190/127656, loss: 0.007224159315228462\n","epoch 0, step: 110200/127656, loss: 0.004906078334897757\n","epoch 0, step: 110210/127656, loss: 0.0057275667786598206\n","epoch 0, step: 110220/127656, loss: 0.008073506876826286\n","epoch 0, step: 110230/127656, loss: 0.007513502612709999\n","epoch 0, step: 110240/127656, loss: 0.006592828780412674\n","epoch 0, step: 110250/127656, loss: 0.008924361318349838\n","epoch 0, step: 110260/127656, loss: 0.01444832794368267\n","epoch 0, step: 110270/127656, loss: 0.005560058169066906\n","epoch 0, step: 110280/127656, loss: 0.008104808628559113\n","epoch 0, step: 110290/127656, loss: 0.009240444749593735\n","epoch 0, step: 110300/127656, loss: 0.011836903169751167\n","epoch 0, step: 110310/127656, loss: 0.005993083119392395\n","epoch 0, step: 110320/127656, loss: 0.007586730644106865\n","epoch 0, step: 110330/127656, loss: 0.010267972014844418\n","epoch 0, step: 110340/127656, loss: 0.010081123560667038\n","epoch 0, step: 110350/127656, loss: 0.013985421508550644\n","epoch 0, step: 110360/127656, loss: 0.008158896118402481\n","epoch 0, step: 110370/127656, loss: 0.012256464920938015\n","epoch 0, step: 110380/127656, loss: 0.00768204964697361\n","epoch 0, step: 110390/127656, loss: 0.008356234058737755\n","epoch 0, step: 110400/127656, loss: 0.013395050540566444\n","epoch 0, step: 110410/127656, loss: 0.004901672713458538\n","epoch 0, step: 110420/127656, loss: 0.6778387427330017\n","epoch 0, step: 110430/127656, loss: 0.006698926445096731\n","epoch 0, step: 110440/127656, loss: 0.007310838904231787\n","epoch 0, step: 110450/127656, loss: 0.00742053147405386\n","epoch 0, step: 110460/127656, loss: 0.00983585137873888\n","epoch 0, step: 110470/127656, loss: 0.010144989006221294\n","epoch 0, step: 110480/127656, loss: 0.006758661475032568\n","epoch 0, step: 110490/127656, loss: 0.008753582835197449\n","epoch 0, step: 110500/127656, loss: 0.005911781452596188\n","epoch 0, step: 110510/127656, loss: 0.008399737998843193\n","epoch 0, step: 110520/127656, loss: 0.009359901770949364\n","epoch 0, step: 110530/127656, loss: 0.009336719289422035\n","epoch 0, step: 110540/127656, loss: 0.26802557706832886\n","epoch 0, step: 110550/127656, loss: 0.010513758286833763\n","epoch 0, step: 110560/127656, loss: 0.12574556469917297\n","epoch 0, step: 110570/127656, loss: 0.007224747445434332\n","epoch 0, step: 110580/127656, loss: 0.010172883979976177\n","epoch 0, step: 110590/127656, loss: 0.00673139002174139\n","epoch 0, step: 110600/127656, loss: 0.010527217760682106\n","epoch 0, step: 110610/127656, loss: 0.008949710987508297\n","epoch 0, step: 110620/127656, loss: 0.005390966311097145\n","epoch 0, step: 110630/127656, loss: 0.012520570307970047\n","epoch 0, step: 110640/127656, loss: 0.13228337466716766\n","epoch 0, step: 110650/127656, loss: 0.016938982531428337\n","epoch 0, step: 110660/127656, loss: 0.007681167684495449\n","epoch 0, step: 110670/127656, loss: 0.007799004670232534\n","epoch 0, step: 110680/127656, loss: 0.006118025630712509\n","epoch 0, step: 110690/127656, loss: 0.008424564264714718\n","epoch 0, step: 110700/127656, loss: 0.006604953669011593\n","epoch 0, step: 110710/127656, loss: 0.009389137849211693\n","epoch 0, step: 110720/127656, loss: 0.008194737136363983\n","epoch 0, step: 110730/127656, loss: 0.012246740981936455\n","epoch 0, step: 110740/127656, loss: 0.1356182098388672\n","epoch 0, step: 110750/127656, loss: 0.009042278863489628\n","epoch 0, step: 110760/127656, loss: 0.003929277881979942\n","epoch 0, step: 110770/127656, loss: 0.007557091303169727\n","epoch 0, step: 110780/127656, loss: 0.012727931141853333\n","epoch 0, step: 110790/127656, loss: 0.014951160177588463\n","epoch 0, step: 110800/127656, loss: 0.005744366906583309\n","epoch 0, step: 110810/127656, loss: 0.008665153756737709\n","epoch 0, step: 110820/127656, loss: 0.007171263452619314\n","epoch 0, step: 110830/127656, loss: 0.009153100661933422\n","epoch 0, step: 110840/127656, loss: 0.00402883579954505\n","epoch 0, step: 110850/127656, loss: 0.008752834983170033\n","epoch 0, step: 110860/127656, loss: 0.004359671380370855\n","epoch 0, step: 110870/127656, loss: 0.014257295057177544\n","epoch 0, step: 110880/127656, loss: 0.41253870725631714\n","epoch 0, step: 110890/127656, loss: 0.012014223262667656\n","epoch 0, step: 110900/127656, loss: 0.008120786398649216\n","epoch 0, step: 110910/127656, loss: 0.005564517341554165\n","epoch 0, step: 110920/127656, loss: 0.016955344006419182\n","epoch 0, step: 110930/127656, loss: 0.005025829188525677\n","epoch 0, step: 110940/127656, loss: 0.007207897491753101\n","epoch 0, step: 110950/127656, loss: 0.01065005548298359\n","epoch 0, step: 110960/127656, loss: 0.0053722308948636055\n","epoch 0, step: 110970/127656, loss: 0.013205586932599545\n","epoch 0, step: 110980/127656, loss: 0.009312405250966549\n","epoch 0, step: 110990/127656, loss: 0.01058182679116726\n","epoch 0, step: 111000/127656, loss: 0.006905301474034786\n","epoch 0, step: 111010/127656, loss: 0.0051822061650455\n","epoch 0, step: 111020/127656, loss: 0.006882499437779188\n","epoch 0, step: 111030/127656, loss: 0.004911845549941063\n","epoch 0, step: 111040/127656, loss: 0.008164283819496632\n","epoch 0, step: 111050/127656, loss: 0.009182271547615528\n","epoch 0, step: 111060/127656, loss: 0.011298330500721931\n","epoch 0, step: 111070/127656, loss: 0.011011078953742981\n","epoch 0, step: 111080/127656, loss: 0.008983034640550613\n","epoch 0, step: 111090/127656, loss: 0.010155296884477139\n","epoch 0, step: 111100/127656, loss: 0.1367689073085785\n","epoch 0, step: 111110/127656, loss: 0.013732492923736572\n","epoch 0, step: 111120/127656, loss: 0.011786749586462975\n","epoch 0, step: 111130/127656, loss: 0.004792447667568922\n","epoch 0, step: 111140/127656, loss: 0.00624605780467391\n","epoch 0, step: 111150/127656, loss: 0.41876929998397827\n","epoch 0, step: 111160/127656, loss: 0.010401862673461437\n","epoch 0, step: 111170/127656, loss: 0.008370624855160713\n","epoch 0, step: 111180/127656, loss: 0.006297153886407614\n","epoch 0, step: 111190/127656, loss: 0.007738220505416393\n","epoch 0, step: 111200/127656, loss: 0.008409696631133556\n","epoch 0, step: 111210/127656, loss: 0.15433384478092194\n","epoch 0, step: 111220/127656, loss: 0.2878786623477936\n","epoch 0, step: 111230/127656, loss: 0.011155683547258377\n","epoch 0, step: 111240/127656, loss: 0.01296321116387844\n","epoch 0, step: 111250/127656, loss: 0.010838029906153679\n","epoch 0, step: 111260/127656, loss: 0.5217282772064209\n","epoch 0, step: 111270/127656, loss: 0.006351769901812077\n","epoch 0, step: 111280/127656, loss: 0.005144411697983742\n","epoch 0, step: 111290/127656, loss: 0.005692781414836645\n","epoch 0, step: 111300/127656, loss: 0.007360201328992844\n","epoch 0, step: 111310/127656, loss: 0.010085951536893845\n","epoch 0, step: 111320/127656, loss: 0.006982294376939535\n","epoch 0, step: 111330/127656, loss: 0.01276942528784275\n","epoch 0, step: 111340/127656, loss: 0.12868952751159668\n","epoch 0, step: 111350/127656, loss: 0.009895452298223972\n","epoch 0, step: 111360/127656, loss: 0.006529209669679403\n","epoch 0, step: 111370/127656, loss: 0.28338390588760376\n","epoch 0, step: 111380/127656, loss: 0.005976091139018536\n","epoch 0, step: 111390/127656, loss: 0.00983036682009697\n","epoch 0, step: 111400/127656, loss: 0.007897749543190002\n","epoch 0, step: 111410/127656, loss: 0.009734525345265865\n","epoch 0, step: 111420/127656, loss: 0.009634196758270264\n","epoch 0, step: 111430/127656, loss: 0.008730189874768257\n","epoch 0, step: 111440/127656, loss: 0.13737085461616516\n","epoch 0, step: 111450/127656, loss: 0.007062733173370361\n","epoch 0, step: 111460/127656, loss: 0.0075262258760631084\n","epoch 0, step: 111470/127656, loss: 0.007312167435884476\n","epoch 0, step: 111480/127656, loss: 0.010264331474900246\n","epoch 0, step: 111490/127656, loss: 0.006775212474167347\n","epoch 0, step: 111500/127656, loss: 0.006229782477021217\n","epoch 0, step: 111510/127656, loss: 0.0071865469217300415\n","epoch 0, step: 111520/127656, loss: 0.0049455976113677025\n","epoch 0, step: 111530/127656, loss: 0.006362743675708771\n","epoch 0, step: 111540/127656, loss: 0.003562245052307844\n","epoch 0, step: 111550/127656, loss: 0.0058328500017523766\n","epoch 0, step: 111560/127656, loss: 0.0069323210045695305\n","epoch 0, step: 111570/127656, loss: 0.009921906515955925\n","epoch 0, step: 111580/127656, loss: 0.005300659686326981\n","epoch 0, step: 111590/127656, loss: 0.011121954768896103\n","epoch 0, step: 111600/127656, loss: 0.0033225826919078827\n","epoch 0, step: 111610/127656, loss: 0.005082685500383377\n","epoch 0, step: 111620/127656, loss: 0.011538962833583355\n","epoch 0, step: 111630/127656, loss: 0.009162851609289646\n","epoch 0, step: 111640/127656, loss: 0.008521361276507378\n","epoch 0, step: 111650/127656, loss: 0.010212114080786705\n","epoch 0, step: 111660/127656, loss: 0.006636735051870346\n","epoch 0, step: 111670/127656, loss: 0.00899554044008255\n","epoch 0, step: 111680/127656, loss: 0.01288074254989624\n","epoch 0, step: 111690/127656, loss: 0.011377088725566864\n","epoch 0, step: 111700/127656, loss: 0.13834349811077118\n","epoch 0, step: 111710/127656, loss: 0.011501731351017952\n","epoch 0, step: 111720/127656, loss: 0.41219276189804077\n","epoch 0, step: 111730/127656, loss: 0.006416703574359417\n","epoch 0, step: 111740/127656, loss: 0.006917423568665981\n","epoch 0, step: 111750/127656, loss: 0.010127817280590534\n","epoch 0, step: 111760/127656, loss: 0.007676498033106327\n","epoch 0, step: 111770/127656, loss: 0.007950307801365852\n","epoch 0, step: 111780/127656, loss: 0.008790779858827591\n","epoch 0, step: 111790/127656, loss: 0.009397049434483051\n","epoch 0, step: 111800/127656, loss: 0.010601663962006569\n","epoch 0, step: 111810/127656, loss: 0.0067150588147342205\n","epoch 0, step: 111820/127656, loss: 0.010569058358669281\n","epoch 0, step: 111830/127656, loss: 0.0141683928668499\n","epoch 0, step: 111840/127656, loss: 0.00889504048973322\n","epoch 0, step: 111850/127656, loss: 0.00792943499982357\n","epoch 0, step: 111860/127656, loss: 0.004852502606809139\n","epoch 0, step: 111870/127656, loss: 0.010808786377310753\n","epoch 0, step: 111880/127656, loss: 0.00479518435895443\n","epoch 0, step: 111890/127656, loss: 0.007036758586764336\n","epoch 0, step: 111900/127656, loss: 0.0076552750542759895\n","epoch 0, step: 111910/127656, loss: 0.2507646679878235\n","epoch 0, step: 111920/127656, loss: 0.011307972483336926\n","epoch 0, step: 111930/127656, loss: 0.005359857343137264\n","epoch 0, step: 111940/127656, loss: 0.005052516702562571\n","epoch 0, step: 111950/127656, loss: 0.007526329718530178\n","epoch 0, step: 111960/127656, loss: 0.00530443899333477\n","epoch 0, step: 111970/127656, loss: 0.010591521859169006\n","epoch 0, step: 111980/127656, loss: 0.1306838095188141\n","epoch 0, step: 111990/127656, loss: 0.008439533412456512\n","epoch 0, step: 112000/127656, loss: 0.009592409245669842\n","epoch 0, step: 112010/127656, loss: 0.008736281655728817\n","epoch 0, step: 112020/127656, loss: 0.14209221303462982\n","epoch 0, step: 112030/127656, loss: 0.01100027747452259\n","epoch 0, step: 112040/127656, loss: 0.005733913742005825\n","epoch 0, step: 112050/127656, loss: 0.008154273964464664\n","epoch 0, step: 112060/127656, loss: 0.009388851001858711\n","epoch 0, step: 112070/127656, loss: 0.26553797721862793\n","epoch 0, step: 112080/127656, loss: 0.005543095525354147\n","epoch 0, step: 112090/127656, loss: 0.00577367190271616\n","epoch 0, step: 112100/127656, loss: 0.00520114041864872\n","epoch 0, step: 112110/127656, loss: 0.010691352188587189\n","epoch 0, step: 112120/127656, loss: 0.010587084107100964\n","epoch 0, step: 112130/127656, loss: 0.009582925587892532\n","epoch 0, step: 112140/127656, loss: 0.008483660407364368\n","epoch 0, step: 112150/127656, loss: 0.009584994986653328\n","epoch 0, step: 112160/127656, loss: 0.005515339784324169\n","epoch 0, step: 112170/127656, loss: 0.005197154823690653\n","epoch 0, step: 112180/127656, loss: 0.007570925168693066\n","epoch 0, step: 112190/127656, loss: 0.005489204078912735\n","epoch 0, step: 112200/127656, loss: 0.010577494278550148\n","epoch 0, step: 112210/127656, loss: 0.008147211745381355\n","epoch 0, step: 112220/127656, loss: 0.010120077058672905\n","epoch 0, step: 112230/127656, loss: 0.005254209041595459\n","epoch 0, step: 112240/127656, loss: 0.00838466640561819\n","epoch 0, step: 112250/127656, loss: 0.0087081678211689\n","epoch 0, step: 112260/127656, loss: 0.013768191449344158\n","epoch 0, step: 112270/127656, loss: 0.008604823611676693\n","epoch 0, step: 112280/127656, loss: 0.008209931664168835\n","epoch 0, step: 112290/127656, loss: 0.006942488253116608\n","epoch 0, step: 112300/127656, loss: 0.013394440524280071\n","epoch 0, step: 112310/127656, loss: 0.008037391118705273\n","epoch 0, step: 112320/127656, loss: 0.010131975635886192\n","epoch 0, step: 112330/127656, loss: 0.006460926961153746\n","epoch 0, step: 112340/127656, loss: 0.013815566897392273\n","epoch 0, step: 112350/127656, loss: 0.00604904955253005\n","epoch 0, step: 112360/127656, loss: 0.007878927513957024\n","epoch 0, step: 112370/127656, loss: 0.13970930874347687\n","epoch 0, step: 112380/127656, loss: 0.009809030219912529\n","epoch 0, step: 112390/127656, loss: 0.009065686725080013\n","epoch 0, step: 112400/127656, loss: 0.006863956339657307\n","epoch 0, step: 112410/127656, loss: 0.13595052063465118\n","epoch 0, step: 112420/127656, loss: 0.008811237290501595\n","epoch 0, step: 112430/127656, loss: 0.0049286093562841415\n","epoch 0, step: 112440/127656, loss: 0.006684637628495693\n","epoch 0, step: 112450/127656, loss: 0.009739205241203308\n","epoch 0, step: 112460/127656, loss: 0.007049215491861105\n","epoch 0, step: 112470/127656, loss: 0.011154672130942345\n","epoch 0, step: 112480/127656, loss: 0.008428622037172318\n","epoch 0, step: 112490/127656, loss: 0.007292538415640593\n","epoch 0, step: 112500/127656, loss: 0.004905493929982185\n","epoch 0, step: 112510/127656, loss: 0.011232912540435791\n","epoch 0, step: 112520/127656, loss: 0.007448762655258179\n","epoch 0, step: 112530/127656, loss: 0.01126542966812849\n","epoch 0, step: 112540/127656, loss: 0.01062946766614914\n","epoch 0, step: 112550/127656, loss: 0.257236123085022\n","epoch 0, step: 112560/127656, loss: 0.005965854972600937\n","epoch 0, step: 112570/127656, loss: 0.271756649017334\n","epoch 0, step: 112580/127656, loss: 0.01011070515960455\n","epoch 0, step: 112590/127656, loss: 0.004756830632686615\n","epoch 0, step: 112600/127656, loss: 0.009116658940911293\n","epoch 0, step: 112610/127656, loss: 0.009582997299730778\n","epoch 0, step: 112620/127656, loss: 0.13504475355148315\n","epoch 0, step: 112630/127656, loss: 0.006666988134384155\n","epoch 0, step: 112640/127656, loss: 0.008680114522576332\n","epoch 0, step: 112650/127656, loss: 0.012144959531724453\n","epoch 0, step: 112660/127656, loss: 0.011238062754273415\n","epoch 0, step: 112670/127656, loss: 0.01267942227423191\n","epoch 0, step: 112680/127656, loss: 0.00900450348854065\n","epoch 0, step: 112690/127656, loss: 0.006064917892217636\n","epoch 0, step: 112700/127656, loss: 0.007695663720369339\n","epoch 0, step: 112710/127656, loss: 0.006425911094993353\n","epoch 0, step: 112720/127656, loss: 0.42006367444992065\n","epoch 0, step: 112730/127656, loss: 0.007191847078502178\n","epoch 0, step: 112740/127656, loss: 0.2738552689552307\n","epoch 0, step: 112750/127656, loss: 0.0067953928373754025\n","epoch 0, step: 112760/127656, loss: 0.006884903647005558\n","epoch 0, step: 112770/127656, loss: 0.008766023442149162\n","epoch 0, step: 112780/127656, loss: 0.13306725025177002\n","epoch 0, step: 112790/127656, loss: 0.005591731518507004\n","epoch 0, step: 112800/127656, loss: 0.008390918374061584\n","epoch 0, step: 112810/127656, loss: 0.00824130792170763\n","epoch 0, step: 112820/127656, loss: 0.008138081058859825\n","epoch 0, step: 112830/127656, loss: 0.005929871462285519\n","epoch 0, step: 112840/127656, loss: 0.005795647390186787\n","epoch 0, step: 112850/127656, loss: 0.00839827861636877\n","epoch 0, step: 112860/127656, loss: 0.006024496629834175\n","epoch 0, step: 112870/127656, loss: 0.008866705931723118\n","epoch 0, step: 112880/127656, loss: 0.011027820408344269\n","epoch 0, step: 112890/127656, loss: 0.6906275153160095\n","epoch 0, step: 112900/127656, loss: 0.0073596457950770855\n","epoch 0, step: 112910/127656, loss: 0.01027311198413372\n","epoch 0, step: 112920/127656, loss: 0.012040457688272\n","epoch 0, step: 112930/127656, loss: 0.007409378886222839\n","epoch 0, step: 112940/127656, loss: 0.006490325089544058\n","epoch 0, step: 112950/127656, loss: 0.008340494707226753\n","epoch 0, step: 112960/127656, loss: 0.009423216804862022\n","epoch 0, step: 112970/127656, loss: 0.14790239930152893\n","epoch 0, step: 112980/127656, loss: 0.2754063308238983\n","epoch 0, step: 112990/127656, loss: 0.006555846892297268\n","epoch 0, step: 113000/127656, loss: 0.010449199937283993\n","epoch 0, step: 113010/127656, loss: 0.0068872245028615\n","epoch 0, step: 113020/127656, loss: 0.00561416894197464\n","epoch 0, step: 113030/127656, loss: 0.010286200791597366\n","epoch 0, step: 113040/127656, loss: 0.005175712984055281\n","epoch 0, step: 113050/127656, loss: 0.0061195059679448605\n","epoch 0, step: 113060/127656, loss: 0.00867673009634018\n","epoch 0, step: 113070/127656, loss: 0.28715673089027405\n","epoch 0, step: 113080/127656, loss: 0.011308904737234116\n","epoch 0, step: 113090/127656, loss: 0.007783078588545322\n","epoch 0, step: 113100/127656, loss: 0.006084398832172155\n","epoch 0, step: 113110/127656, loss: 0.006641795393079519\n","epoch 0, step: 113120/127656, loss: 0.011167595162987709\n","epoch 0, step: 113130/127656, loss: 0.005160922184586525\n","epoch 0, step: 113140/127656, loss: 0.006699476391077042\n","epoch 0, step: 113150/127656, loss: 0.007036607246845961\n","epoch 0, step: 113160/127656, loss: 0.015120372176170349\n","epoch 0, step: 113170/127656, loss: 0.00841422751545906\n","epoch 0, step: 113180/127656, loss: 0.005164316855370998\n","epoch 0, step: 113190/127656, loss: 0.007323445286601782\n","epoch 0, step: 113200/127656, loss: 0.010943002998828888\n","epoch 0, step: 113210/127656, loss: 0.007997272536158562\n","epoch 0, step: 113220/127656, loss: 0.009147364646196365\n","epoch 0, step: 113230/127656, loss: 0.00465215090662241\n","epoch 0, step: 113240/127656, loss: 0.007690670900046825\n","epoch 0, step: 113250/127656, loss: 0.0073315659537911415\n","epoch 0, step: 113260/127656, loss: 0.011269859969615936\n","epoch 0, step: 113270/127656, loss: 0.008764764294028282\n","epoch 0, step: 113280/127656, loss: 0.009968448430299759\n","epoch 0, step: 113290/127656, loss: 0.007622571662068367\n","epoch 0, step: 113300/127656, loss: 0.006340562365949154\n","epoch 0, step: 113310/127656, loss: 0.0065818652510643005\n","epoch 0, step: 113320/127656, loss: 0.0058465804904699326\n","epoch 0, step: 113330/127656, loss: 0.005690404679626226\n","epoch 0, step: 113340/127656, loss: 0.008742116391658783\n","epoch 0, step: 113350/127656, loss: 0.014444900676608086\n","epoch 0, step: 113360/127656, loss: 0.008632088080048561\n","epoch 0, step: 113370/127656, loss: 0.009459543973207474\n","epoch 0, step: 113380/127656, loss: 0.013710270635783672\n","epoch 0, step: 113390/127656, loss: 0.007574410643428564\n","epoch 0, step: 113400/127656, loss: 0.00459723686799407\n","epoch 0, step: 113410/127656, loss: 0.009015701711177826\n","epoch 0, step: 113420/127656, loss: 0.0061573549173772335\n","epoch 0, step: 113430/127656, loss: 0.007415571715682745\n","epoch 0, step: 113440/127656, loss: 0.007536867633461952\n","epoch 0, step: 113450/127656, loss: 0.008544541895389557\n","epoch 0, step: 113460/127656, loss: 0.2635772228240967\n","epoch 0, step: 113470/127656, loss: 0.008104241453111172\n","epoch 0, step: 113480/127656, loss: 0.008108341135084629\n","epoch 0, step: 113490/127656, loss: 0.005411859136074781\n","epoch 0, step: 113500/127656, loss: 0.008387084119021893\n","epoch 0, step: 113510/127656, loss: 0.003819477278739214\n","epoch 0, step: 113520/127656, loss: 0.00887956004589796\n","epoch 0, step: 113530/127656, loss: 0.010433290153741837\n","epoch 0, step: 113540/127656, loss: 0.006033126264810562\n","epoch 0, step: 113550/127656, loss: 0.007424637209624052\n","epoch 0, step: 113560/127656, loss: 0.006945883389562368\n","epoch 0, step: 113570/127656, loss: 0.411592572927475\n","epoch 0, step: 113580/127656, loss: 0.008897542953491211\n","epoch 0, step: 113590/127656, loss: 0.007768443785607815\n","epoch 0, step: 113600/127656, loss: 0.008202958852052689\n","epoch 0, step: 113610/127656, loss: 0.007036917842924595\n","epoch 0, step: 113620/127656, loss: 0.008067836984992027\n","epoch 0, step: 113630/127656, loss: 0.008505372330546379\n","epoch 0, step: 113640/127656, loss: 0.26392245292663574\n","epoch 0, step: 113650/127656, loss: 0.007646143436431885\n","epoch 0, step: 113660/127656, loss: 0.00800386443734169\n","epoch 0, step: 113670/127656, loss: 0.005721536930650473\n","epoch 0, step: 113680/127656, loss: 0.011146861128509045\n","epoch 0, step: 113690/127656, loss: 0.0097845159471035\n","epoch 0, step: 113700/127656, loss: 0.43430039286613464\n","epoch 0, step: 113710/127656, loss: 0.008254577405750751\n","epoch 0, step: 113720/127656, loss: 0.00887612160295248\n","epoch 0, step: 113730/127656, loss: 0.008929114788770676\n","epoch 0, step: 113740/127656, loss: 0.010367752984166145\n","epoch 0, step: 113750/127656, loss: 0.012576401233673096\n","epoch 0, step: 113760/127656, loss: 0.00771993026137352\n","epoch 0, step: 113770/127656, loss: 0.005477462895214558\n","epoch 0, step: 113780/127656, loss: 0.005442178808152676\n","epoch 0, step: 113790/127656, loss: 0.00867394544184208\n","epoch 0, step: 113800/127656, loss: 0.548845648765564\n","epoch 0, step: 113810/127656, loss: 0.5455359220504761\n","epoch 0, step: 113820/127656, loss: 0.008852832950651646\n","epoch 0, step: 113830/127656, loss: 0.007439378648996353\n","epoch 0, step: 113840/127656, loss: 0.1416187286376953\n","epoch 0, step: 113850/127656, loss: 0.008960838429629803\n","epoch 0, step: 113860/127656, loss: 0.012300675734877586\n","epoch 0, step: 113870/127656, loss: 0.007259204052388668\n","epoch 0, step: 113880/127656, loss: 0.0046018450520932674\n","epoch 0, step: 113890/127656, loss: 0.00643534492701292\n","epoch 0, step: 113900/127656, loss: 0.006818266119807959\n","epoch 0, step: 113910/127656, loss: 0.010266158729791641\n","epoch 0, step: 113920/127656, loss: 0.0075321560725569725\n","epoch 0, step: 113930/127656, loss: 0.007169464137405157\n","epoch 0, step: 113940/127656, loss: 0.40310367941856384\n","epoch 0, step: 113950/127656, loss: 0.00962458923459053\n","epoch 0, step: 113960/127656, loss: 0.010708784684538841\n","epoch 0, step: 113970/127656, loss: 0.007501772604882717\n","epoch 0, step: 113980/127656, loss: 0.012664251029491425\n","epoch 0, step: 113990/127656, loss: 0.007371329702436924\n","epoch 0, step: 114000/127656, loss: 0.008647017180919647\n","epoch 0, step: 114010/127656, loss: 0.009011849761009216\n","epoch 0, step: 114020/127656, loss: 0.009196236729621887\n","epoch 0, step: 114030/127656, loss: 0.13999292254447937\n","epoch 0, step: 114040/127656, loss: 0.011353777721524239\n","epoch 0, step: 114050/127656, loss: 0.009434716776013374\n","epoch 0, step: 114060/127656, loss: 0.004626525100320578\n","epoch 0, step: 114070/127656, loss: 0.005699218716472387\n","epoch 0, step: 114080/127656, loss: 0.009718919172883034\n","epoch 0, step: 114090/127656, loss: 0.006396793760359287\n","epoch 0, step: 114100/127656, loss: 0.006734754890203476\n","epoch 0, step: 114110/127656, loss: 0.00759215047582984\n","epoch 0, step: 114120/127656, loss: 0.008489388041198254\n","epoch 0, step: 114130/127656, loss: 0.00965171679854393\n","epoch 0, step: 114140/127656, loss: 0.006861110217869282\n","epoch 0, step: 114150/127656, loss: 0.01270383596420288\n","epoch 0, step: 114160/127656, loss: 0.011593692004680634\n","epoch 0, step: 114170/127656, loss: 0.005903120152652264\n","epoch 0, step: 114180/127656, loss: 0.008023849688470364\n","epoch 0, step: 114190/127656, loss: 0.007384129799902439\n","epoch 0, step: 114200/127656, loss: 0.006855183746665716\n","epoch 0, step: 114210/127656, loss: 0.010026298463344574\n","epoch 0, step: 114220/127656, loss: 0.009924394078552723\n","epoch 0, step: 114230/127656, loss: 0.008836108259856701\n","epoch 0, step: 114240/127656, loss: 0.006631569005548954\n","epoch 0, step: 114250/127656, loss: 0.01074221171438694\n","epoch 0, step: 114260/127656, loss: 0.00911034271121025\n","epoch 0, step: 114270/127656, loss: 0.015319610945880413\n","epoch 0, step: 114280/127656, loss: 0.010153083130717278\n","epoch 0, step: 114290/127656, loss: 0.5631773471832275\n","epoch 0, step: 114300/127656, loss: 0.007038217969238758\n","epoch 0, step: 114310/127656, loss: 0.008582664653658867\n","epoch 0, step: 114320/127656, loss: 0.008841082453727722\n","epoch 0, step: 114330/127656, loss: 0.006809060927480459\n","epoch 0, step: 114340/127656, loss: 0.008381444029510021\n","epoch 0, step: 114350/127656, loss: 0.007607686799019575\n","epoch 0, step: 114360/127656, loss: 0.011907952837646008\n","epoch 0, step: 114370/127656, loss: 0.5580548644065857\n","epoch 0, step: 114380/127656, loss: 0.01155024766921997\n","epoch 0, step: 114390/127656, loss: 0.0067949313670396805\n","epoch 0, step: 114400/127656, loss: 0.006467398256063461\n","epoch 0, step: 114410/127656, loss: 0.006303492002189159\n","epoch 0, step: 114420/127656, loss: 0.007234192918986082\n","epoch 0, step: 114430/127656, loss: 0.008896871469914913\n","epoch 0, step: 114440/127656, loss: 0.010164432227611542\n","epoch 0, step: 114450/127656, loss: 0.006287558935582638\n","epoch 0, step: 114460/127656, loss: 0.008526872843503952\n","epoch 0, step: 114470/127656, loss: 0.0070102401077747345\n","epoch 0, step: 114480/127656, loss: 0.005174068734049797\n","epoch 0, step: 114490/127656, loss: 0.008175039663910866\n","epoch 0, step: 114500/127656, loss: 0.008962681517004967\n","epoch 0, step: 114510/127656, loss: 0.1413712054491043\n","epoch 0, step: 114520/127656, loss: 0.008112498559057713\n","epoch 0, step: 114530/127656, loss: 0.005018789321184158\n","epoch 0, step: 114540/127656, loss: 0.005914691835641861\n","epoch 0, step: 114550/127656, loss: 0.010425308719277382\n","epoch 0, step: 114560/127656, loss: 0.008024735376238823\n","epoch 0, step: 114570/127656, loss: 0.008132310584187508\n","epoch 0, step: 114580/127656, loss: 0.006260785739868879\n","epoch 0, step: 114590/127656, loss: 0.007954791188240051\n","epoch 0, step: 114600/127656, loss: 0.008722210302948952\n","epoch 0, step: 114610/127656, loss: 0.009068061597645283\n","epoch 0, step: 114620/127656, loss: 0.014597821049392223\n","epoch 0, step: 114630/127656, loss: 0.004936516284942627\n","epoch 0, step: 114640/127656, loss: 0.009009858593344688\n","epoch 0, step: 114650/127656, loss: 0.009380142204463482\n","epoch 0, step: 114660/127656, loss: 0.009879568591713905\n","epoch 0, step: 114670/127656, loss: 0.005166665650904179\n","epoch 0, step: 114680/127656, loss: 0.01146010309457779\n","epoch 0, step: 114690/127656, loss: 0.0102357417345047\n","epoch 0, step: 114700/127656, loss: 0.008199041709303856\n","epoch 0, step: 114710/127656, loss: 0.005822710692882538\n","epoch 0, step: 114720/127656, loss: 0.010276734828948975\n","epoch 0, step: 114730/127656, loss: 0.0107814846560359\n","epoch 0, step: 114740/127656, loss: 0.00907151773571968\n","epoch 0, step: 114750/127656, loss: 0.00640309602022171\n","epoch 0, step: 114760/127656, loss: 0.007260605692863464\n","epoch 0, step: 114770/127656, loss: 0.01206512562930584\n","epoch 0, step: 114780/127656, loss: 0.008758576586842537\n","epoch 0, step: 114790/127656, loss: 0.007482258137315512\n","epoch 0, step: 114800/127656, loss: 0.007684935815632343\n","epoch 0, step: 114810/127656, loss: 0.010223085060715675\n","epoch 0, step: 114820/127656, loss: 0.007356574293226004\n","epoch 0, step: 114830/127656, loss: 0.009333517402410507\n","epoch 0, step: 114840/127656, loss: 0.016684485599398613\n","epoch 0, step: 114850/127656, loss: 0.011650650762021542\n","epoch 0, step: 114860/127656, loss: 0.008674048818647861\n","epoch 0, step: 114870/127656, loss: 0.1253681778907776\n","epoch 0, step: 114880/127656, loss: 0.2611834406852722\n","epoch 0, step: 114890/127656, loss: 0.008187963627278805\n","epoch 0, step: 114900/127656, loss: 0.00789475068449974\n","epoch 0, step: 114910/127656, loss: 0.007148833479732275\n","epoch 0, step: 114920/127656, loss: 0.005581321194767952\n","epoch 0, step: 114930/127656, loss: 0.011273576878011227\n","epoch 0, step: 114940/127656, loss: 0.010518928989768028\n","epoch 0, step: 114950/127656, loss: 0.0075753978453576565\n","epoch 0, step: 114960/127656, loss: 0.5260242223739624\n","epoch 0, step: 114970/127656, loss: 0.01224118284881115\n","epoch 0, step: 114980/127656, loss: 0.007253724150359631\n","epoch 0, step: 114990/127656, loss: 0.011343481950461864\n","epoch 0, step: 115000/127656, loss: 0.007261555176228285\n","epoch 0, step: 115010/127656, loss: 0.006170689594000578\n","epoch 0, step: 115020/127656, loss: 0.006464079022407532\n","epoch 0, step: 115030/127656, loss: 0.011755312792956829\n","epoch 0, step: 115040/127656, loss: 0.5574754476547241\n","epoch 0, step: 115050/127656, loss: 0.009913846850395203\n","epoch 0, step: 115060/127656, loss: 0.004779442213475704\n","epoch 0, step: 115070/127656, loss: 0.0046327728778123856\n","epoch 0, step: 115080/127656, loss: 0.13242585957050323\n","epoch 0, step: 115090/127656, loss: 0.008300590328872204\n","epoch 0, step: 115100/127656, loss: 0.011052731424570084\n","epoch 0, step: 115110/127656, loss: 0.11979841440916061\n","epoch 0, step: 115120/127656, loss: 0.005485917907208204\n","epoch 0, step: 115130/127656, loss: 0.007788620889186859\n","epoch 0, step: 115140/127656, loss: 0.007429401855915785\n","epoch 0, step: 115150/127656, loss: 0.007313246838748455\n","epoch 0, step: 115160/127656, loss: 0.012451911345124245\n","epoch 0, step: 115170/127656, loss: 0.009556107223033905\n","epoch 0, step: 115180/127656, loss: 0.006345647387206554\n","epoch 0, step: 115190/127656, loss: 0.005576338153332472\n","epoch 0, step: 115200/127656, loss: 0.005812068935483694\n","epoch 0, step: 115210/127656, loss: 0.004378755111247301\n","epoch 0, step: 115220/127656, loss: 0.27909988164901733\n","epoch 0, step: 115230/127656, loss: 0.009202703833580017\n","epoch 0, step: 115240/127656, loss: 0.00642356276512146\n","epoch 0, step: 115250/127656, loss: 0.3922111988067627\n","epoch 0, step: 115260/127656, loss: 0.004792673978954554\n","epoch 0, step: 115270/127656, loss: 0.010282790288329124\n","epoch 0, step: 115280/127656, loss: 0.006390771828591824\n","epoch 0, step: 115290/127656, loss: 0.004579243715852499\n","epoch 0, step: 115300/127656, loss: 0.006364231463521719\n","epoch 0, step: 115310/127656, loss: 0.009788289666175842\n","epoch 0, step: 115320/127656, loss: 0.009952323511242867\n","epoch 0, step: 115330/127656, loss: 0.006972187664359808\n","epoch 0, step: 115340/127656, loss: 0.26322901248931885\n","epoch 0, step: 115350/127656, loss: 0.00971852894872427\n","epoch 0, step: 115360/127656, loss: 0.004758622497320175\n","epoch 0, step: 115370/127656, loss: 0.005966947879642248\n","epoch 0, step: 115380/127656, loss: 0.005722193978726864\n","epoch 0, step: 115390/127656, loss: 0.011476023122668266\n","epoch 0, step: 115400/127656, loss: 0.011270581744611263\n","epoch 0, step: 115410/127656, loss: 0.009346836246550083\n","epoch 0, step: 115420/127656, loss: 0.14009806513786316\n","epoch 0, step: 115430/127656, loss: 0.00862249918282032\n","epoch 0, step: 115440/127656, loss: 0.008312514051795006\n","epoch 0, step: 115450/127656, loss: 0.007033211644738913\n","epoch 0, step: 115460/127656, loss: 0.011095983907580376\n","epoch 0, step: 115470/127656, loss: 0.006581611931324005\n","epoch 0, step: 115480/127656, loss: 0.006111478433012962\n","epoch 0, step: 115490/127656, loss: 0.008038878440856934\n","epoch 0, step: 115500/127656, loss: 0.003264710307121277\n","epoch 0, step: 115510/127656, loss: 0.3846346139907837\n","epoch 0, step: 115520/127656, loss: 0.013380507938563824\n","epoch 0, step: 115530/127656, loss: 0.008018380030989647\n","epoch 0, step: 115540/127656, loss: 0.00955873727798462\n","epoch 0, step: 115550/127656, loss: 0.005253837909549475\n","epoch 0, step: 115560/127656, loss: 0.00972602516412735\n","epoch 0, step: 115570/127656, loss: 0.00766701390966773\n","epoch 0, step: 115580/127656, loss: 0.007342717610299587\n","epoch 0, step: 115590/127656, loss: 0.01265503466129303\n","epoch 0, step: 115600/127656, loss: 0.005920120514929295\n","epoch 0, step: 115610/127656, loss: 0.13540038466453552\n","epoch 0, step: 115620/127656, loss: 0.008070584386587143\n","epoch 0, step: 115630/127656, loss: 0.006418311502784491\n","epoch 0, step: 115640/127656, loss: 0.008387467823922634\n","epoch 0, step: 115650/127656, loss: 0.008244751021265984\n","epoch 0, step: 115660/127656, loss: 0.010169515386223793\n","epoch 0, step: 115670/127656, loss: 0.0065801991149783134\n","epoch 0, step: 115680/127656, loss: 0.004258883651345968\n","epoch 0, step: 115690/127656, loss: 0.007896347902715206\n","epoch 0, step: 115700/127656, loss: 0.2763942778110504\n","epoch 0, step: 115710/127656, loss: 0.0054127369076013565\n","epoch 0, step: 115720/127656, loss: 0.008866912685334682\n","epoch 0, step: 115730/127656, loss: 0.005667730700224638\n","epoch 0, step: 115740/127656, loss: 0.006349107250571251\n","epoch 0, step: 115750/127656, loss: 0.006021886132657528\n","epoch 0, step: 115760/127656, loss: 0.007412971928715706\n","epoch 0, step: 115770/127656, loss: 0.006058742292225361\n","epoch 0, step: 115780/127656, loss: 0.009159008041024208\n","epoch 0, step: 115790/127656, loss: 0.0073170289397239685\n","epoch 0, step: 115800/127656, loss: 0.009027807973325253\n","epoch 0, step: 115810/127656, loss: 0.011403035372495651\n","epoch 0, step: 115820/127656, loss: 0.008574704639613628\n","epoch 0, step: 115830/127656, loss: 0.01132463850080967\n","epoch 0, step: 115840/127656, loss: 0.007967274636030197\n","epoch 0, step: 115850/127656, loss: 0.007319390773773193\n","epoch 0, step: 115860/127656, loss: 0.2680266499519348\n","epoch 0, step: 115870/127656, loss: 0.00976722314953804\n","epoch 0, step: 115880/127656, loss: 0.008075506426393986\n","epoch 0, step: 115890/127656, loss: 0.007128348108381033\n","epoch 0, step: 115900/127656, loss: 0.00891314260661602\n","epoch 0, step: 115910/127656, loss: 0.006829218938946724\n","epoch 0, step: 115920/127656, loss: 0.0071913329884409904\n","epoch 0, step: 115930/127656, loss: 0.006745472550392151\n","epoch 0, step: 115940/127656, loss: 0.12690910696983337\n","epoch 0, step: 115950/127656, loss: 0.008290882222354412\n","epoch 0, step: 115960/127656, loss: 0.007758216001093388\n","epoch 0, step: 115970/127656, loss: 0.006500798277556896\n","epoch 0, step: 115980/127656, loss: 0.0074020265601575375\n","epoch 0, step: 115990/127656, loss: 0.009461845271289349\n","epoch 0, step: 116000/127656, loss: 0.5659645795822144\n","epoch 0, step: 116010/127656, loss: 0.007154157385230064\n","epoch 0, step: 116020/127656, loss: 0.1415921151638031\n","epoch 0, step: 116030/127656, loss: 0.007032245397567749\n","epoch 0, step: 116040/127656, loss: 0.0084272725507617\n","epoch 0, step: 116050/127656, loss: 0.005321820732206106\n","epoch 0, step: 116060/127656, loss: 0.006970517337322235\n","epoch 0, step: 116070/127656, loss: 0.007352951914072037\n","epoch 0, step: 116080/127656, loss: 0.008013652637600899\n","epoch 0, step: 116090/127656, loss: 0.009849322028458118\n","epoch 0, step: 116100/127656, loss: 0.007717429660260677\n","epoch 0, step: 116110/127656, loss: 0.007165963761508465\n","epoch 0, step: 116120/127656, loss: 0.004764888435602188\n","epoch 0, step: 116130/127656, loss: 0.007018336560577154\n","epoch 0, step: 116140/127656, loss: 0.00814975704997778\n","epoch 0, step: 116150/127656, loss: 0.006346250884234905\n","epoch 0, step: 116160/127656, loss: 0.00886729545891285\n","epoch 0, step: 116170/127656, loss: 0.0106106698513031\n","epoch 0, step: 116180/127656, loss: 0.00967565830796957\n","epoch 0, step: 116190/127656, loss: 0.005858990829437971\n","epoch 0, step: 116200/127656, loss: 0.006063263863325119\n","epoch 0, step: 116210/127656, loss: 0.004665639251470566\n","epoch 0, step: 116220/127656, loss: 0.011410634033381939\n","epoch 0, step: 116230/127656, loss: 0.006761485245078802\n","epoch 0, step: 116240/127656, loss: 0.0068716551177203655\n","epoch 0, step: 116250/127656, loss: 0.007910667918622494\n","epoch 0, step: 116260/127656, loss: 0.004745105281472206\n","epoch 0, step: 116270/127656, loss: 0.011009417474269867\n","epoch 0, step: 116280/127656, loss: 0.005700035952031612\n","epoch 0, step: 116290/127656, loss: 0.008341263048350811\n","epoch 0, step: 116300/127656, loss: 0.008721606805920601\n","epoch 0, step: 116310/127656, loss: 0.006292872130870819\n","epoch 0, step: 116320/127656, loss: 0.0065485648810863495\n","epoch 0, step: 116330/127656, loss: 0.007080206647515297\n","epoch 0, step: 116340/127656, loss: 0.006943525746464729\n","epoch 0, step: 116350/127656, loss: 0.00905752182006836\n","epoch 0, step: 116360/127656, loss: 0.5447131991386414\n","epoch 0, step: 116370/127656, loss: 0.00793385785073042\n","epoch 0, step: 116380/127656, loss: 0.00879792682826519\n","epoch 0, step: 116390/127656, loss: 0.007465203758329153\n","epoch 0, step: 116400/127656, loss: 0.00959685817360878\n","epoch 0, step: 116410/127656, loss: 0.12898457050323486\n","epoch 0, step: 116420/127656, loss: 0.5415760278701782\n","epoch 0, step: 116430/127656, loss: 0.006632249802350998\n","epoch 0, step: 116440/127656, loss: 0.010369319468736649\n","epoch 0, step: 116450/127656, loss: 0.00908138882368803\n","epoch 0, step: 116460/127656, loss: 0.01232980377972126\n","epoch 0, step: 116470/127656, loss: 0.008852910250425339\n","epoch 0, step: 116480/127656, loss: 0.008085906505584717\n","epoch 0, step: 116490/127656, loss: 0.005834250710904598\n","epoch 0, step: 116500/127656, loss: 0.011695711873471737\n","epoch 0, step: 116510/127656, loss: 0.007364836987107992\n","epoch 0, step: 116520/127656, loss: 0.009844779968261719\n","epoch 0, step: 116530/127656, loss: 0.011839356273412704\n","epoch 0, step: 116540/127656, loss: 0.010181386023759842\n","epoch 0, step: 116550/127656, loss: 0.007714310195297003\n","epoch 0, step: 116560/127656, loss: 0.006808048114180565\n","epoch 0, step: 116570/127656, loss: 0.00643129600211978\n","epoch 0, step: 116580/127656, loss: 0.006963891908526421\n","epoch 0, step: 116590/127656, loss: 0.26112982630729675\n","epoch 0, step: 116600/127656, loss: 0.011148652993142605\n","epoch 0, step: 116610/127656, loss: 0.00879164319485426\n","epoch 0, step: 116620/127656, loss: 0.00914595928043127\n","epoch 0, step: 116630/127656, loss: 0.008835475891828537\n","epoch 0, step: 116640/127656, loss: 0.005677750334143639\n","epoch 0, step: 116650/127656, loss: 0.006255583371967077\n","epoch 0, step: 116660/127656, loss: 0.006572015583515167\n","epoch 0, step: 116670/127656, loss: 0.0091206980869174\n","epoch 0, step: 116680/127656, loss: 0.006046987138688564\n","epoch 0, step: 116690/127656, loss: 0.006244441028684378\n","epoch 0, step: 116700/127656, loss: 0.01069418154656887\n","epoch 0, step: 116710/127656, loss: 0.007990106008946896\n","epoch 0, step: 116720/127656, loss: 0.007644254248589277\n","epoch 0, step: 116730/127656, loss: 0.007040935568511486\n","epoch 0, step: 116740/127656, loss: 0.005730625241994858\n","epoch 0, step: 116750/127656, loss: 0.008161612786352634\n","epoch 0, step: 116760/127656, loss: 0.005773298442363739\n","epoch 0, step: 116770/127656, loss: 0.009636815637350082\n","epoch 0, step: 116780/127656, loss: 0.004244641400873661\n","epoch 0, step: 116790/127656, loss: 0.0065023466013371944\n","epoch 0, step: 116800/127656, loss: 0.008580315858125687\n","epoch 0, step: 116810/127656, loss: 0.010536844842135906\n","epoch 0, step: 116820/127656, loss: 0.008842144161462784\n","epoch 0, step: 116830/127656, loss: 0.004201759118586779\n","epoch 0, step: 116840/127656, loss: 0.003659376874566078\n","epoch 0, step: 116850/127656, loss: 0.0062295254319906235\n","epoch 0, step: 116860/127656, loss: 0.010080083273351192\n","epoch 0, step: 116870/127656, loss: 0.009011217392981052\n","epoch 0, step: 116880/127656, loss: 0.007857589051127434\n","epoch 0, step: 116890/127656, loss: 0.005811592098325491\n","epoch 0, step: 116900/127656, loss: 0.007757752668112516\n","epoch 0, step: 116910/127656, loss: 0.00990083534270525\n","epoch 0, step: 116920/127656, loss: 0.008104007691144943\n","epoch 0, step: 116930/127656, loss: 0.009612057358026505\n","epoch 0, step: 116940/127656, loss: 0.005195106379687786\n","epoch 0, step: 116950/127656, loss: 0.005756733939051628\n","epoch 0, step: 116960/127656, loss: 0.005998033564537764\n","epoch 0, step: 116970/127656, loss: 0.012015154585242271\n","epoch 0, step: 116980/127656, loss: 0.12982872128486633\n","epoch 0, step: 116990/127656, loss: 0.006524290889501572\n","epoch 0, step: 117000/127656, loss: 0.008997742086648941\n","epoch 0, step: 117010/127656, loss: 0.006218503229320049\n","epoch 0, step: 117020/127656, loss: 0.00854266993701458\n","epoch 0, step: 117030/127656, loss: 0.007228530012071133\n","epoch 0, step: 117040/127656, loss: 0.5539320111274719\n","epoch 0, step: 117050/127656, loss: 0.006990163587033749\n","epoch 0, step: 117060/127656, loss: 0.01297727506607771\n","epoch 0, step: 117070/127656, loss: 0.008604693226516247\n","epoch 0, step: 117080/127656, loss: 0.00966552458703518\n","epoch 0, step: 117090/127656, loss: 0.007086101453751326\n","epoch 0, step: 117100/127656, loss: 0.006951853167265654\n","epoch 0, step: 117110/127656, loss: 0.00993726309388876\n","epoch 0, step: 117120/127656, loss: 0.007481644861400127\n","epoch 0, step: 117130/127656, loss: 0.007052870001643896\n","epoch 0, step: 117140/127656, loss: 0.004533976782113314\n","epoch 0, step: 117150/127656, loss: 0.005584265571087599\n","epoch 0, step: 117160/127656, loss: 0.39013728499412537\n","epoch 0, step: 117170/127656, loss: 0.007012126035988331\n","epoch 0, step: 117180/127656, loss: 0.004786463920027018\n","epoch 0, step: 117190/127656, loss: 0.012337404303252697\n","epoch 0, step: 117200/127656, loss: 0.005676888860762119\n","epoch 0, step: 117210/127656, loss: 0.006282988935709\n","epoch 0, step: 117220/127656, loss: 0.43555694818496704\n","epoch 0, step: 117230/127656, loss: 0.0059654670767486095\n","epoch 0, step: 117240/127656, loss: 0.008324755355715752\n","epoch 0, step: 117250/127656, loss: 0.008036254905164242\n","epoch 0, step: 117260/127656, loss: 0.28831779956817627\n","epoch 0, step: 117270/127656, loss: 0.005924719385802746\n","epoch 0, step: 117280/127656, loss: 0.009971508756279945\n","epoch 0, step: 117290/127656, loss: 0.004757091403007507\n","epoch 0, step: 117300/127656, loss: 0.007965872064232826\n","epoch 0, step: 117310/127656, loss: 0.008865482173860073\n","epoch 0, step: 117320/127656, loss: 0.007083085831254721\n","epoch 0, step: 117330/127656, loss: 0.007349214516580105\n","epoch 0, step: 117340/127656, loss: 0.005694013088941574\n","epoch 0, step: 117350/127656, loss: 0.008248591795563698\n","epoch 0, step: 117360/127656, loss: 0.005608743987977505\n","epoch 0, step: 117370/127656, loss: 0.007478377781808376\n","epoch 0, step: 117380/127656, loss: 0.009061849676072598\n","epoch 0, step: 117390/127656, loss: 0.0060989754274487495\n","epoch 0, step: 117400/127656, loss: 0.009511351585388184\n","epoch 0, step: 117410/127656, loss: 0.0062318770214915276\n","epoch 0, step: 117420/127656, loss: 0.010431434027850628\n","epoch 0, step: 117430/127656, loss: 0.008873446844518185\n","epoch 0, step: 117440/127656, loss: 0.010502984747290611\n","epoch 0, step: 117450/127656, loss: 0.0038815762382000685\n","epoch 0, step: 117460/127656, loss: 0.0045463829301297665\n","epoch 0, step: 117470/127656, loss: 0.008077388629317284\n","epoch 0, step: 117480/127656, loss: 0.0073644742369651794\n","epoch 0, step: 117490/127656, loss: 0.006070966832339764\n","epoch 0, step: 117500/127656, loss: 0.007622695527970791\n","epoch 0, step: 117510/127656, loss: 0.007822785526514053\n","epoch 0, step: 117520/127656, loss: 0.009686468169093132\n","epoch 0, step: 117530/127656, loss: 0.0070443484000861645\n","epoch 0, step: 117540/127656, loss: 0.008035429753363132\n","epoch 0, step: 117550/127656, loss: 0.008192487061023712\n","epoch 0, step: 117560/127656, loss: 0.011547146365046501\n","epoch 0, step: 117570/127656, loss: 0.009272299706935883\n","epoch 0, step: 117580/127656, loss: 0.00637426134198904\n","epoch 0, step: 117590/127656, loss: 0.00881792139261961\n","epoch 0, step: 117600/127656, loss: 0.008122555911540985\n","epoch 0, step: 117610/127656, loss: 0.0044358400627970695\n","epoch 0, step: 117620/127656, loss: 0.00597480870783329\n","epoch 0, step: 117630/127656, loss: 0.00909518450498581\n","epoch 0, step: 117640/127656, loss: 0.007952305488288403\n","epoch 0, step: 117650/127656, loss: 0.006418356206268072\n","epoch 0, step: 117660/127656, loss: 0.005655617453157902\n","epoch 0, step: 117670/127656, loss: 0.008330592885613441\n","epoch 0, step: 117680/127656, loss: 0.004869387950748205\n","epoch 0, step: 117690/127656, loss: 0.00824070069938898\n","epoch 0, step: 117700/127656, loss: 0.0076745543628931046\n","epoch 0, step: 117710/127656, loss: 0.41279906034469604\n","epoch 0, step: 117720/127656, loss: 0.007013446651399136\n","epoch 0, step: 117730/127656, loss: 0.007006495725363493\n","epoch 0, step: 117740/127656, loss: 0.008145715110003948\n","epoch 0, step: 117750/127656, loss: 0.008507169783115387\n","epoch 0, step: 117760/127656, loss: 0.007241309620440006\n","epoch 0, step: 117770/127656, loss: 0.2691524922847748\n","epoch 0, step: 117780/127656, loss: 0.010699121281504631\n","epoch 0, step: 117790/127656, loss: 0.44134968519210815\n","epoch 0, step: 117800/127656, loss: 0.006785082630813122\n","epoch 0, step: 117810/127656, loss: 0.014483815059065819\n","epoch 0, step: 117820/127656, loss: 0.006580483168363571\n","epoch 0, step: 117830/127656, loss: 0.0068053859286010265\n","epoch 0, step: 117840/127656, loss: 0.006805158220231533\n","epoch 0, step: 117850/127656, loss: 0.005928344093263149\n","epoch 0, step: 117860/127656, loss: 0.009069183841347694\n","epoch 0, step: 117870/127656, loss: 0.007691896520555019\n","epoch 0, step: 117880/127656, loss: 0.007642918732017279\n","epoch 0, step: 117890/127656, loss: 0.008712345734238625\n","epoch 0, step: 117900/127656, loss: 0.005787894129753113\n","epoch 0, step: 117910/127656, loss: 0.008270107209682465\n","epoch 0, step: 117920/127656, loss: 0.0051283459179103374\n","epoch 0, step: 117930/127656, loss: 0.006945149507373571\n","epoch 0, step: 117940/127656, loss: 0.00742532592266798\n","epoch 0, step: 117950/127656, loss: 0.009886610321700573\n","epoch 0, step: 117960/127656, loss: 0.005838116630911827\n","epoch 0, step: 117970/127656, loss: 0.006732087582349777\n","epoch 0, step: 117980/127656, loss: 0.009366326034069061\n","epoch 0, step: 117990/127656, loss: 0.008215881884098053\n","epoch 0, step: 118000/127656, loss: 0.007081364747136831\n","epoch 0, step: 118010/127656, loss: 0.013275139033794403\n","epoch 0, step: 118020/127656, loss: 0.006112881004810333\n","epoch 0, step: 118030/127656, loss: 0.009522924199700356\n","epoch 0, step: 118040/127656, loss: 0.1454307734966278\n","epoch 0, step: 118050/127656, loss: 0.010593300685286522\n","epoch 0, step: 118060/127656, loss: 0.007828577421605587\n","epoch 0, step: 118070/127656, loss: 0.018556058406829834\n","epoch 0, step: 118080/127656, loss: 0.011000944301486015\n","epoch 0, step: 118090/127656, loss: 0.010197821073234081\n","epoch 0, step: 118100/127656, loss: 0.004454740788787603\n","epoch 0, step: 118110/127656, loss: 0.557568371295929\n","epoch 0, step: 118120/127656, loss: 0.006641697138547897\n","epoch 0, step: 118130/127656, loss: 0.004938030615448952\n","epoch 0, step: 118140/127656, loss: 0.015008410438895226\n","epoch 0, step: 118150/127656, loss: 0.006939203944057226\n","epoch 0, step: 118160/127656, loss: 0.008200457319617271\n","epoch 0, step: 118170/127656, loss: 0.007447662763297558\n","epoch 0, step: 118180/127656, loss: 0.008065275847911835\n","epoch 0, step: 118190/127656, loss: 0.008758085779845715\n","epoch 0, step: 118200/127656, loss: 0.007417751010507345\n","epoch 0, step: 118210/127656, loss: 0.009424160234630108\n","epoch 0, step: 118220/127656, loss: 0.00846757274121046\n","epoch 0, step: 118230/127656, loss: 0.27459684014320374\n","epoch 0, step: 118240/127656, loss: 0.00575290760025382\n","epoch 0, step: 118250/127656, loss: 0.005516581237316132\n","epoch 0, step: 118260/127656, loss: 0.4015182852745056\n","epoch 0, step: 118270/127656, loss: 0.01156847458332777\n","epoch 0, step: 118280/127656, loss: 0.005760239437222481\n","epoch 0, step: 118290/127656, loss: 0.008235971443355083\n","epoch 0, step: 118300/127656, loss: 0.01122798677533865\n","epoch 0, step: 118310/127656, loss: 0.01342667918652296\n","epoch 0, step: 118320/127656, loss: 0.010265128687024117\n","epoch 0, step: 118330/127656, loss: 0.006991369184106588\n","epoch 0, step: 118340/127656, loss: 0.008942731656134129\n","epoch 0, step: 118350/127656, loss: 0.008363151922821999\n","epoch 0, step: 118360/127656, loss: 0.146627277135849\n","epoch 0, step: 118370/127656, loss: 0.007889891043305397\n","epoch 0, step: 118380/127656, loss: 0.005807383917272091\n","epoch 0, step: 118390/127656, loss: 0.0038810833357274532\n","epoch 0, step: 118400/127656, loss: 0.005692059174180031\n","epoch 0, step: 118410/127656, loss: 0.007878057658672333\n","epoch 0, step: 118420/127656, loss: 0.006620436441153288\n","epoch 0, step: 118430/127656, loss: 0.012257879599928856\n","epoch 0, step: 118440/127656, loss: 0.006592294201254845\n","epoch 0, step: 118450/127656, loss: 0.010823885910212994\n","epoch 0, step: 118460/127656, loss: 0.008121808990836143\n","epoch 0, step: 118470/127656, loss: 0.28073471784591675\n","epoch 0, step: 118480/127656, loss: 0.013934753835201263\n","epoch 0, step: 118490/127656, loss: 0.00554975401610136\n","epoch 0, step: 118500/127656, loss: 0.008568231016397476\n","epoch 0, step: 118510/127656, loss: 0.13185745477676392\n","epoch 0, step: 118520/127656, loss: 0.008092180825769901\n","epoch 0, step: 118530/127656, loss: 0.007518095429986715\n","epoch 0, step: 118540/127656, loss: 0.005390942096710205\n","epoch 0, step: 118550/127656, loss: 0.009172176942229271\n","epoch 0, step: 118560/127656, loss: 0.010198412463068962\n","epoch 0, step: 118570/127656, loss: 0.013971968553960323\n","epoch 0, step: 118580/127656, loss: 0.009458223357796669\n","epoch 0, step: 118590/127656, loss: 0.010116973891854286\n","epoch 0, step: 118600/127656, loss: 0.5560082793235779\n","epoch 0, step: 118610/127656, loss: 0.0059492080472409725\n","epoch 0, step: 118620/127656, loss: 0.005035204812884331\n","epoch 0, step: 118630/127656, loss: 0.015121030621230602\n","epoch 0, step: 118640/127656, loss: 0.007283850573003292\n","epoch 0, step: 118650/127656, loss: 0.011548764072358608\n","epoch 0, step: 118660/127656, loss: 0.01170445792376995\n","epoch 0, step: 118670/127656, loss: 0.006768101826310158\n","epoch 0, step: 118680/127656, loss: 0.008779716677963734\n","epoch 0, step: 118690/127656, loss: 0.008101226761937141\n","epoch 0, step: 118700/127656, loss: 0.012470867484807968\n","epoch 0, step: 118710/127656, loss: 0.005796864163130522\n","epoch 0, step: 118720/127656, loss: 0.0063767521642148495\n","epoch 0, step: 118730/127656, loss: 0.007353523280471563\n","epoch 0, step: 118740/127656, loss: 0.0076036714017391205\n","epoch 0, step: 118750/127656, loss: 0.008391276001930237\n","epoch 0, step: 118760/127656, loss: 0.008863198570907116\n","epoch 0, step: 118770/127656, loss: 0.012095927260816097\n","epoch 0, step: 118780/127656, loss: 0.008210256695747375\n","epoch 0, step: 118790/127656, loss: 0.006070077419281006\n","epoch 0, step: 118800/127656, loss: 0.006223925855010748\n","epoch 0, step: 118810/127656, loss: 0.5594691634178162\n","epoch 0, step: 118820/127656, loss: 0.006762268021702766\n","epoch 0, step: 118830/127656, loss: 0.006007678806781769\n","epoch 0, step: 118840/127656, loss: 0.00685650622472167\n","epoch 0, step: 118850/127656, loss: 0.008624362759292126\n","epoch 0, step: 118860/127656, loss: 0.012832820415496826\n","epoch 0, step: 118870/127656, loss: 0.006066364236176014\n","epoch 0, step: 118880/127656, loss: 0.008124872110784054\n","epoch 0, step: 118890/127656, loss: 0.007940511219203472\n","epoch 0, step: 118900/127656, loss: 0.005991223733872175\n","epoch 0, step: 118910/127656, loss: 0.009630130603909492\n","epoch 0, step: 118920/127656, loss: 0.012719607912003994\n","epoch 0, step: 118930/127656, loss: 0.00585488136857748\n","epoch 0, step: 118940/127656, loss: 0.5558054447174072\n","epoch 0, step: 118950/127656, loss: 0.15060584247112274\n","epoch 0, step: 118960/127656, loss: 0.007578612770885229\n","epoch 0, step: 118970/127656, loss: 0.005846032407134771\n","epoch 0, step: 118980/127656, loss: 0.008531054481863976\n","epoch 0, step: 118990/127656, loss: 0.007202629931271076\n","epoch 0, step: 119000/127656, loss: 0.010858084075152874\n","epoch 0, step: 119010/127656, loss: 0.006256233900785446\n","epoch 0, step: 119020/127656, loss: 0.006198299583047628\n","epoch 0, step: 119030/127656, loss: 0.0061172316782176495\n","epoch 0, step: 119040/127656, loss: 0.0092617841437459\n","epoch 0, step: 119050/127656, loss: 0.006559367291629314\n","epoch 0, step: 119060/127656, loss: 0.0070322416722774506\n","epoch 0, step: 119070/127656, loss: 0.009853723458945751\n","epoch 0, step: 119080/127656, loss: 0.007636047899723053\n","epoch 0, step: 119090/127656, loss: 0.010345912538468838\n","epoch 0, step: 119100/127656, loss: 0.008060375228524208\n","epoch 0, step: 119110/127656, loss: 0.00713387131690979\n","epoch 0, step: 119120/127656, loss: 0.004706679843366146\n","epoch 0, step: 119130/127656, loss: 0.007002281956374645\n","epoch 0, step: 119140/127656, loss: 0.00879492238163948\n","epoch 0, step: 119150/127656, loss: 0.00854402780532837\n","epoch 0, step: 119160/127656, loss: 0.008834205567836761\n","epoch 0, step: 119170/127656, loss: 0.401943176984787\n","epoch 0, step: 119180/127656, loss: 0.011479939334094524\n","epoch 0, step: 119190/127656, loss: 0.0062881773337721825\n","epoch 0, step: 119200/127656, loss: 0.011340206488966942\n","epoch 0, step: 119210/127656, loss: 0.011370956897735596\n","epoch 0, step: 119220/127656, loss: 0.00858800858259201\n","epoch 0, step: 119230/127656, loss: 0.011603424325585365\n","epoch 0, step: 119240/127656, loss: 0.008311552926898003\n","epoch 0, step: 119250/127656, loss: 0.011014211922883987\n","epoch 0, step: 119260/127656, loss: 0.0072484975680708885\n","epoch 0, step: 119270/127656, loss: 0.2618473172187805\n","epoch 0, step: 119280/127656, loss: 0.009863311424851418\n","epoch 0, step: 119290/127656, loss: 0.13259968161582947\n","epoch 0, step: 119300/127656, loss: 0.009304235689342022\n","epoch 0, step: 119310/127656, loss: 0.006630311720073223\n","epoch 0, step: 119320/127656, loss: 0.007208499126136303\n","epoch 0, step: 119330/127656, loss: 0.005113670602440834\n","epoch 0, step: 119340/127656, loss: 0.009656639769673347\n","epoch 0, step: 119350/127656, loss: 0.01355578750371933\n","epoch 0, step: 119360/127656, loss: 0.1507912427186966\n","epoch 0, step: 119370/127656, loss: 0.008079549297690392\n","epoch 0, step: 119380/127656, loss: 0.006733185611665249\n","epoch 0, step: 119390/127656, loss: 0.007465565111488104\n","epoch 0, step: 119400/127656, loss: 0.010398369282484055\n","epoch 0, step: 119410/127656, loss: 0.006599294953048229\n","epoch 0, step: 119420/127656, loss: 0.14627540111541748\n","epoch 0, step: 119430/127656, loss: 0.010080110281705856\n","epoch 0, step: 119440/127656, loss: 0.0049428571946918964\n","epoch 0, step: 119450/127656, loss: 0.00482858344912529\n","epoch 0, step: 119460/127656, loss: 0.005939572583884001\n","epoch 0, step: 119470/127656, loss: 0.016820421442389488\n","epoch 0, step: 119480/127656, loss: 0.008325805887579918\n","epoch 0, step: 119490/127656, loss: 0.008140220306813717\n","epoch 0, step: 119500/127656, loss: 0.004159471020102501\n","epoch 0, step: 119510/127656, loss: 0.009744048118591309\n","epoch 0, step: 119520/127656, loss: 0.14371640980243683\n","epoch 0, step: 119530/127656, loss: 0.008838747628033161\n","epoch 0, step: 119540/127656, loss: 0.006147640757262707\n","epoch 0, step: 119550/127656, loss: 0.007627950981259346\n","epoch 0, step: 119560/127656, loss: 0.007171971257776022\n","epoch 0, step: 119570/127656, loss: 0.007897745817899704\n","epoch 0, step: 119580/127656, loss: 0.013215724378824234\n","epoch 0, step: 119590/127656, loss: 0.010085239075124264\n","epoch 0, step: 119600/127656, loss: 0.013581844046711922\n","epoch 0, step: 119610/127656, loss: 0.005886831786483526\n","epoch 0, step: 119620/127656, loss: 0.009261460974812508\n","epoch 0, step: 119630/127656, loss: 0.007292497903108597\n","epoch 0, step: 119640/127656, loss: 0.5527678728103638\n","epoch 0, step: 119650/127656, loss: 0.004705382511019707\n","epoch 0, step: 119660/127656, loss: 0.43316173553466797\n","epoch 0, step: 119670/127656, loss: 0.004509258083999157\n","epoch 0, step: 119680/127656, loss: 0.005022028461098671\n","epoch 0, step: 119690/127656, loss: 0.005270801484584808\n","epoch 0, step: 119700/127656, loss: 0.010974312201142311\n","epoch 0, step: 119710/127656, loss: 0.01146240159869194\n","epoch 0, step: 119720/127656, loss: 0.0037132275756448507\n","epoch 0, step: 119730/127656, loss: 0.00690182950347662\n","epoch 0, step: 119740/127656, loss: 0.28942060470581055\n","epoch 0, step: 119750/127656, loss: 0.008915763348340988\n","epoch 0, step: 119760/127656, loss: 0.006776976864784956\n","epoch 0, step: 119770/127656, loss: 0.009020703844726086\n","epoch 0, step: 119780/127656, loss: 0.008090222254395485\n","epoch 0, step: 119790/127656, loss: 0.014778426848351955\n","epoch 0, step: 119800/127656, loss: 0.1424490064382553\n","epoch 0, step: 119810/127656, loss: 0.13622522354125977\n","epoch 0, step: 119820/127656, loss: 0.005969011690467596\n","epoch 0, step: 119830/127656, loss: 0.2713370621204376\n","epoch 0, step: 119840/127656, loss: 0.006801017560064793\n","epoch 0, step: 119850/127656, loss: 0.011561261489987373\n","epoch 0, step: 119860/127656, loss: 0.005484605208039284\n","epoch 0, step: 119870/127656, loss: 0.007484134286642075\n","epoch 0, step: 119880/127656, loss: 0.00494038499891758\n","epoch 0, step: 119890/127656, loss: 0.009181763976812363\n","epoch 0, step: 119900/127656, loss: 0.007368152029812336\n","epoch 0, step: 119910/127656, loss: 0.008925927802920341\n","epoch 0, step: 119920/127656, loss: 0.01113987248390913\n","epoch 0, step: 119930/127656, loss: 0.007789276074618101\n","epoch 0, step: 119940/127656, loss: 0.00465285824611783\n","epoch 0, step: 119950/127656, loss: 0.0073508620262146\n","epoch 0, step: 119960/127656, loss: 0.008924691006541252\n","epoch 0, step: 119970/127656, loss: 0.00659660529345274\n","epoch 0, step: 119980/127656, loss: 0.007699096109718084\n","epoch 0, step: 119990/127656, loss: 0.006910708732903004\n","epoch 0, step: 120000/127656, loss: 0.006886755581945181\n","epoch 0, step: 120010/127656, loss: 0.00904105044901371\n","epoch 0, step: 120020/127656, loss: 0.006867223884910345\n","epoch 0, step: 120030/127656, loss: 0.006527467630803585\n","epoch 0, step: 120040/127656, loss: 0.007017610594630241\n","epoch 0, step: 120050/127656, loss: 0.008248349651694298\n","epoch 0, step: 120060/127656, loss: 0.012034650892019272\n","epoch 0, step: 120070/127656, loss: 0.00545788649469614\n","epoch 0, step: 120080/127656, loss: 0.005788774695247412\n","epoch 0, step: 120090/127656, loss: 0.007428336888551712\n","epoch 0, step: 120100/127656, loss: 0.006473511457443237\n","epoch 0, step: 120110/127656, loss: 0.0066920253448188305\n","epoch 0, step: 120120/127656, loss: 0.008320624008774757\n","epoch 0, step: 120130/127656, loss: 0.005624844692647457\n","epoch 0, step: 120140/127656, loss: 0.2794116735458374\n","epoch 0, step: 120150/127656, loss: 0.008325587958097458\n","epoch 0, step: 120160/127656, loss: 0.011638223193585873\n","epoch 0, step: 120170/127656, loss: 0.010025075636804104\n","epoch 0, step: 120180/127656, loss: 0.007215722464025021\n","epoch 0, step: 120190/127656, loss: 0.00616849958896637\n","epoch 0, step: 120200/127656, loss: 0.005411427468061447\n","epoch 0, step: 120210/127656, loss: 0.006062205880880356\n","epoch 0, step: 120220/127656, loss: 0.009221634827554226\n","epoch 0, step: 120230/127656, loss: 0.00659465417265892\n","epoch 0, step: 120240/127656, loss: 0.008060398511588573\n","epoch 0, step: 120250/127656, loss: 0.008571064099669456\n","epoch 0, step: 120260/127656, loss: 0.005038911011070013\n","epoch 0, step: 120270/127656, loss: 0.009186368435621262\n","epoch 0, step: 120280/127656, loss: 0.008284192532300949\n","epoch 0, step: 120290/127656, loss: 0.008194146677851677\n","epoch 0, step: 120300/127656, loss: 0.013197745196521282\n","epoch 0, step: 120310/127656, loss: 0.005645600613206625\n","epoch 0, step: 120320/127656, loss: 0.007955178618431091\n","epoch 0, step: 120330/127656, loss: 0.005571786314249039\n","epoch 0, step: 120340/127656, loss: 0.008378993719816208\n","epoch 0, step: 120350/127656, loss: 0.006060553248971701\n","epoch 0, step: 120360/127656, loss: 0.007583766244351864\n","epoch 0, step: 120370/127656, loss: 0.004026306327432394\n","epoch 0, step: 120380/127656, loss: 0.007582000456750393\n","epoch 0, step: 120390/127656, loss: 0.008529998362064362\n","epoch 0, step: 120400/127656, loss: 0.00892658531665802\n","epoch 0, step: 120410/127656, loss: 0.004261534195393324\n","epoch 0, step: 120420/127656, loss: 0.0069971876218914986\n","epoch 0, step: 120430/127656, loss: 0.013263239525258541\n","epoch 0, step: 120440/127656, loss: 0.008284823969006538\n","epoch 0, step: 120450/127656, loss: 0.004903953056782484\n","epoch 0, step: 120460/127656, loss: 0.010668891482055187\n","epoch 0, step: 120470/127656, loss: 0.007872411981225014\n","epoch 0, step: 120480/127656, loss: 0.008444724604487419\n","epoch 0, step: 120490/127656, loss: 0.009066681377589703\n","epoch 0, step: 120500/127656, loss: 0.006121758371591568\n","epoch 0, step: 120510/127656, loss: 0.00523398956283927\n","epoch 0, step: 120520/127656, loss: 0.012123054824769497\n","epoch 0, step: 120530/127656, loss: 0.006371553987264633\n","epoch 0, step: 120540/127656, loss: 0.010007346980273724\n","epoch 0, step: 120550/127656, loss: 0.007115907035768032\n","epoch 0, step: 120560/127656, loss: 0.005662630312144756\n","epoch 0, step: 120570/127656, loss: 0.013833776116371155\n","epoch 0, step: 120580/127656, loss: 0.00905854906886816\n","epoch 0, step: 120590/127656, loss: 0.020754102617502213\n","epoch 0, step: 120600/127656, loss: 0.008399940095841885\n","epoch 0, step: 120610/127656, loss: 0.006935031618922949\n","epoch 0, step: 120620/127656, loss: 0.007486602291464806\n","epoch 0, step: 120630/127656, loss: 0.004473811946809292\n","epoch 0, step: 120640/127656, loss: 0.006104500964283943\n","epoch 0, step: 120650/127656, loss: 0.01615644246339798\n","epoch 0, step: 120660/127656, loss: 0.011616654694080353\n","epoch 0, step: 120670/127656, loss: 0.007771145552396774\n","epoch 0, step: 120680/127656, loss: 0.010154258459806442\n","epoch 0, step: 120690/127656, loss: 0.007449496537446976\n","epoch 0, step: 120700/127656, loss: 0.005694573279470205\n","epoch 0, step: 120710/127656, loss: 0.009068509563803673\n","epoch 0, step: 120720/127656, loss: 0.00738538196310401\n","epoch 0, step: 120730/127656, loss: 0.008710818365216255\n","epoch 0, step: 120740/127656, loss: 0.005555774085223675\n","epoch 0, step: 120750/127656, loss: 0.00864451751112938\n","epoch 0, step: 120760/127656, loss: 0.008077619597315788\n","epoch 0, step: 120770/127656, loss: 0.01068118866533041\n","epoch 0, step: 120780/127656, loss: 0.005192636512219906\n","epoch 0, step: 120790/127656, loss: 0.007096037268638611\n","epoch 0, step: 120800/127656, loss: 0.0051766689866781235\n","epoch 0, step: 120810/127656, loss: 0.005904762074351311\n","epoch 0, step: 120820/127656, loss: 0.00825691968202591\n","epoch 0, step: 120830/127656, loss: 0.005918486975133419\n","epoch 0, step: 120840/127656, loss: 0.004646641667932272\n","epoch 0, step: 120850/127656, loss: 0.008343968540430069\n","epoch 0, step: 120860/127656, loss: 0.01200413703918457\n","epoch 0, step: 120870/127656, loss: 0.00874282605946064\n","epoch 0, step: 120880/127656, loss: 0.005043061450123787\n","epoch 0, step: 120890/127656, loss: 0.005683989729732275\n","epoch 0, step: 120900/127656, loss: 0.010892740450799465\n","epoch 0, step: 120910/127656, loss: 0.009230691939592361\n","epoch 0, step: 120920/127656, loss: 0.005357054527848959\n","epoch 0, step: 120930/127656, loss: 0.007442023605108261\n","epoch 0, step: 120940/127656, loss: 0.007383608724921942\n","epoch 0, step: 120950/127656, loss: 0.005306174512952566\n","epoch 0, step: 120960/127656, loss: 0.006784599274396896\n","epoch 0, step: 120970/127656, loss: 0.007485720794647932\n","epoch 0, step: 120980/127656, loss: 0.011294099502265453\n","epoch 0, step: 120990/127656, loss: 0.008091110736131668\n","epoch 0, step: 121000/127656, loss: 0.006530062295496464\n","epoch 0, step: 121010/127656, loss: 0.00663464330136776\n","epoch 0, step: 121020/127656, loss: 0.0058939699083566666\n","epoch 0, step: 121030/127656, loss: 0.008727427572011948\n","epoch 0, step: 121040/127656, loss: 0.01086331531405449\n","epoch 0, step: 121050/127656, loss: 0.006243843585252762\n","epoch 0, step: 121060/127656, loss: 0.008351748809218407\n","epoch 0, step: 121070/127656, loss: 0.0054459841921925545\n","epoch 0, step: 121080/127656, loss: 0.009774970822036266\n","epoch 0, step: 121090/127656, loss: 0.009006001055240631\n","epoch 0, step: 121100/127656, loss: 0.14282244443893433\n","epoch 0, step: 121110/127656, loss: 0.009405422955751419\n","epoch 0, step: 121120/127656, loss: 0.006171332206577063\n","epoch 0, step: 121130/127656, loss: 0.006306930910795927\n","epoch 0, step: 121140/127656, loss: 0.00806228257715702\n","epoch 0, step: 121150/127656, loss: 0.008971374481916428\n","epoch 0, step: 121160/127656, loss: 0.14272856712341309\n","epoch 0, step: 121170/127656, loss: 0.007708835881203413\n","epoch 0, step: 121180/127656, loss: 0.004739274736493826\n","epoch 0, step: 121190/127656, loss: 0.006877554580569267\n","epoch 0, step: 121200/127656, loss: 0.005625396501272917\n","epoch 0, step: 121210/127656, loss: 0.009413992054760456\n","epoch 0, step: 121220/127656, loss: 0.005391458980739117\n","epoch 0, step: 121230/127656, loss: 0.004517978057265282\n","epoch 0, step: 121240/127656, loss: 0.007154114078730345\n","epoch 0, step: 121250/127656, loss: 0.007644414436072111\n","epoch 0, step: 121260/127656, loss: 0.4036499857902527\n","epoch 0, step: 121270/127656, loss: 0.004640813916921616\n","epoch 0, step: 121280/127656, loss: 0.009856641292572021\n","epoch 0, step: 121290/127656, loss: 0.008295895531773567\n","epoch 0, step: 121300/127656, loss: 0.008964873850345612\n","epoch 0, step: 121310/127656, loss: 0.007321764715015888\n","epoch 0, step: 121320/127656, loss: 0.009813687764108181\n","epoch 0, step: 121330/127656, loss: 0.008877738378942013\n","epoch 0, step: 121340/127656, loss: 0.0061687263660132885\n","epoch 0, step: 121350/127656, loss: 0.008332308381795883\n","epoch 0, step: 121360/127656, loss: 0.13527704775333405\n","epoch 0, step: 121370/127656, loss: 0.008149533532559872\n","epoch 0, step: 121380/127656, loss: 0.004389852751046419\n","epoch 0, step: 121390/127656, loss: 0.005586148705333471\n","epoch 0, step: 121400/127656, loss: 0.00973590463399887\n","epoch 0, step: 121410/127656, loss: 0.010846460238099098\n","epoch 0, step: 121420/127656, loss: 0.006064422428607941\n","epoch 0, step: 121430/127656, loss: 0.0073658255860209465\n","epoch 0, step: 121440/127656, loss: 0.011113902553915977\n","epoch 0, step: 121450/127656, loss: 0.007709129713475704\n","epoch 0, step: 121460/127656, loss: 0.005588384345173836\n","epoch 0, step: 121470/127656, loss: 0.006371706258505583\n","epoch 0, step: 121480/127656, loss: 0.004005593713372946\n","epoch 0, step: 121490/127656, loss: 0.003965751267969608\n","epoch 0, step: 121500/127656, loss: 0.42896437644958496\n","epoch 0, step: 121510/127656, loss: 0.00878487154841423\n","epoch 0, step: 121520/127656, loss: 0.00824022851884365\n","epoch 0, step: 121530/127656, loss: 0.010583125054836273\n","epoch 0, step: 121540/127656, loss: 0.005591951310634613\n","epoch 0, step: 121550/127656, loss: 0.012714153155684471\n","epoch 0, step: 121560/127656, loss: 0.009402211755514145\n","epoch 0, step: 121570/127656, loss: 0.00506163202226162\n","epoch 0, step: 121580/127656, loss: 0.009430316276848316\n","epoch 0, step: 121590/127656, loss: 0.0070061832666397095\n","epoch 0, step: 121600/127656, loss: 0.005886716302484274\n","epoch 0, step: 121610/127656, loss: 0.006462027784436941\n","epoch 0, step: 121620/127656, loss: 0.13323235511779785\n","epoch 0, step: 121630/127656, loss: 0.010280825197696686\n","epoch 0, step: 121640/127656, loss: 0.5451028943061829\n","epoch 0, step: 121650/127656, loss: 0.005304399877786636\n","epoch 0, step: 121660/127656, loss: 0.003936626482754946\n","epoch 0, step: 121670/127656, loss: 0.00904308445751667\n","epoch 0, step: 121680/127656, loss: 0.004805785603821278\n","epoch 0, step: 121690/127656, loss: 0.012661822140216827\n","epoch 0, step: 121700/127656, loss: 0.010483033955097198\n","epoch 0, step: 121710/127656, loss: 0.005331682972609997\n","epoch 0, step: 121720/127656, loss: 0.007205660920590162\n","epoch 0, step: 121730/127656, loss: 0.00453175650909543\n","epoch 0, step: 121740/127656, loss: 0.01681535691022873\n","epoch 0, step: 121750/127656, loss: 0.012374705635011196\n","epoch 0, step: 121760/127656, loss: 0.009720303118228912\n","epoch 0, step: 121770/127656, loss: 0.008687112480401993\n","epoch 0, step: 121780/127656, loss: 0.0053850207477808\n","epoch 0, step: 121790/127656, loss: 0.4358604848384857\n","epoch 0, step: 121800/127656, loss: 0.005375740118324757\n","epoch 0, step: 121810/127656, loss: 0.007663312833756208\n","epoch 0, step: 121820/127656, loss: 0.006207985803484917\n","epoch 0, step: 121830/127656, loss: 0.00504485284909606\n","epoch 0, step: 121840/127656, loss: 0.008334774523973465\n","epoch 0, step: 121850/127656, loss: 0.006181917153298855\n","epoch 0, step: 121860/127656, loss: 0.2885124683380127\n","epoch 0, step: 121870/127656, loss: 0.007684232667088509\n","epoch 0, step: 121880/127656, loss: 0.009477805346250534\n","epoch 0, step: 121890/127656, loss: 0.004852251149713993\n","epoch 0, step: 121900/127656, loss: 0.014212889596819878\n","epoch 0, step: 121910/127656, loss: 0.007923288270831108\n","epoch 0, step: 121920/127656, loss: 0.013507853262126446\n","epoch 0, step: 121930/127656, loss: 0.011608844622969627\n","epoch 0, step: 121940/127656, loss: 0.007907259277999401\n","epoch 0, step: 121950/127656, loss: 0.006099428050220013\n","epoch 0, step: 121960/127656, loss: 0.006911498960107565\n","epoch 0, step: 121970/127656, loss: 0.009535200893878937\n","epoch 0, step: 121980/127656, loss: 0.006609317846596241\n","epoch 0, step: 121990/127656, loss: 0.5520721673965454\n","epoch 0, step: 122000/127656, loss: 0.007989400997757912\n","epoch 0, step: 122010/127656, loss: 0.7206203937530518\n","epoch 0, step: 122020/127656, loss: 0.007219608873128891\n","epoch 0, step: 122030/127656, loss: 0.006310425698757172\n","epoch 0, step: 122040/127656, loss: 0.007758104708045721\n","epoch 0, step: 122050/127656, loss: 0.004950585775077343\n","epoch 0, step: 122060/127656, loss: 0.008052990771830082\n","epoch 0, step: 122070/127656, loss: 0.1459459662437439\n","epoch 0, step: 122080/127656, loss: 0.004671235103160143\n","epoch 0, step: 122090/127656, loss: 0.005155730526894331\n","epoch 0, step: 122100/127656, loss: 0.006261640228331089\n","epoch 0, step: 122110/127656, loss: 0.12506523728370667\n","epoch 0, step: 122120/127656, loss: 0.004411792382597923\n","epoch 0, step: 122130/127656, loss: 0.0069504003040492535\n","epoch 0, step: 122140/127656, loss: 0.008065793663263321\n","epoch 0, step: 122150/127656, loss: 0.006710548885166645\n","epoch 0, step: 122160/127656, loss: 0.41152554750442505\n","epoch 0, step: 122170/127656, loss: 0.008775941096246243\n","epoch 0, step: 122180/127656, loss: 0.008832881227135658\n","epoch 0, step: 122190/127656, loss: 0.006132050417363644\n","epoch 0, step: 122200/127656, loss: 0.007587939500808716\n","epoch 0, step: 122210/127656, loss: 0.28070858120918274\n","epoch 0, step: 122220/127656, loss: 0.0064659095369279385\n","epoch 0, step: 122230/127656, loss: 0.13591930270195007\n","epoch 0, step: 122240/127656, loss: 0.0075235506519675255\n","epoch 0, step: 122250/127656, loss: 0.00590225774794817\n","epoch 0, step: 122260/127656, loss: 0.006922508589923382\n","epoch 0, step: 122270/127656, loss: 0.005266815423965454\n","epoch 0, step: 122280/127656, loss: 0.007891288958489895\n","epoch 0, step: 122290/127656, loss: 0.0111471489071846\n","epoch 0, step: 122300/127656, loss: 0.005796746350824833\n","epoch 0, step: 122310/127656, loss: 0.007283557206392288\n","epoch 0, step: 122320/127656, loss: 0.005547221750020981\n","epoch 0, step: 122330/127656, loss: 0.010417303070425987\n","epoch 0, step: 122340/127656, loss: 0.005008191801607609\n","epoch 0, step: 122350/127656, loss: 0.007902519777417183\n","epoch 0, step: 122360/127656, loss: 0.006541473791003227\n","epoch 0, step: 122370/127656, loss: 0.007674196269363165\n","epoch 0, step: 122380/127656, loss: 0.006260372698307037\n","epoch 0, step: 122390/127656, loss: 0.0069229453802108765\n","epoch 0, step: 122400/127656, loss: 0.006402703933417797\n","epoch 0, step: 122410/127656, loss: 0.0060145738534629345\n","epoch 0, step: 122420/127656, loss: 0.01135275699198246\n","epoch 0, step: 122430/127656, loss: 0.41908854246139526\n","epoch 0, step: 122440/127656, loss: 0.006985391490161419\n","epoch 0, step: 122450/127656, loss: 0.01034490391612053\n","epoch 0, step: 122460/127656, loss: 0.005874658934772015\n","epoch 0, step: 122470/127656, loss: 0.004426662810146809\n","epoch 0, step: 122480/127656, loss: 0.003877778071910143\n","epoch 0, step: 122490/127656, loss: 0.010179271921515465\n","epoch 0, step: 122500/127656, loss: 0.00738164409995079\n","epoch 0, step: 122510/127656, loss: 0.15031462907791138\n","epoch 0, step: 122520/127656, loss: 0.00876526441425085\n","epoch 0, step: 122530/127656, loss: 0.009132900275290012\n","epoch 0, step: 122540/127656, loss: 0.007522835396230221\n","epoch 0, step: 122550/127656, loss: 0.12741726636886597\n","epoch 0, step: 122560/127656, loss: 0.009956158697605133\n","epoch 0, step: 122570/127656, loss: 0.01047157309949398\n","epoch 0, step: 122580/127656, loss: 0.5270116329193115\n","epoch 0, step: 122590/127656, loss: 0.0073960283771157265\n","epoch 0, step: 122600/127656, loss: 0.009096290916204453\n","epoch 0, step: 122610/127656, loss: 0.008466798812150955\n","epoch 0, step: 122620/127656, loss: 0.010775613598525524\n","epoch 0, step: 122630/127656, loss: 0.007635356392711401\n","epoch 0, step: 122640/127656, loss: 0.00737000210210681\n","epoch 0, step: 122650/127656, loss: 0.004492445848882198\n","epoch 0, step: 122660/127656, loss: 0.005801328457891941\n","epoch 0, step: 122670/127656, loss: 0.1270732879638672\n","epoch 0, step: 122680/127656, loss: 0.0068474444560706615\n","epoch 0, step: 122690/127656, loss: 0.006684043910354376\n","epoch 0, step: 122700/127656, loss: 0.26618456840515137\n","epoch 0, step: 122710/127656, loss: 0.004287632182240486\n","epoch 0, step: 122720/127656, loss: 0.009267866611480713\n","epoch 0, step: 122730/127656, loss: 0.006746262311935425\n","epoch 0, step: 122740/127656, loss: 0.41923367977142334\n","epoch 0, step: 122750/127656, loss: 0.433004230260849\n","epoch 0, step: 122760/127656, loss: 0.007276596035808325\n","epoch 0, step: 122770/127656, loss: 0.27915531396865845\n","epoch 0, step: 122780/127656, loss: 0.009251847863197327\n","epoch 0, step: 122790/127656, loss: 0.010500757023692131\n","epoch 0, step: 122800/127656, loss: 0.14376303553581238\n","epoch 0, step: 122810/127656, loss: 0.005314575508236885\n","epoch 0, step: 122820/127656, loss: 0.008326378650963306\n","epoch 0, step: 122830/127656, loss: 0.0057649556547403336\n","epoch 0, step: 122840/127656, loss: 0.01313747651875019\n","epoch 0, step: 122850/127656, loss: 0.428910493850708\n","epoch 0, step: 122860/127656, loss: 0.006595812737941742\n","epoch 0, step: 122870/127656, loss: 0.006237374152988195\n","epoch 0, step: 122880/127656, loss: 0.008465046063065529\n","epoch 0, step: 122890/127656, loss: 0.0056951604783535\n","epoch 0, step: 122900/127656, loss: 0.005434557795524597\n","epoch 0, step: 122910/127656, loss: 0.0077460757456719875\n","epoch 0, step: 122920/127656, loss: 0.011863037943840027\n","epoch 0, step: 122930/127656, loss: 0.007484142668545246\n","epoch 0, step: 122940/127656, loss: 0.0067278980277478695\n","epoch 0, step: 122950/127656, loss: 0.009215699508786201\n","epoch 0, step: 122960/127656, loss: 0.007721887435764074\n","epoch 0, step: 122970/127656, loss: 0.008031051605939865\n","epoch 0, step: 122980/127656, loss: 0.0071038249880075455\n","epoch 0, step: 122990/127656, loss: 0.008051438257098198\n","epoch 0, step: 123000/127656, loss: 0.008146899752318859\n","epoch 0, step: 123010/127656, loss: 0.007947638630867004\n","epoch 0, step: 123020/127656, loss: 0.004512871615588665\n","epoch 0, step: 123030/127656, loss: 0.004989044740796089\n","epoch 0, step: 123040/127656, loss: 0.00885801948606968\n","epoch 0, step: 123050/127656, loss: 0.5648425817489624\n","epoch 0, step: 123060/127656, loss: 0.009895901195704937\n","epoch 0, step: 123070/127656, loss: 0.0074975332245230675\n","epoch 0, step: 123080/127656, loss: 0.007709057070314884\n","epoch 0, step: 123090/127656, loss: 0.007356013171374798\n","epoch 0, step: 123100/127656, loss: 0.012861634604632854\n","epoch 0, step: 123110/127656, loss: 0.015442187897861004\n","epoch 0, step: 123120/127656, loss: 0.006689862813800573\n","epoch 0, step: 123130/127656, loss: 0.00815322995185852\n","epoch 0, step: 123140/127656, loss: 0.0056701041758060455\n","epoch 0, step: 123150/127656, loss: 0.004980913363397121\n","epoch 0, step: 123160/127656, loss: 0.005358225665986538\n","epoch 0, step: 123170/127656, loss: 0.13723474740982056\n","epoch 0, step: 123180/127656, loss: 0.008708946406841278\n","epoch 0, step: 123190/127656, loss: 0.007191125303506851\n","epoch 0, step: 123200/127656, loss: 0.006700200494378805\n","epoch 0, step: 123210/127656, loss: 0.006753413937985897\n","epoch 0, step: 123220/127656, loss: 0.005672336556017399\n","epoch 0, step: 123230/127656, loss: 0.0037345956079661846\n","epoch 0, step: 123240/127656, loss: 0.007614159490913153\n","epoch 0, step: 123250/127656, loss: 0.010069026611745358\n","epoch 0, step: 123260/127656, loss: 0.006986585911363363\n","epoch 0, step: 123270/127656, loss: 0.0053084492683410645\n","epoch 0, step: 123280/127656, loss: 0.009488124400377274\n","epoch 0, step: 123290/127656, loss: 0.5831855535507202\n","epoch 0, step: 123300/127656, loss: 0.01283952035009861\n","epoch 0, step: 123310/127656, loss: 0.008856012485921383\n","epoch 0, step: 123320/127656, loss: 0.010118590667843819\n","epoch 0, step: 123330/127656, loss: 0.008977889083325863\n","epoch 0, step: 123340/127656, loss: 0.27524691820144653\n","epoch 0, step: 123350/127656, loss: 0.409926176071167\n","epoch 0, step: 123360/127656, loss: 0.007084024604409933\n","epoch 0, step: 123370/127656, loss: 0.007866401225328445\n","epoch 0, step: 123380/127656, loss: 0.0075387535616755486\n","epoch 0, step: 123390/127656, loss: 0.009969105944037437\n","epoch 0, step: 123400/127656, loss: 0.0103421276435256\n","epoch 0, step: 123410/127656, loss: 0.005682629533112049\n","epoch 0, step: 123420/127656, loss: 0.00563286617398262\n","epoch 0, step: 123430/127656, loss: 0.006650666706264019\n","epoch 0, step: 123440/127656, loss: 0.005831183400005102\n","epoch 0, step: 123450/127656, loss: 0.012135805562138557\n","epoch 0, step: 123460/127656, loss: 0.011704914271831512\n","epoch 0, step: 123470/127656, loss: 0.007099539041519165\n","epoch 0, step: 123480/127656, loss: 0.006584160961210728\n","epoch 0, step: 123490/127656, loss: 0.011651411652565002\n","epoch 0, step: 123500/127656, loss: 0.009287229739129543\n","epoch 0, step: 123510/127656, loss: 0.010018570348620415\n","epoch 0, step: 123520/127656, loss: 0.4104689061641693\n","epoch 0, step: 123530/127656, loss: 0.01086224615573883\n","epoch 0, step: 123540/127656, loss: 0.010339399799704552\n","epoch 0, step: 123550/127656, loss: 0.006648301612585783\n","epoch 0, step: 123560/127656, loss: 0.0071720159612596035\n","epoch 0, step: 123570/127656, loss: 0.008964493870735168\n","epoch 0, step: 123580/127656, loss: 0.005132472142577171\n","epoch 0, step: 123590/127656, loss: 0.00840878114104271\n","epoch 0, step: 123600/127656, loss: 0.011212344281375408\n","epoch 0, step: 123610/127656, loss: 0.141330286860466\n","epoch 0, step: 123620/127656, loss: 0.010973244905471802\n","epoch 0, step: 123630/127656, loss: 0.004014470614492893\n","epoch 0, step: 123640/127656, loss: 0.004742042627185583\n","epoch 0, step: 123650/127656, loss: 0.009564273059368134\n","epoch 0, step: 123660/127656, loss: 0.009929648600518703\n","epoch 0, step: 123670/127656, loss: 0.010365632362663746\n","epoch 0, step: 123680/127656, loss: 0.00578034482896328\n","epoch 0, step: 123690/127656, loss: 0.00473996764048934\n","epoch 0, step: 123700/127656, loss: 0.007183525245636702\n","epoch 0, step: 123710/127656, loss: 0.007031670771539211\n","epoch 0, step: 123720/127656, loss: 0.008417600765824318\n","epoch 0, step: 123730/127656, loss: 0.005752156488597393\n","epoch 0, step: 123740/127656, loss: 0.01005320530384779\n","epoch 0, step: 123750/127656, loss: 0.007910529151558876\n","epoch 0, step: 123760/127656, loss: 0.0056718699634075165\n","epoch 0, step: 123770/127656, loss: 0.006417468190193176\n","epoch 0, step: 123780/127656, loss: 0.01075016986578703\n","epoch 0, step: 123790/127656, loss: 0.006749730557203293\n","epoch 0, step: 123800/127656, loss: 0.00923212245106697\n","epoch 0, step: 123810/127656, loss: 0.01071549765765667\n","epoch 0, step: 123820/127656, loss: 0.2672654092311859\n","epoch 0, step: 123830/127656, loss: 0.14494310319423676\n","epoch 0, step: 123840/127656, loss: 0.006463346537202597\n","epoch 0, step: 123850/127656, loss: 0.13888107240200043\n","epoch 0, step: 123860/127656, loss: 0.004982632584869862\n","epoch 0, step: 123870/127656, loss: 0.010407758876681328\n","epoch 0, step: 123880/127656, loss: 0.007186010479927063\n","epoch 0, step: 123890/127656, loss: 0.011142833158373833\n","epoch 0, step: 123900/127656, loss: 0.006563910748809576\n","epoch 0, step: 123910/127656, loss: 0.005493118427693844\n","epoch 0, step: 123920/127656, loss: 0.006502827163785696\n","epoch 0, step: 123930/127656, loss: 0.00744986068457365\n","epoch 0, step: 123940/127656, loss: 0.008441334590315819\n","epoch 0, step: 123950/127656, loss: 0.005549568682909012\n","epoch 0, step: 123960/127656, loss: 0.005401398055255413\n","epoch 0, step: 123970/127656, loss: 0.009271465241909027\n","epoch 0, step: 123980/127656, loss: 0.007523529231548309\n","epoch 0, step: 123990/127656, loss: 0.005325695034116507\n","epoch 0, step: 124000/127656, loss: 0.004140723031014204\n","epoch 0, step: 124010/127656, loss: 0.008759486488997936\n","epoch 0, step: 124020/127656, loss: 0.004334266297519207\n","epoch 0, step: 124030/127656, loss: 0.005234476178884506\n","epoch 0, step: 124040/127656, loss: 0.009162492118775845\n","epoch 0, step: 124050/127656, loss: 0.011984344571828842\n","epoch 0, step: 124060/127656, loss: 0.00646690558642149\n","epoch 0, step: 124070/127656, loss: 0.00865806546062231\n","epoch 0, step: 124080/127656, loss: 0.007705772295594215\n","epoch 0, step: 124090/127656, loss: 0.00478264270350337\n","epoch 0, step: 124100/127656, loss: 0.010547875426709652\n","epoch 0, step: 124110/127656, loss: 0.008040880784392357\n","epoch 0, step: 124120/127656, loss: 0.00863269716501236\n","epoch 0, step: 124130/127656, loss: 0.005969101097434759\n","epoch 0, step: 124140/127656, loss: 0.0054780482314527035\n","epoch 0, step: 124150/127656, loss: 0.005626744590699673\n","epoch 0, step: 124160/127656, loss: 0.004723267164081335\n","epoch 0, step: 124170/127656, loss: 0.007737508974969387\n","epoch 0, step: 124180/127656, loss: 0.13710913062095642\n","epoch 0, step: 124190/127656, loss: 0.007706987671554089\n","epoch 0, step: 124200/127656, loss: 0.007096910383552313\n","epoch 0, step: 124210/127656, loss: 0.0063943504355847836\n","epoch 0, step: 124220/127656, loss: 0.006997144781053066\n","epoch 0, step: 124230/127656, loss: 0.006810171063989401\n","epoch 0, step: 124240/127656, loss: 0.006174481939524412\n","epoch 0, step: 124250/127656, loss: 0.008752292022109032\n","epoch 0, step: 124260/127656, loss: 0.007985645905137062\n","epoch 0, step: 124270/127656, loss: 0.28108471632003784\n","epoch 0, step: 124280/127656, loss: 0.007166845258325338\n","epoch 0, step: 124290/127656, loss: 0.005797871388494968\n","epoch 0, step: 124300/127656, loss: 0.0034788046032190323\n","epoch 0, step: 124310/127656, loss: 0.006880701519548893\n","epoch 0, step: 124320/127656, loss: 0.008475938811898232\n","epoch 0, step: 124330/127656, loss: 0.008105912245810032\n","epoch 0, step: 124340/127656, loss: 0.007128949277102947\n","epoch 0, step: 124350/127656, loss: 0.007002475671470165\n","epoch 0, step: 124360/127656, loss: 0.01074952632188797\n","epoch 0, step: 124370/127656, loss: 0.008270209655165672\n","epoch 0, step: 124380/127656, loss: 0.013295980170369148\n","epoch 0, step: 124390/127656, loss: 0.007307000458240509\n","epoch 0, step: 124400/127656, loss: 0.1432076096534729\n","epoch 0, step: 124410/127656, loss: 0.009131553582847118\n","epoch 0, step: 124420/127656, loss: 0.5578093528747559\n","epoch 0, step: 124430/127656, loss: 0.006632107309997082\n","epoch 0, step: 124440/127656, loss: 0.007664388045668602\n","epoch 0, step: 124450/127656, loss: 0.007389857899397612\n","epoch 0, step: 124460/127656, loss: 0.013947374187409878\n","epoch 0, step: 124470/127656, loss: 0.004527215845882893\n","epoch 0, step: 124480/127656, loss: 0.0070641483180224895\n","epoch 0, step: 124490/127656, loss: 0.003952912054955959\n","epoch 0, step: 124500/127656, loss: 0.006731068715453148\n","epoch 0, step: 124510/127656, loss: 0.008112285286188126\n","epoch 0, step: 124520/127656, loss: 0.010159581899642944\n","epoch 0, step: 124530/127656, loss: 0.007638678885996342\n","epoch 0, step: 124540/127656, loss: 0.5194862484931946\n","epoch 0, step: 124550/127656, loss: 0.005086707882583141\n","epoch 0, step: 124560/127656, loss: 0.007803957909345627\n","epoch 0, step: 124570/127656, loss: 0.014124262146651745\n","epoch 0, step: 124580/127656, loss: 0.007140961475670338\n","epoch 0, step: 124590/127656, loss: 0.2884315550327301\n","epoch 0, step: 124600/127656, loss: 0.007376302033662796\n","epoch 0, step: 124610/127656, loss: 0.011187046766281128\n","epoch 0, step: 124620/127656, loss: 0.006013485603034496\n","epoch 0, step: 124630/127656, loss: 0.003798169083893299\n","epoch 0, step: 124640/127656, loss: 0.004975522868335247\n","epoch 0, step: 124650/127656, loss: 0.006921018939465284\n","epoch 0, step: 124660/127656, loss: 0.005912111606448889\n","epoch 0, step: 124670/127656, loss: 0.00819962378591299\n","epoch 0, step: 124680/127656, loss: 0.007675293367356062\n","epoch 0, step: 124690/127656, loss: 0.006598549894988537\n","epoch 0, step: 124700/127656, loss: 0.005195289850234985\n","epoch 0, step: 124710/127656, loss: 0.008006561547517776\n","epoch 0, step: 124720/127656, loss: 0.407886266708374\n","epoch 0, step: 124730/127656, loss: 0.006773391272872686\n","epoch 0, step: 124740/127656, loss: 0.0075659132562577724\n","epoch 0, step: 124750/127656, loss: 0.2906873822212219\n","epoch 0, step: 124760/127656, loss: 0.006273773964494467\n","epoch 0, step: 124770/127656, loss: 0.007800649851560593\n","epoch 0, step: 124780/127656, loss: 0.0073472363874316216\n","epoch 0, step: 124790/127656, loss: 0.006072739139199257\n","epoch 0, step: 124800/127656, loss: 0.005530438385903835\n","epoch 0, step: 124810/127656, loss: 0.0068943193182349205\n","epoch 0, step: 124820/127656, loss: 0.005624078214168549\n","epoch 0, step: 124830/127656, loss: 0.007466254290193319\n","epoch 0, step: 124840/127656, loss: 0.009377687238156796\n","epoch 0, step: 124850/127656, loss: 0.009046249091625214\n","epoch 0, step: 124860/127656, loss: 0.004627999849617481\n","epoch 0, step: 124870/127656, loss: 0.005580142140388489\n","epoch 0, step: 124880/127656, loss: 0.008088745176792145\n","epoch 0, step: 124890/127656, loss: 0.01754925213754177\n","epoch 0, step: 124900/127656, loss: 0.00678115664049983\n","epoch 0, step: 124910/127656, loss: 0.004189079161733389\n","epoch 0, step: 124920/127656, loss: 0.0058637578040361404\n","epoch 0, step: 124930/127656, loss: 0.009950116276741028\n","epoch 0, step: 124940/127656, loss: 0.5611435174942017\n","epoch 0, step: 124950/127656, loss: 0.008686251938343048\n","epoch 0, step: 124960/127656, loss: 0.1423991173505783\n","epoch 0, step: 124970/127656, loss: 0.007956507615745068\n","epoch 0, step: 124980/127656, loss: 0.43437284231185913\n","epoch 0, step: 124990/127656, loss: 0.00889284722507\n","epoch 0, step: 125000/127656, loss: 0.00737379677593708\n","epoch 0, step: 125010/127656, loss: 0.008532905019819736\n","epoch 0, step: 125020/127656, loss: 0.010042558424174786\n","epoch 0, step: 125030/127656, loss: 0.0064072879031300545\n","epoch 0, step: 125040/127656, loss: 0.007177797611802816\n","epoch 0, step: 125050/127656, loss: 0.005607437342405319\n","epoch 0, step: 125060/127656, loss: 0.006355856545269489\n","epoch 0, step: 125070/127656, loss: 0.009201652370393276\n","epoch 0, step: 125080/127656, loss: 0.007022242993116379\n","epoch 0, step: 125090/127656, loss: 0.012230864726006985\n","epoch 0, step: 125100/127656, loss: 0.007651085965335369\n","epoch 0, step: 125110/127656, loss: 0.01372592430561781\n","epoch 0, step: 125120/127656, loss: 0.007746999617666006\n","epoch 0, step: 125130/127656, loss: 0.4169207811355591\n","epoch 0, step: 125140/127656, loss: 0.0077164387330412865\n","epoch 0, step: 125150/127656, loss: 0.1285599172115326\n","epoch 0, step: 125160/127656, loss: 0.00725724920630455\n","epoch 0, step: 125170/127656, loss: 0.2707308530807495\n","epoch 0, step: 125180/127656, loss: 0.5323726534843445\n","epoch 0, step: 125190/127656, loss: 0.008566392585635185\n","epoch 0, step: 125200/127656, loss: 0.00865696370601654\n","epoch 0, step: 125210/127656, loss: 0.009197558276355267\n","epoch 0, step: 125220/127656, loss: 0.007810542359948158\n","epoch 0, step: 125230/127656, loss: 0.009582600556313992\n","epoch 0, step: 125240/127656, loss: 0.007907593622803688\n","epoch 0, step: 125250/127656, loss: 0.005888450425118208\n","epoch 0, step: 125260/127656, loss: 0.005581939592957497\n","epoch 0, step: 125270/127656, loss: 0.014703977853059769\n","epoch 0, step: 125280/127656, loss: 0.005522104911506176\n","epoch 0, step: 125290/127656, loss: 0.006432283669710159\n","epoch 0, step: 125300/127656, loss: 0.007377468049526215\n","epoch 0, step: 125310/127656, loss: 0.007340395823121071\n","epoch 0, step: 125320/127656, loss: 0.008589064702391624\n","epoch 0, step: 125330/127656, loss: 0.009792495518922806\n","epoch 0, step: 125340/127656, loss: 0.006541569717228413\n","epoch 0, step: 125350/127656, loss: 0.00795308779925108\n","epoch 0, step: 125360/127656, loss: 0.009882355108857155\n","epoch 0, step: 125370/127656, loss: 0.007363644894212484\n","epoch 0, step: 125380/127656, loss: 0.008248642086982727\n","epoch 0, step: 125390/127656, loss: 0.007318486925214529\n","epoch 0, step: 125400/127656, loss: 0.00766802579164505\n","epoch 0, step: 125410/127656, loss: 0.01137481164187193\n","epoch 0, step: 125420/127656, loss: 0.014319241046905518\n","epoch 0, step: 125430/127656, loss: 0.008724440820515156\n","epoch 0, step: 125440/127656, loss: 0.005122220143675804\n","epoch 0, step: 125450/127656, loss: 0.005707032047212124\n","epoch 0, step: 125460/127656, loss: 0.006074558012187481\n","epoch 0, step: 125470/127656, loss: 0.009052987210452557\n","epoch 0, step: 125480/127656, loss: 0.008978018537163734\n","epoch 0, step: 125490/127656, loss: 0.008989974856376648\n","epoch 0, step: 125500/127656, loss: 0.010454932227730751\n","epoch 0, step: 125510/127656, loss: 0.01407778263092041\n","epoch 0, step: 125520/127656, loss: 0.006447543855756521\n","epoch 0, step: 125530/127656, loss: 0.009421948343515396\n","epoch 0, step: 125540/127656, loss: 0.008591936901211739\n","epoch 0, step: 125550/127656, loss: 0.13967637717723846\n","epoch 0, step: 125560/127656, loss: 0.00661176722496748\n","epoch 0, step: 125570/127656, loss: 0.007785840891301632\n","epoch 0, step: 125580/127656, loss: 0.006236596964299679\n","epoch 0, step: 125590/127656, loss: 0.007406231015920639\n","epoch 0, step: 125600/127656, loss: 0.005914164241403341\n","epoch 0, step: 125610/127656, loss: 0.1400998830795288\n","epoch 0, step: 125620/127656, loss: 0.004657966084778309\n","epoch 0, step: 125630/127656, loss: 0.0061714909970760345\n","epoch 0, step: 125640/127656, loss: 0.006690351292490959\n","epoch 0, step: 125650/127656, loss: 0.00892005953937769\n","epoch 0, step: 125660/127656, loss: 0.012352376244962215\n","epoch 0, step: 125670/127656, loss: 0.006257525645196438\n","epoch 0, step: 125680/127656, loss: 0.004387717694044113\n","epoch 0, step: 125690/127656, loss: 0.007100989576429129\n","epoch 0, step: 125700/127656, loss: 0.007698453031480312\n","epoch 0, step: 125710/127656, loss: 0.004819442518055439\n","epoch 0, step: 125720/127656, loss: 0.005593148060142994\n","epoch 0, step: 125730/127656, loss: 0.004835787694901228\n","epoch 0, step: 125740/127656, loss: 0.42814022302627563\n","epoch 0, step: 125750/127656, loss: 0.005612565204501152\n","epoch 0, step: 125760/127656, loss: 0.005439396016299725\n","epoch 0, step: 125770/127656, loss: 0.00794164091348648\n","epoch 0, step: 125780/127656, loss: 0.006412493996322155\n","epoch 0, step: 125790/127656, loss: 0.006738933734595776\n","epoch 0, step: 125800/127656, loss: 0.00798368826508522\n","epoch 0, step: 125810/127656, loss: 0.008222982287406921\n","epoch 0, step: 125820/127656, loss: 0.006585574708878994\n","epoch 0, step: 125830/127656, loss: 0.0034254472702741623\n","epoch 0, step: 125840/127656, loss: 0.011790911667048931\n","epoch 0, step: 125850/127656, loss: 0.0105553288012743\n","epoch 0, step: 125860/127656, loss: 0.007736334577202797\n","epoch 0, step: 125870/127656, loss: 0.011843502521514893\n","epoch 0, step: 125880/127656, loss: 0.007721829228103161\n","epoch 0, step: 125890/127656, loss: 0.00537173543125391\n","epoch 0, step: 125900/127656, loss: 0.008100965991616249\n","epoch 0, step: 125910/127656, loss: 0.008514393121004105\n","epoch 0, step: 125920/127656, loss: 0.0059369648806750774\n","epoch 0, step: 125930/127656, loss: 0.006642369087785482\n","epoch 0, step: 125940/127656, loss: 0.004169643856585026\n","epoch 0, step: 125950/127656, loss: 0.00611342117190361\n","epoch 0, step: 125960/127656, loss: 0.005359617993235588\n","epoch 0, step: 125970/127656, loss: 0.007618898060172796\n","epoch 0, step: 125980/127656, loss: 0.009065739810466766\n","epoch 0, step: 125990/127656, loss: 0.009427926503121853\n","epoch 0, step: 126000/127656, loss: 0.27972814440727234\n","epoch 0, step: 126010/127656, loss: 0.005468012299388647\n","epoch 0, step: 126020/127656, loss: 0.007001219317317009\n","epoch 0, step: 126030/127656, loss: 0.008060766384005547\n","epoch 0, step: 126040/127656, loss: 0.005471342243254185\n","epoch 0, step: 126050/127656, loss: 0.014201691374182701\n","epoch 0, step: 126060/127656, loss: 0.006676199845969677\n","epoch 0, step: 126070/127656, loss: 0.007596570998430252\n","epoch 0, step: 126080/127656, loss: 0.009431852959096432\n","epoch 0, step: 126090/127656, loss: 0.010273628868162632\n","epoch 0, step: 126100/127656, loss: 0.010458252392709255\n","epoch 0, step: 126110/127656, loss: 0.0063655562698841095\n","epoch 0, step: 126120/127656, loss: 0.2933889329433441\n","epoch 0, step: 126130/127656, loss: 0.005030018277466297\n","epoch 0, step: 126140/127656, loss: 0.004392449744045734\n","epoch 0, step: 126150/127656, loss: 0.004694432020187378\n","epoch 0, step: 126160/127656, loss: 0.006700418423861265\n","epoch 0, step: 126170/127656, loss: 0.008722560480237007\n","epoch 0, step: 126180/127656, loss: 0.007297781761735678\n","epoch 0, step: 126190/127656, loss: 0.008890628814697266\n","epoch 0, step: 126200/127656, loss: 0.010543416254222393\n","epoch 0, step: 126210/127656, loss: 0.4205237030982971\n","epoch 0, step: 126220/127656, loss: 0.00778347160667181\n","epoch 0, step: 126230/127656, loss: 0.0070876991376280785\n","epoch 0, step: 126240/127656, loss: 0.00520273158326745\n","epoch 0, step: 126250/127656, loss: 0.14845305681228638\n","epoch 0, step: 126260/127656, loss: 0.015827583149075508\n","epoch 0, step: 126270/127656, loss: 0.009626897051930428\n","epoch 0, step: 126280/127656, loss: 0.010261855088174343\n","epoch 0, step: 126290/127656, loss: 0.0064465999603271484\n","epoch 0, step: 126300/127656, loss: 0.006517406553030014\n","epoch 0, step: 126310/127656, loss: 0.008425971493124962\n","epoch 0, step: 126320/127656, loss: 0.010061552748084068\n","epoch 0, step: 126330/127656, loss: 0.00831698440015316\n","epoch 0, step: 126340/127656, loss: 0.005597956478595734\n","epoch 0, step: 126350/127656, loss: 0.009292054921388626\n","epoch 0, step: 126360/127656, loss: 0.003465314395725727\n","epoch 0, step: 126370/127656, loss: 0.008853370323777199\n","epoch 0, step: 126380/127656, loss: 0.006273757666349411\n","epoch 0, step: 126390/127656, loss: 0.006941385567188263\n","epoch 0, step: 126400/127656, loss: 0.008247433230280876\n","epoch 0, step: 126410/127656, loss: 0.5340245962142944\n","epoch 0, step: 126420/127656, loss: 0.006705672480165958\n","epoch 0, step: 126430/127656, loss: 0.011185835115611553\n","epoch 0, step: 126440/127656, loss: 0.007662705611437559\n","epoch 0, step: 126450/127656, loss: 0.008563587442040443\n","epoch 0, step: 126460/127656, loss: 0.005103379022330046\n","epoch 0, step: 126470/127656, loss: 0.13950034976005554\n","epoch 0, step: 126480/127656, loss: 0.004290517885237932\n","epoch 0, step: 126490/127656, loss: 0.007313068024814129\n","epoch 0, step: 126500/127656, loss: 0.005652390420436859\n","epoch 0, step: 126510/127656, loss: 0.006376422010362148\n","epoch 0, step: 126520/127656, loss: 0.007241975516080856\n","epoch 0, step: 126530/127656, loss: 0.011564681306481361\n","epoch 0, step: 126540/127656, loss: 0.0086302999407053\n","epoch 0, step: 126550/127656, loss: 0.005554150324314833\n","epoch 0, step: 126560/127656, loss: 0.008007763884961605\n","epoch 0, step: 126570/127656, loss: 0.4162658452987671\n","epoch 0, step: 126580/127656, loss: 0.00582619896158576\n","epoch 0, step: 126590/127656, loss: 0.1442389190196991\n","epoch 0, step: 126600/127656, loss: 0.5554405450820923\n","epoch 0, step: 126610/127656, loss: 0.007677996065467596\n","epoch 0, step: 126620/127656, loss: 0.006724949926137924\n","epoch 0, step: 126630/127656, loss: 0.005652830936014652\n","epoch 0, step: 126640/127656, loss: 0.007391124032437801\n","epoch 0, step: 126650/127656, loss: 0.011842340230941772\n","epoch 0, step: 126660/127656, loss: 0.0065540047362446785\n","epoch 0, step: 126670/127656, loss: 0.009928276762366295\n","epoch 0, step: 126680/127656, loss: 0.009695577435195446\n","epoch 0, step: 126690/127656, loss: 0.010041089728474617\n","epoch 0, step: 126700/127656, loss: 0.007272438611835241\n","epoch 0, step: 126710/127656, loss: 0.004512382671236992\n","epoch 0, step: 126720/127656, loss: 0.42520996928215027\n","epoch 0, step: 126730/127656, loss: 0.007824370637536049\n","epoch 0, step: 126740/127656, loss: 0.010813849978148937\n","epoch 0, step: 126750/127656, loss: 0.005119666922837496\n","epoch 0, step: 126760/127656, loss: 0.003277922049164772\n","epoch 0, step: 126770/127656, loss: 0.006893043871968985\n","epoch 0, step: 126780/127656, loss: 0.004739618860185146\n","epoch 0, step: 126790/127656, loss: 0.00858210027217865\n","epoch 0, step: 126800/127656, loss: 0.006104062777012587\n","epoch 0, step: 126810/127656, loss: 0.007809584494680166\n","epoch 0, step: 126820/127656, loss: 0.014028213918209076\n","epoch 0, step: 126830/127656, loss: 0.40407952666282654\n","epoch 0, step: 126840/127656, loss: 0.00922252144664526\n","epoch 0, step: 126850/127656, loss: 0.011869262903928757\n","epoch 0, step: 126860/127656, loss: 0.005503024905920029\n","epoch 0, step: 126870/127656, loss: 0.28322356939315796\n","epoch 0, step: 126880/127656, loss: 0.005745771341025829\n","epoch 0, step: 126890/127656, loss: 0.007704565301537514\n","epoch 0, step: 126900/127656, loss: 0.004705274477601051\n","epoch 0, step: 126910/127656, loss: 0.007731183432042599\n","epoch 0, step: 126920/127656, loss: 0.0063717057928442955\n","epoch 0, step: 126930/127656, loss: 0.012297367677092552\n","epoch 0, step: 126940/127656, loss: 0.003118587424978614\n","epoch 0, step: 126950/127656, loss: 0.008009511977434158\n","epoch 0, step: 126960/127656, loss: 0.005978643428534269\n","epoch 0, step: 126970/127656, loss: 0.007392344530671835\n","epoch 0, step: 126980/127656, loss: 0.004483995493501425\n","epoch 0, step: 126990/127656, loss: 0.00946062058210373\n","epoch 0, step: 127000/127656, loss: 0.008132131770253181\n","epoch 0, step: 127010/127656, loss: 0.00803414173424244\n","epoch 0, step: 127020/127656, loss: 0.0069320229813456535\n","epoch 0, step: 127030/127656, loss: 0.005578863900154829\n","epoch 0, step: 127040/127656, loss: 0.006036138162016869\n","epoch 0, step: 127050/127656, loss: 0.005613255314528942\n","epoch 0, step: 127060/127656, loss: 0.42332345247268677\n","epoch 0, step: 127070/127656, loss: 0.010127713903784752\n","epoch 0, step: 127080/127656, loss: 0.01349544432014227\n","epoch 0, step: 127090/127656, loss: 0.004009909462183714\n","epoch 0, step: 127100/127656, loss: 0.00804063118994236\n","epoch 0, step: 127110/127656, loss: 0.0060968175530433655\n","epoch 0, step: 127120/127656, loss: 0.010156603530049324\n","epoch 0, step: 127130/127656, loss: 0.005450359545648098\n","epoch 0, step: 127140/127656, loss: 0.003750526113435626\n","epoch 0, step: 127150/127656, loss: 0.011558622121810913\n","epoch 0, step: 127160/127656, loss: 0.005682568531483412\n","epoch 0, step: 127170/127656, loss: 0.006038866937160492\n","epoch 0, step: 127180/127656, loss: 0.007532897405326366\n","epoch 0, step: 127190/127656, loss: 0.006676696240901947\n","epoch 0, step: 127200/127656, loss: 0.006854495964944363\n","epoch 0, step: 127210/127656, loss: 0.009919666685163975\n","epoch 0, step: 127220/127656, loss: 0.00770540302619338\n","epoch 0, step: 127230/127656, loss: 0.01162627898156643\n","epoch 0, step: 127240/127656, loss: 0.006364748813211918\n","epoch 0, step: 127250/127656, loss: 0.008536107838153839\n","epoch 0, step: 127260/127656, loss: 0.004787708632647991\n","epoch 0, step: 127270/127656, loss: 0.28498587012290955\n","epoch 0, step: 127280/127656, loss: 0.010168014094233513\n","epoch 0, step: 127290/127656, loss: 0.007707095704972744\n","epoch 0, step: 127300/127656, loss: 0.004957505501806736\n","epoch 0, step: 127310/127656, loss: 0.004242805764079094\n","epoch 0, step: 127320/127656, loss: 0.007111720275133848\n","epoch 0, step: 127330/127656, loss: 0.008627692237496376\n","epoch 0, step: 127340/127656, loss: 0.008436758071184158\n","epoch 0, step: 127350/127656, loss: 0.005283907055854797\n","epoch 0, step: 127360/127656, loss: 0.006497051101177931\n","epoch 0, step: 127370/127656, loss: 0.009270548820495605\n","epoch 0, step: 127380/127656, loss: 0.00576828746125102\n","epoch 0, step: 127390/127656, loss: 0.005020971409976482\n","epoch 0, step: 127400/127656, loss: 0.008808568120002747\n","epoch 0, step: 127410/127656, loss: 0.00507915485650301\n","epoch 0, step: 127420/127656, loss: 0.006755191367119551\n","epoch 0, step: 127430/127656, loss: 0.006193868350237608\n","epoch 0, step: 127440/127656, loss: 0.006604380905628204\n","epoch 0, step: 127450/127656, loss: 0.007728904485702515\n","epoch 0, step: 127460/127656, loss: 0.009259704500436783\n","epoch 0, step: 127470/127656, loss: 0.004860951565206051\n","epoch 0, step: 127480/127656, loss: 0.007066288497298956\n","epoch 0, step: 127490/127656, loss: 0.007843650877475739\n","epoch 0, step: 127500/127656, loss: 0.008904194459319115\n","epoch 0, step: 127510/127656, loss: 0.005107450764626265\n","epoch 0, step: 127520/127656, loss: 0.00692124804481864\n","epoch 0, step: 127530/127656, loss: 0.004637970123440027\n","epoch 0, step: 127540/127656, loss: 0.005314941518008709\n","epoch 0, step: 127550/127656, loss: 0.006747812032699585\n","epoch 0, step: 127560/127656, loss: 0.00945262797176838\n","epoch 0, step: 127570/127656, loss: 0.00699493195861578\n","epoch 0, step: 127580/127656, loss: 0.566470742225647\n","epoch 0, step: 127590/127656, loss: 0.00881657749414444\n","epoch 0, step: 127600/127656, loss: 0.007681208662688732\n","epoch 0, step: 127610/127656, loss: 0.006156559102237225\n","epoch 0, step: 127620/127656, loss: 0.00641629658639431\n","epoch 0, step: 127630/127656, loss: 0.12290772050619125\n","epoch 0, step: 127640/127656, loss: 0.0072057899087667465\n","epoch 0, step: 127650/127656, loss: 0.008543498814105988\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 31915/31915 [07:28<00:00, 71.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss: 1185.1531435800716, accuracy: 0.9632774591445923\n","epoch 1, step: 0/127656, loss: 0.012906039133667946\n","epoch 1, step: 10/127656, loss: 0.006058826111257076\n","epoch 1, step: 20/127656, loss: 0.009030120447278023\n","epoch 1, step: 30/127656, loss: 0.004060372244566679\n","epoch 1, step: 40/127656, loss: 0.00726316636428237\n","epoch 1, step: 50/127656, loss: 0.0045259008184075356\n","epoch 1, step: 60/127656, loss: 0.006032948382198811\n","epoch 1, step: 70/127656, loss: 0.005029780324548483\n","epoch 1, step: 80/127656, loss: 0.009778342209756374\n","epoch 1, step: 90/127656, loss: 0.007706912234425545\n","epoch 1, step: 100/127656, loss: 0.004693928640335798\n","epoch 1, step: 110/127656, loss: 0.004794818814843893\n","epoch 1, step: 120/127656, loss: 0.0051606185734272\n","epoch 1, step: 130/127656, loss: 0.004712600260972977\n","epoch 1, step: 140/127656, loss: 0.006563751958310604\n","epoch 1, step: 150/127656, loss: 0.009322567842900753\n","epoch 1, step: 160/127656, loss: 0.004660601262003183\n","epoch 1, step: 170/127656, loss: 0.007555277552455664\n","epoch 1, step: 180/127656, loss: 0.004591092001646757\n","epoch 1, step: 190/127656, loss: 0.015217921696603298\n","epoch 1, step: 200/127656, loss: 0.005228342022746801\n","epoch 1, step: 210/127656, loss: 0.0060581485740840435\n","epoch 1, step: 220/127656, loss: 0.006126925349235535\n","epoch 1, step: 230/127656, loss: 0.006062617059797049\n","epoch 1, step: 240/127656, loss: 0.007995840162038803\n","epoch 1, step: 250/127656, loss: 0.0040284739807248116\n","epoch 1, step: 260/127656, loss: 0.015157616697251797\n","epoch 1, step: 270/127656, loss: 0.015588043257594109\n","epoch 1, step: 280/127656, loss: 0.010093078948557377\n","epoch 1, step: 290/127656, loss: 0.0035760016180574894\n","epoch 1, step: 300/127656, loss: 0.0034909662790596485\n","epoch 1, step: 310/127656, loss: 0.005269910208880901\n","epoch 1, step: 320/127656, loss: 0.006181026808917522\n","epoch 1, step: 330/127656, loss: 0.0075926994904875755\n","epoch 1, step: 340/127656, loss: 0.011391213163733482\n","epoch 1, step: 350/127656, loss: 0.00831952691078186\n","epoch 1, step: 360/127656, loss: 0.006452087312936783\n","epoch 1, step: 370/127656, loss: 0.004638060461729765\n","epoch 1, step: 380/127656, loss: 0.006595776882022619\n","epoch 1, step: 390/127656, loss: 0.005869842600077391\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-1263b716064c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import gc\n","for epoch in range(100):\n","    torch.save(model, f\"drive/MyDrive/toxic_comments/model_{epoch}.pth\")\n","    for i, (text, labels) in enumerate(train_loader):\n","        torch.cuda.empty_cache()\n","        optimizer.zero_grad()\n","        labels = labels.to(device)\n","        mask = text['attention_mask'].to(device)\n","        input_id = text['input_ids'].squeeze(1).to(device)\n","        \n","        output = model(input_id, mask)\n","        \n","        loss = criterion(output, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if(i % 10 == 0):\n","            print(f\"epoch {epoch}, step: {i}/{len(train_data)}, loss: {loss}\")\n","\n","    test_model()"]},{"cell_type":"code","source":["test_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"id":"FAktBHWbIRPb","executionInfo":{"status":"error","timestamp":1643740400587,"user_tz":300,"elapsed":3156,"user":{"displayName":"Nikash Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1sMZYDA6a0JlBWLzs4jCRMIYEPcfUJB3g1F3b=s64","userId":"09073734246033888052"}},"outputId":"31a30fff-e8ab-4c09-cdd9-7af918c8c819"},"id":"FAktBHWbIRPb","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 113/31915 [00:03<14:07, 37.53it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-d08da2596b53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-4a2e6a2db6dc>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-b2f69290a6e4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_id, mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         )\n\u001b[1;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                 )\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         )\n\u001b[1;32m    479\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"id":"necessary-lexington","metadata":{"id":"necessary-lexington"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"unable-counter","metadata":{"id":"unable-counter"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"indonesian-behavior","metadata":{"id":"indonesian-behavior"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"remarkable-robert","metadata":{"id":"remarkable-robert"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"enormous-appearance","metadata":{"id":"enormous-appearance"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"supported-omega","metadata":{"id":"supported-omega"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"third-atlantic","metadata":{"id":"third-atlantic"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"name":"toxic_comments.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}